{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  this is to use to generate segmenation dataset\n",
    "## The label less 20 will be ignored. The raw numpy dicom values will be instored in np file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the dataset root dir\n",
    "train_root_dir = \"E:/dataset/Leisang/myTry/BleedingDataDCM/train/\"\n",
    "print(\"train_root_dir:\", train_root_dir)\n",
    "\n",
    "# find all the case folder list\n",
    "train_case_list = glob(train_root_dir + \"*\")\n",
    "# print(\"case_list:\", case_list)\n",
    "print(\"len of case_list:\", len(train_case_list))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each case list start to make npy file by checking the label file\n",
    "delay_datatype = np.short\n",
    "marked_datatype =  np.ubyte\n",
    "#     label = np.fromfile(mask_path, dtype = datatype)\n",
    "label_path = None\n",
    "input_path =  None\n",
    "\n",
    "num_nonzeros_label_list=[]\n",
    "num_nonzeros_equal_one = []\n",
    "for each_case in train_case_list:\n",
    "    # for the raw files fisrt\n",
    "    print(each_case)\n",
    "    raw_files = glob(each_case + \"/*.raw\")\n",
    "    dicom_files = glob(each_case + \"/*.DCM\")\n",
    "    \n",
    "    print(\"len of dicoms:\", len(dicom_files))\n",
    "#     print(\"raw_files in one case:\", raw_files)\n",
    "#     print(\"len of raws:\", len(raw_files))\n",
    "    \n",
    "#     if \"ZA-021_001\" in each_case or \"ZA-022_001\" in each_case or \"ZA-029_001\" in each_case or  \"ZA-033_001\" in each_case or \"ZA-037_001\" in each_case or \"ZA-038_001\" in each_case  or \"ZA-041_001\" in each_case  or \"ZA-043_001\" in each_case or \"ZA-044_001\" in each_case: \n",
    "#         print(\"bad delay file xxxxxx------------------------->\")\n",
    "#         continue\n",
    "\n",
    "#     if len(raw_files) == 2: # read delay as raw npy.\n",
    "    for each_path in raw_files:\n",
    "        if \"marked\" in each_path:\n",
    "            label_path =  each_path\n",
    "#                 elif \"delay\" in each_path:\n",
    "#                     input_path =  each_path\n",
    "        else:\n",
    "            pass\n",
    "#             print(\"input path:\", input_path)\n",
    "    print(\"label path:\", label_path)\n",
    "\n",
    "#             # read inputs--->\n",
    "#             inputs = np.fromfile(input_path, dtype = delay_datatype)\n",
    "#             print(\"input raw shape:\", inputs.shape)\n",
    "#             # reshape the read file to the  512,x512s\n",
    "#             reshaped_inputs =  inputs.reshape([-1, 512, 512])\n",
    "#             print(\"reshape inputs shape:\", reshaped_inputs.shape)\n",
    "#             assert  reshaped_inputs.shape[0] ==  len(dicom_files) , 'len of dicoms is not equal to the raw input values'\n",
    "\n",
    "    # read labels\n",
    "    labels = np.fromfile(label_path, dtype = marked_datatype)\n",
    "#             print(\"input raw shape:\", labels.shape)\n",
    "#             labels[labels> 0] = 255\n",
    "    reshaped_labels =  labels.reshape([-1, 512, 512])\n",
    "    print(\"reshape labels shape:\", reshaped_labels.shape)\n",
    "    assert  reshaped_labels.shape[0] ==  len(dicom_files)  , 'len of dicoms is not equal to the raw label values'\n",
    "\n",
    "    # count the number of non-zeros in each label\n",
    "    for i in range(reshaped_labels.shape[0]):\n",
    "        num_nonzero = np.count_nonzero(reshaped_labels[i])\n",
    "        if num_nonzero>0:\n",
    "            num_nonzeros_label_list.append(num_nonzero)\n",
    "#         if num_nonzero ==1:\n",
    "#             num_nonzeros_equal_one.append(num_nonzero)\n",
    "\n",
    "\n",
    "    print(\"--------------------------\")\n",
    "#         delay_values = np.fromfile(mask_path, dtype = datatype)\n",
    "    pass\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"len(num_nonzeros_equal_one)\", num_nonzeros_label_list.count(1))\n",
    "# plot histogram of num_nonzeros_label_list\n",
    "import matplotlib.pyplot as plt\n",
    "plt.hist(num_nonzeros_label_list, bins=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10], color='b',\n",
    "         edgecolor='k', align='left')\n",
    "plt.xlabel('# of nonzeros pixels for each slice')\n",
    "plt.ylabel('count')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot histogram of num_nonzeros_label_list\n",
    "bins =  range(1, max(num_nonzeros_label_list))\n",
    "print(bins)\n",
    "import matplotlib.pyplot as plt\n",
    "plt.hist(num_nonzeros_label_list, bins = bins, color='b',\n",
    "         edgecolor='k')\n",
    "# plt.hist(num_nonzeros_label_list, color='b',\n",
    "#          edgecolor='k', align='left')\n",
    "plt.xlabel('# of nonzeros pixels for each slice')\n",
    "plt.ylabel('count')\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count numbers of labels < 10 pixels\n",
    "total = 0\n",
    "for i in range(1,10):\n",
    " \n",
    "    count =  num_nonzeros_label_list. count(i)\n",
    "    print(str(i), \":\", count)\n",
    "    total = total + count\n",
    "print(\"total:\", total)\n",
    "print(\"total non-zero positive slice:\", len(num_nonzeros_label_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## prepare dataset for train_positive , trian_negative, val_positive, val_negative\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read excel to check whether need to flip the label\n",
    "!pip install xlrd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xlrd\n",
    "\n",
    "# give the location of the file\n",
    "train_excel_loc = \"E:\\\\dataset\\\\Leisang\\\\myTry\\\\BleedingDataDCM/TrainfilpOrNot.xlsx\"\n",
    "print(\"train_excel_loc:\", train_excel_loc)\n",
    "val_excel_loc = \"E:\\\\dataset\\\\Leisang\\\\myTry\\\\BleedingDataDCM/ValfilpOrNot.xlsx\"\n",
    "print(\"val_excel_loc:\", val_excel_loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get work_book of excel\n",
    "train_wb = xlrd.open_workbook(train_excel_loc)\n",
    "# get train filp colomn\n",
    "sheet =  train_wb.sheet_by_index(0)\n",
    "print(\"sheet:\", sheet)\n",
    "\n",
    "# extract_number_of rows\n",
    "print(sheet.nrows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## define TF records feature type and feature description:\n",
    "# The following functions can be used to convert a value to a type compatible\n",
    "# with tf.Example.\n",
    "\n",
    "def _bytes_feature(value):\n",
    "  \"\"\"Returns a bytes_list from a string / byte.\"\"\"\n",
    "  if isinstance(value, type(tf.constant(0))):\n",
    "    value = value.numpy() # BytesList won't unpack a string from an EagerTensor.\n",
    "  return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "def _float_feature(value):\n",
    "  \"\"\"Returns a float_list from a float / double.\"\"\"\n",
    "  return tf.train.Feature(float_list=tf.train.FloatList(value=[value]))\n",
    "\n",
    "def _int64_feature(value):\n",
    "  \"\"\"Returns an int64_list from a bool / enum / int / uint.\"\"\"\n",
    "  return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n",
    "\n",
    "\n",
    "# Create a dictionary with features that may be relevant. # create tf.example message\n",
    "def image_example(each_dicom_path, seg_label, cls_label):\n",
    "    feature = {\n",
    "      \n",
    "        'dicom_path': _bytes_feature(each_dicom_path.encode()),\n",
    "        'seg_label': _bytes_feature(seg_label.tostring()), \n",
    "        'cls_label': _int64_feature(cls_label),\n",
    "        \n",
    "    }\n",
    "\n",
    "    return tf.train.Example(features=tf.train.Features(feature=feature))\n",
    "\n",
    "train_positive_samples_file = 'train_positive_samples.tfrecords'\n",
    "train_negative_samples_file = 'train_negative_samples.tfrecords'\n",
    "\n",
    "val_positive_samples_file = 'val_positive_samples.tfrecords'\n",
    "val_negative_samples_file = 'val_negative_samples.tfrecords'\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pydicom\n",
    "from IPython import display\n",
    "# for train data set ------------------------->\n",
    "# loop to get the flip colom value\n",
    "train_case_names =  []\n",
    "train_case_flips = []\n",
    "for i in range(1, sheet.nrows):\n",
    "    train_case_names.append(sheet.cell_value(i, 0))\n",
    "    train_case_flips.append(sheet.cell_value(i, 1))\n",
    "    \n",
    "# convert flip notes into integer \n",
    "print(\"train_case_flips:\", train_case_flips)\n",
    "train_case_flips =[int(s) for s in train_case_flips] \n",
    "print(\"train_case_names:\", train_case_names)\n",
    "print(\"train_case_flips:\", train_case_flips)\n",
    "print(\"len of train case data:\", len(train_case_names))\n",
    "print(\"len of train flip data:\", len(train_case_flips))\n",
    "\n",
    "# check train case list\n",
    "print(\"train case list len:\", len(train_case_list))\n",
    "assert len(train_case_names) ==  len(train_case_flips), \" len of cases names does not macthed with flip len in train excel \"\n",
    "assert len(train_case_list) ==  len(train_case_names), \" len of cases in the train folder not macthed with cases in train excel \"\n",
    "\n",
    "my_positive_exmaple ={}\n",
    "my_negative_exmaple={}\n",
    "\n",
    "for case_index, each_case in enumerate(train_case_list):\n",
    "    print(each_case)\n",
    "    # get case name from case list:\n",
    "    each_case_str = os.path.split(each_case) \n",
    "    print(\"each_case_str:\", each_case_str)\n",
    "    case_name =  each_case_str[-1]\n",
    "    print(\"case name:\", case_name)\n",
    "    print(\"case index:\", i, \"case name:\", train_case_names[case_index])\n",
    "    # check case name match or not\n",
    "    assert case_name ==  train_case_names[case_index]\n",
    "\n",
    "    # read_labels for specified case name\n",
    "    raw_files = glob(each_case + \"/*.raw\")\n",
    "    dicom_files = glob(each_case + \"/*.DCM\")\n",
    "    # get label path\n",
    "    for each_path in raw_files:\n",
    "        if \"marked\" in each_path:\n",
    "            label_path =  each_path\n",
    "    print(\"label path:\", label_path)\n",
    "    # read labels\n",
    "    labels = np.fromfile(label_path, dtype = marked_datatype)\n",
    "    reshaped_labels =  labels.reshape([-1, 512, 512])\n",
    "    print(\"reshape labels shape:\", reshaped_labels.shape)\n",
    "    print(\"len of dicoms in this case:\", len(dicom_files))\n",
    "    assert  reshaped_labels.shape[0] ==  len(dicom_files)  , 'len of dicoms is not equal to the raw label values'\n",
    "\n",
    "   # check whether needs to flip the label\n",
    "    if train_case_flips[case_index] ==1: # flip the enitre label\n",
    "        train_final_case_labels = np.flip(reshaped_labels, 0)\n",
    "    else: \n",
    "        train_final_case_labels =  reshaped_labels\n",
    "\n",
    "    None_zero_label_index = []\n",
    "    # read inputs\n",
    "    for dicom_index, each_dicom in enumerate(dicom_files):\n",
    "#         print(\"reading current dicom index:\", dicom_index)\n",
    "#         print(each_dicom)\n",
    "        #read dicoms\n",
    "#         ds = pydicom.dcmread(each_dicom)\n",
    "#          # get dcm input pixel content:\n",
    "#         pix = ds.pixel_array\n",
    "# #         print(\"one slice input shape:\", pix.shape)\n",
    "#         # reshape input as [1, shape0, shape1]\n",
    "#         pix = pix.reshape([1, pix.shape[0], pix.shape[1]])\n",
    "#         print(\"one slice reshaped input shape:\", pix.shape)\n",
    "#         # get label with the same index\n",
    "#         label = reshaped_labels[i]\n",
    "#         print(\"one slice label shape:\", label.shape)\n",
    "\n",
    "\n",
    "\n",
    "        # lets make dataset arrays\n",
    "#         if dicom_index == 0:\n",
    "#             train_final_case_inputs =  pix\n",
    "#         else:\n",
    "#             train_final_case_inputs =  np.concatenate((train_final_case_inputs, pix), axis=0)\n",
    "#         print(\"train_final_case_inputs shape:\", train_final_case_inputs.shape)\n",
    "\n",
    "\n",
    "        # Lets generate tf records-----------------------> with feauture formmat [\"dicom path\":segmentation label(float), classification label(int)]\n",
    "        # we use tensorflow dicom decoder to decode dicom image , so we only need the dicom paths to be the input feature\n",
    "        # genarete the classification labels: bleed with 1. No bleed for that slice with 0\n",
    "        each_slice_label =  train_final_case_labels[dicom_index]\n",
    "\n",
    "        # count non_zeros value from the each slice label\n",
    "        num_nonzero = np.count_nonzero(each_slice_label)\n",
    "\n",
    "        if num_nonzero > 10:  # only consider the # of nonzeros values >10 to be the positive labels\n",
    "            each_slice_cls_label = 1\n",
    "            print(\"num_nonzero:\", num_nonzero)\n",
    "            None_zero_label_index.append(dicom_index+1)\n",
    "            print(\"each_slice_label shape:\", each_slice_label.shape)\n",
    "            print(\"each_slice label range before binarization: [{}, {}]\".format(each_slice_label.min(), each_slice_label.max()))\n",
    "\n",
    "            # binarize the mask label\n",
    "            each_slice_label[each_slice_label> 0] = 255\n",
    "            print(\"each_slice label range before after binarization: [{}, {}]\".format(each_slice_label.min(), each_slice_label.max()))\n",
    "\n",
    "            # genrate positive examples\n",
    "            my_positive_exmaple[each_dicom]=(each_slice_label, each_slice_cls_label)\n",
    "\n",
    "        else: # negative examples\n",
    "            each_slice_cls_label = 0\n",
    "            my_negative_exmaple[each_dicom]=(each_slice_label, each_slice_cls_label)\n",
    "\n",
    "        if dicom_index % 20 == 0:\n",
    "                display.clear_output(wait=True)  \n",
    "\n",
    "        if dicom_index == len(dicom_files)-1:\n",
    "            print(\"total \" + str(len(dicom_files)) + \" checked ---------------------------------------------------------------------------->\")\n",
    "\n",
    "#         print(\"my_example:\", my_exmaple)\n",
    "\n",
    "#         # clear the output of the cell\n",
    "       \n",
    "    # check the length for each case\n",
    "    print(\"generate positive {} positive samples at currently case step:\".format(len(my_positive_exmaple)))\n",
    "    print(\"generate positive {} negative samples at currently case step:\".format(len(my_negative_exmaple)))   \n",
    "#     # check the dimension of train_inputs and train_labels\n",
    "#     print(\"train_final_case_inputs shape:\", train_final_case_inputs.shape)\n",
    "#     print(\"train_final_case_labels shape:\", train_final_case_labels.shape)\n",
    "\n",
    "\n",
    "#     image_string = open(filename, 'rb').read()\n",
    "#     tf_example = image_example(image_string, label)  # need image_example function to genearte mesage for writing tf records\n",
    "#     writer.write(tf_example.SerializeToString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_display = 0\n",
    "# strat to write postive train dicoms\n",
    "with tf.io.TFRecordWriter(train_positive_samples_file) as writer:\n",
    "    for dicom_path, labels in my_positive_exmaple.items():\n",
    "        print(\"dicom_path:\", dicom_path)\n",
    "#         dicom_path_string = open(dicom_path, 'rb').read()\n",
    "        seg_label = labels[0]\n",
    "        print(seg_label.shape)\n",
    "        cls_label = labels[1]\n",
    "        tf_example = image_example(dicom_path, seg_label, cls_label)  # need image_example function to genearte mesage for writing tf records\n",
    "        writer.write(tf_example.SerializeToString())\n",
    "        count_display+=1\n",
    "        if count_display >20:\n",
    "            display.clear_output(wait=True)  \n",
    "print(\"train_positve records generated finished.\")\n",
    "#         print(\"seg_label shape:\", labels[0].shape)\n",
    "#         print(\"cls_label:\", labels[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# start to write negative train \n",
    "with tf.io.TFRecordWriter(train_negative_samples_file) as writer:\n",
    "    for dicom_path, labels in my_negative_exmaple.items():\n",
    "        print(\"dicom_path:\", dicom_path)\n",
    "        dicom_path_string = open(dicom_path, 'rb').read()\n",
    "        seg_label = labels[0]\n",
    "        cls_label = labels[1]\n",
    "        tf_example = image_example(dicom_path_string, seg_label, cls_label)  # need image_example function to genearte mesage for writing tf records\n",
    "        writer.write(tf_example.SerializeToString())\n",
    "        count_display+=1\n",
    "        if count_display >20:\n",
    "            display.clear_output(wait=True)  \n",
    "print(\"train_negative records generated finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## read generated tfrecords to check the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_io as tfio\n",
    "train_positive_dataset = tf.data.TFRecordDataset('train_positive_samples.tfrecords')\n",
    "\n",
    "# Create a dictionary describing the features.\n",
    "image_feature_description = {\n",
    "        'dicom_path': tf.io.FixedLenFeature([], tf.string),\n",
    "        'seg_label': tf.io.FixedLenFeature([], tf.string), \n",
    "        'cls_label': tf.io.FixedLenFeature([], tf.int64),\n",
    "}\n",
    "\n",
    "\n",
    "def _parse_image_function(example_proto):\n",
    "    parsed_features = tf.io.parse_single_example(example_proto, image_feature_description)\n",
    "    # Parse the input tf.Example proto using the dictionary above.\n",
    "    \n",
    "    \n",
    "    # decode dicom\n",
    "    dicom_path = parsed_features[\"dicom_path\"]\n",
    "    image_bytes = tf.io.read_file(dicom_path)\n",
    "    input_image = tf.cast(tfio.image.decode_dicom_image(image_bytes, dtype=tf.uint16), tf.float32)\n",
    "    # decode mask\n",
    "    seg_label = tf.io.decode_raw(parsed_features['seg_label'], tf.uint8)\n",
    "    return dicom_path, input_image, seg_label,  parsed_features[\"cls_label\"]\n",
    "\n",
    "\n",
    "parsed_train_positive_dataset = train_positive_dataset.map(_parse_image_function)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# window the input\n",
    "def winwise(input,LB,HB):\n",
    "    # 20 ,380 for range (-32768, 32767)\n",
    "    # for tf input , (0, 65535)-? LB =  32788, 33148\n",
    "    input[input<LB] = LB # low boundary , if < LW , set to LW\n",
    "    input[input>HB] = HB # high boundary, if > Hw, Set to 255\n",
    "    return input\n",
    "\n",
    "# print(len(parsed_train_positive_dataset))\n",
    "BUFFER_SIZE =512\n",
    "# random shuffle the train positive dagtaset\n",
    "parsed_train_positive_dataset = parsed_train_positive_dataset.shuffle(BUFFER_SIZE)\n",
    "\n",
    "\n",
    "# \n",
    "\n",
    "\n",
    "for image_features in parsed_train_positive_dataset.take(10):\n",
    "    dicom_path =  image_features[0]\n",
    "    input =  image_features[1]\n",
    "#     dicom_path = image_features[0].numpy()\n",
    "    target = image_features[2]\n",
    "    cls_label = image_features[3]\n",
    "    print(\"dicom_path:\", dicom_path)\n",
    "    print(\"input_image.shape\", input.shape)\n",
    "#     print(\"dicom_path\", dicom_path)\n",
    "#     print(\"dicom_path:\", dicom_path)\n",
    "    print(\"seg_label\", target.shape)\n",
    "    print(\"cls_label\", cls_label)\n",
    "    \n",
    "#     # reshape label \n",
    "#     target =  tf.reshape(target, input.shape)\n",
    "#     fig, axes = plt.subplots(1,3, figsize=(20,20))\n",
    "    \n",
    "#     # \n",
    "#     mask =  np.squeeze(target.numpy())\n",
    "#     input_arr = np.squeeze(winwise(input.numpy(), 32788,33148 ))\n",
    "#     masked_in = mask + input_arr\n",
    "#     masked = np.ma.masked_where(mask == 0, mask)\n",
    "    \n",
    "#     axes[0].imshow(np.squeeze(winwise(input.numpy(), 32788,33148 )), cmap='gray')\n",
    "#     axes[0].set_title('input range:[{}, {}]'.format((input.numpy().min()), np.max(input.numpy())))\n",
    "#     axes[1].imshow(np.squeeze(target.numpy()), cmap='gray')\n",
    "#     axes[1].set_title('target range:[{}, {}]'.format(np.min(target), np.max(target)))\n",
    "#     axes[2].imshow(masked, 'jet', interpolation='none', alpha=0.7)\n",
    "#     axes[2].set_title(\"maksed_input\")\n",
    "#     print(image_path)\n",
    "    # rehsape \n",
    "#     display.display(display.Image(data=dicom_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functionmal TFRECORDS generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pydicom\n",
    "from IPython import display\n",
    "import xlrd\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def _bytes_feature(value):\n",
    "  \"\"\"Returns a bytes_list from a string / byte.\"\"\"\n",
    "  if isinstance(value, type(tf.constant(0))):\n",
    "    value = value.numpy() # BytesList won't unpack a string from an EagerTensor.\n",
    "  return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "def _float_feature(value):\n",
    "  \"\"\"Returns a float_list from a float / double.\"\"\"\n",
    "  return tf.train.Feature(float_list=tf.train.FloatList(value=[value]))\n",
    "\n",
    "def _int64_feature(value):\n",
    "  \"\"\"Returns an int64_list from a bool / enum / int / uint.\"\"\"\n",
    "  return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n",
    "\n",
    "\n",
    "# Create a dictionary with features that may be relevant. # create tf.example message\n",
    "def image_example(each_dicom_path, seg_label, cls_label):\n",
    "    feature = {\n",
    "      \n",
    "        'dicom_path': _bytes_feature(each_dicom_path.encode()),\n",
    "        'seg_label': _bytes_feature(seg_label.tostring()), \n",
    "        'cls_label': _int64_feature(cls_label),\n",
    "        \n",
    "    }\n",
    "\n",
    "    return tf.train.Example(features=tf.train.Features(feature=feature))\n",
    "\n",
    "def make_samples(root_folder_path_for_cases, excel_loc, marked_datatype= np.ubyte):\n",
    "    \n",
    "    \n",
    "    # get case_list\n",
    "    case_list = glob(root_folder_path_for_cases + \"*\")\n",
    "    # print(\"case_list:\", case_list)\n",
    "    print(\"len of case_list:\", len(case_list))\n",
    "    \n",
    "    \n",
    "    # get work_book of excel\n",
    "    train_wb = xlrd.open_workbook(excel_loc)\n",
    "    # get train filp colomn\n",
    "    sheet =  train_wb.sheet_by_index(0)\n",
    "    print(\"sheet:\", sheet)\n",
    "\n",
    "    # extract_number_of rows\n",
    "    print(sheet.nrows)\n",
    "    \n",
    "    case_names =  []\n",
    "    case_flips = []\n",
    "    for i in range(1, sheet.nrows):\n",
    "        case_names.append(sheet.cell_value(i, 0))\n",
    "        case_flips.append(sheet.cell_value(i, 1))\n",
    "\n",
    "    # convert flip notes into integer \n",
    "    print(\"case_flips:\", case_flips)\n",
    "    case_flips =[int(s) for s in case_flips] \n",
    "    print(\"case_names:\", case_names)\n",
    "    print(\"case_flips:\", case_flips)\n",
    "    print(\"len of case data:\", len(case_names))\n",
    "    print(\"len of flip data:\", len(case_flips))\n",
    "\n",
    "    # check case list\n",
    "    print(\"case list len:\", len(case_list))\n",
    "    assert len(case_names) ==  len(case_flips), \" len of cases names does not macthed with flip len in train excel \"\n",
    "    assert len(case_list) ==  len(case_names), \" len of cases in the train folder not macthed with cases in train excel \"\n",
    "\n",
    "    my_positive_exmaple ={}\n",
    "    my_negative_exmaple={}\n",
    "    \n",
    "    for case_index, each_case in enumerate(case_list):\n",
    "        print(each_case)\n",
    "        # get case name from case list:\n",
    "        each_case_str = os.path.split(each_case) \n",
    "        print(\"each_case_str:\", each_case_str)\n",
    "        case_name =  each_case_str[-1]\n",
    "        print(\"case name:\", case_name)\n",
    "        print(\"case index:\", i, \"case name:\", case_names[case_index])\n",
    "        # check case name match or not\n",
    "        assert case_name ==  case_names[case_index]\n",
    "\n",
    "        # read_labels for specified case name\n",
    "        raw_files = glob(each_case + \"/*.raw\")\n",
    "        dicom_files = glob(each_case + \"/*.DCM\")\n",
    "        # get label path\n",
    "        for each_path in raw_files:\n",
    "            if \"marked\" in each_path:\n",
    "                label_path =  each_path\n",
    "        print(\"label path:\", label_path)\n",
    "        # read labels\n",
    "        labels = np.fromfile(label_path, dtype = marked_datatype)\n",
    "        reshaped_labels =  labels.reshape([-1, 512, 512])\n",
    "        print(\"reshape labels shape:\", reshaped_labels.shape)\n",
    "        print(\"len of dicoms in this case:\", len(dicom_files))\n",
    "        assert  reshaped_labels.shape[0] ==  len(dicom_files)  , 'len of dicoms is not equal to the raw label values'\n",
    "\n",
    "       # check whether needs to flip the label\n",
    "        if case_flips[case_index] ==1: # flip the enitre label\n",
    "            train_final_case_labels = np.flip(reshaped_labels, 0)\n",
    "        else: \n",
    "            train_final_case_labels =  reshaped_labels\n",
    "\n",
    "        None_zero_label_index = []\n",
    "        # read inputs\n",
    "        for dicom_index, each_dicom in enumerate(dicom_files):\n",
    "            \n",
    "            each_slice_label =  train_final_case_labels[dicom_index]\n",
    "            # count non_zeros value from the each slice label\n",
    "            num_nonzero = np.count_nonzero(each_slice_label)\n",
    "\n",
    "            if num_nonzero > 10:  # only consider the # of nonzeros values >10 to be the positive labels\n",
    "                each_slice_cls_label = 1\n",
    "                print(\"num_nonzero:\", num_nonzero)\n",
    "                None_zero_label_index.append(dicom_index+1)\n",
    "                print(\"each_slice_label shape:\", each_slice_label.shape)\n",
    "                print(\"each_slice label range before binarization: [{}, {}]\".format(each_slice_label.min(), each_slice_label.max()))\n",
    "\n",
    "                # binarize the mask label\n",
    "                each_slice_label[each_slice_label> 0] = 255\n",
    "                print(\"each_slice label range before after binarization: [{}, {}]\".format(each_slice_label.min(), each_slice_label.max()))\n",
    "\n",
    "                # genrate positive examples\n",
    "                my_positive_exmaple[each_dicom]=(each_slice_label, each_slice_cls_label)\n",
    "\n",
    "            else: # negative examples\n",
    "                each_slice_cls_label = 0\n",
    "                my_negative_exmaple[each_dicom]=(each_slice_label, each_slice_cls_label)\n",
    "\n",
    "            if dicom_index % 20 == 0:\n",
    "                    display.clear_output(wait=True)  \n",
    "\n",
    "            if dicom_index == len(dicom_files)-1:\n",
    "                print(\"total \" + str(len(dicom_files)) + \" checked ---------------------------------------------------------------------------->\")\n",
    "\n",
    "    #         print(\"my_example:\", my_exmaple)\n",
    "\n",
    "    #         # clear the output of the cell\n",
    "\n",
    "        # check the length for each case\n",
    "        print(\"generate positive {} positive samples at currently case step:\".format(len(my_positive_exmaple)))\n",
    "        print(\"generate positive {} negative samples at currently case step:\".format(len(my_negative_exmaple)))   \n",
    "    return my_positive_exmaple, my_negative_exmaple\n",
    "\n",
    "def Make_TFRecords(feature_samples_dict, file_name):\n",
    "    # start to write \n",
    "    print(\"writing tfrecords....\")\n",
    "    count_display = 0\n",
    "    # strat to write postive train dicoms\n",
    "    with tf.io.TFRecordWriter(file_name) as writer:\n",
    "        for dicom_path, labels in feature_samples_dict.items():\n",
    "            print(\"dicom_path:\", dicom_path)\n",
    "    #         dicom_path_string = open(dicom_path, 'rb').read()\n",
    "            seg_label = labels[0]\n",
    "            print(seg_label.shape)\n",
    "            cls_label = labels[1]\n",
    "            tf_example = image_example(dicom_path, seg_label, cls_label)  # need image_example function to genearte mesage for writing tf records\n",
    "            writer.write(tf_example.SerializeToString())\n",
    "            count_display+=1\n",
    "            if count_display >20:\n",
    "                display.clear_output(wait=True)  \n",
    "    print(\"Generation of TFRecords is finished.\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Train TFRECORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# for train\n",
    "train_root_folder_path_for_cases= \"E:/dataset/Leisang/myTry/BleedingDataDCM/train/\"\n",
    "## Make into functions for genrerate tf records\n",
    "train_positive_samples_file = 'train_positive_samples.tfrecords'\n",
    "train_negative_samples_file = 'train_negative_samples.tfrecords'\n",
    "# give the location of the file\n",
    "train_excel_loc = \"E:\\\\dataset\\\\Leisang\\\\myTry\\\\BleedingDataDCM/TrainfilpOrNot.xlsx\"\n",
    "\n",
    "\n",
    "# make positive and negative samples:\n",
    "train_positive_samples, train_negative_samples = make_samples(root_folder_path_for_cases=train_root_folder_path_for_cases,\n",
    "                                                              excel_loc = train_excel_loc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# make train_positive\n",
    "Make_TFRecords(feature_samples_dict=train_positive_samples, file_name=train_positive_samples_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# make train_negative\n",
    "Make_TFRecords(feature_samples_dict=train_negative_samples, file_name=train_negative_samples_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# check generated positive samples\n",
    "import tensorflow_io as tfio\n",
    "import matplotlib.pyplot as plt\n",
    "train_positive_dataset = tf.data.TFRecordDataset('train_positive_samples.tfrecords')\n",
    "train_negative_dataset = tf.data.TFRecordDataset('train_negative_samples.tfrecords')\n",
    "# Create a dictionary describing the features.\n",
    "image_feature_description = {\n",
    "        'dicom_path': tf.io.FixedLenFeature([], tf.string),\n",
    "        'seg_label': tf.io.FixedLenFeature([], tf.string), \n",
    "        'cls_label': tf.io.FixedLenFeature([], tf.int64),\n",
    "}\n",
    "\n",
    "\n",
    "def _parse_image_function(example_proto):\n",
    "    parsed_features = tf.io.parse_single_example(example_proto, image_feature_description)\n",
    "    # Parse the input tf.Example proto using the dictionary above.\n",
    "    \n",
    "    \n",
    "    # decode dicom\n",
    "    dicom_path = parsed_features[\"dicom_path\"]\n",
    "    image_bytes = tf.io.read_file(dicom_path)\n",
    "    input_image = tf.cast(tfio.image.decode_dicom_image(image_bytes, dtype=tf.uint16), tf.float32)\n",
    "    # decode mask\n",
    "    seg_label = tf.io.decode_raw(parsed_features['seg_label'], tf.uint8)\n",
    "    return dicom_path, input_image, seg_label,  parsed_features[\"cls_label\"]\n",
    "\n",
    "\n",
    "parsed_train_positive_dataset = train_positive_dataset.map(_parse_image_function)\n",
    "parsed_train_negative_dataset = train_negative_dataset.map(_parse_image_function)\n",
    "\n",
    "def check_dataset(dataset):\n",
    "    # window the input\n",
    "    def winwise(input,LB,HB):\n",
    "        # 20 ,380 for range (-32768, 32767)\n",
    "        # for tf input , (0, 65535)-? LB =  32788, 33148\n",
    "        input[input<LB] = LB # low boundary , if < LW , set to LW\n",
    "        input[input>HB] = HB # high boundary, if > Hw, Set to 255\n",
    "        return input\n",
    "\n",
    "    # print(len(parsed_train_positive_dataset))\n",
    "    BUFFER_SIZE =512\n",
    "    # random shuffle the train positive dagtaset\n",
    "    parsed_dataset = dataset.shuffle(BUFFER_SIZE)\n",
    "\n",
    "    for image_features in parsed_dataset.take(10):\n",
    "        dicom_path =  image_features[0]\n",
    "        input =  image_features[1]\n",
    "    #     dicom_path = image_features[0].numpy()\n",
    "        target = image_features[2]\n",
    "        cls_label = image_features[3]\n",
    "        print(\"dicom_path:\", dicom_path)\n",
    "        print(\"input_image.shape\", input.shape)\n",
    "    #     print(\"dicom_path\", dicom_path)\n",
    "    #     print(\"dicom_path:\", dicom_path)\n",
    "        print(\"seg_label\", target.shape)\n",
    "        print(\"cls_label\", cls_label)\n",
    "\n",
    "        # reshape label \n",
    "        target =  tf.reshape(target, input.shape)\n",
    "        fig, axes = plt.subplots(1,3, figsize=(20,20))\n",
    "\n",
    "        # \n",
    "        mask =  np.squeeze(target.numpy())\n",
    "        input_arr = np.squeeze(winwise(input.numpy(), 32788,33148 ))\n",
    "        masked_in = mask + input_arr\n",
    "        masked = np.ma.masked_where(mask == 0, mask)\n",
    "\n",
    "        axes[0].imshow(np.squeeze(winwise(input.numpy(), 32788,33148 )), cmap='gray')\n",
    "        axes[0].set_title('input range:[{}, {}]'.format((input.numpy().min()), np.max(input.numpy())))\n",
    "        axes[1].imshow(np.squeeze(target.numpy()), cmap='gray')\n",
    "        axes[1].set_title('target range:[{}, {}]'.format(np.min(target), np.max(target)))\n",
    "        axes[2].imshow(masked, 'jet', interpolation='none', alpha=0.7)\n",
    "        axes[2].set_title(\"maksed_input\")\n",
    "        print(dicom_path)\n",
    "\n",
    "        \n",
    "check_dataset(parsed_train_positive_dataset)\n",
    "check_dataset(parsed_train_negative_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Val TFRECORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# for val\n",
    "val_root_folder_path_for_cases= \"E:/dataset/Leisang/myTry/BleedingDataDCM/val/\"\n",
    "\n",
    "## Make into functions for genrerate tf records\n",
    "val_positive_samples_file = 'val_positive_samples.tfrecords'\n",
    "val_negative_samples_file = 'val_negative_samples.tfrecords'\n",
    "\n",
    "# excel file\n",
    "val_excel_loc = \"E:\\\\dataset\\\\Leisang\\\\myTry\\\\BleedingDataDCM/ValfilpOrNot.xlsx\"\n",
    "print(\"val_excel_loc:\", val_excel_loc)\n",
    "\n",
    "\n",
    "# make positive and negative samples:\n",
    "val_positive_samples, val_negative_samples = make_samples(root_folder_path_for_cases=val_root_folder_path_for_cases,\n",
    "                                                              excel_loc = val_excel_loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# make val_positive\n",
    "Make_TFRecords(feature_samples_dict=val_positive_samples, file_name=val_positive_samples_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# make val_positive\n",
    "Make_TFRecords(feature_samples_dict=val_negative_samples, file_name=val_negative_samples_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# check validation dataset\n",
    "val_positive_dataset = tf.data.TFRecordDataset('val_positive_samples.tfrecords')\n",
    "val_negative_dataset = tf.data.TFRecordDataset('val_negative_samples.tfrecords')\n",
    "parsed_val_positive_dataset = val_positive_dataset.map(_parse_image_function)\n",
    "parsed_val_negative_dataset = val_negative_dataset.map(_parse_image_function)\n",
    "\n",
    "check_dataset(parsed_val_positive_dataset)\n",
    "check_dataset(parsed_val_negative_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "!pip install pydicom\n",
    "import pydicom\n",
    "from IPython import display\n",
    "!pip install xlrd\n",
    "import xlrd\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def _bytes_feature(value):\n",
    "  \"\"\"Returns a bytes_list from a string / byte.\"\"\"\n",
    "  if isinstance(value, type(tf.constant(0))):\n",
    "    value = value.numpy() # BytesList won't unpack a string from an EagerTensor.\n",
    "  return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "def _float_feature(value):\n",
    "  \"\"\"Returns a float_list from a float / double.\"\"\"\n",
    "  return tf.train.Feature(float_list=tf.train.FloatList(value=[value]))\n",
    "\n",
    "def _int64_feature(value):\n",
    "  \"\"\"Returns an int64_list from a bool / enum / int / uint.\"\"\"\n",
    "  return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n",
    "\n",
    "\n",
    "# Create a dictionary with features that may be relevant. # create tf.example message\n",
    "def image_example(each_dicom_path, seg_label, cls_label):\n",
    "    feature = {\n",
    "      \n",
    "        'dicom_path': _bytes_feature(each_dicom_path.encode()),\n",
    "        'seg_label': _bytes_feature(seg_label.tostring()), \n",
    "        'cls_label': _int64_feature(cls_label),\n",
    "        \n",
    "    }\n",
    "\n",
    "    return tf.train.Example(features=tf.train.Features(feature=feature))\n",
    "\n",
    "def make_samples(root_folder_path_for_cases, excel_loc, marked_datatype= np.ubyte):\n",
    "    \n",
    "    \n",
    "    # get case_list\n",
    "    case_list = glob(root_folder_path_for_cases + \"*\")\n",
    "    # print(\"case_list:\", case_list)\n",
    "    print(\"len of case_list:\", len(case_list))\n",
    "    \n",
    "    \n",
    "    # get work_book of excel\n",
    "    train_wb = xlrd.open_workbook(excel_loc)\n",
    "    # get train filp colomn\n",
    "    sheet =  train_wb.sheet_by_index(0)\n",
    "    print(\"sheet:\", sheet)\n",
    "\n",
    "    # extract_number_of rows\n",
    "    print(sheet.nrows)\n",
    "    \n",
    "    case_names =  []\n",
    "    case_flips = []\n",
    "    for i in range(1, sheet.nrows):\n",
    "        case_names.append(sheet.cell_value(i, 0))\n",
    "        case_flips.append(sheet.cell_value(i, 1))\n",
    "\n",
    "    # convert flip notes into integer \n",
    "    print(\"case_flips:\", case_flips)\n",
    "    case_flips =[int(s) for s in case_flips] \n",
    "    print(\"case_names:\", case_names)\n",
    "    print(\"case_flips:\", case_flips)\n",
    "    print(\"len of case data:\", len(case_names))\n",
    "    print(\"len of flip data:\", len(case_flips))\n",
    "\n",
    "    # check case list\n",
    "    print(\"case list len:\", len(case_list))\n",
    "    assert len(case_names) ==  len(case_flips), \" len of cases names does not macthed with flip len in train excel \"\n",
    "    assert len(case_list) ==  len(case_names), \" len of cases in the train folder not macthed with cases in train excel \"\n",
    "\n",
    "    my_positive_exmaple ={}\n",
    "    my_negative_exmaple={}\n",
    "    \n",
    "    for case_index, each_case in enumerate(case_list):\n",
    "        print(each_case)\n",
    "        # get case name from case list:\n",
    "        each_case_str = os.path.split(each_case) \n",
    "        print(\"each_case_str:\", each_case_str)\n",
    "        case_name =  each_case_str[-1]\n",
    "        print(\"case name:\", case_name)\n",
    "        print(\"case index:\", i, \"case name:\", case_names[case_index])\n",
    "        # check case name match or not\n",
    "        assert case_name ==  case_names[case_index]\n",
    "\n",
    "        # read_labels for specified case name\n",
    "        raw_files = glob(each_case + \"/*.raw\")\n",
    "        dicom_files = glob(each_case + \"/*.DCM\")\n",
    "        # get label path\n",
    "        for each_path in raw_files:\n",
    "            if \"marked\" in each_path:\n",
    "                label_path =  each_path\n",
    "        print(\"label path:\", label_path)\n",
    "        # read labels\n",
    "        labels = np.fromfile(label_path, dtype = marked_datatype)\n",
    "        reshaped_labels =  labels.reshape([-1, 512, 512])\n",
    "        print(\"reshape labels shape:\", reshaped_labels.shape)\n",
    "        print(\"len of dicoms in this case:\", len(dicom_files))\n",
    "        assert  reshaped_labels.shape[0] ==  len(dicom_files)  , 'len of dicoms is not equal to the raw label values'\n",
    "\n",
    "       # check whether needs to flip the label\n",
    "        if case_flips[case_index] ==1: # flip the enitre label\n",
    "            train_final_case_labels = np.flip(reshaped_labels, 0)\n",
    "        else: \n",
    "            train_final_case_labels =  reshaped_labels\n",
    "\n",
    "        None_zero_label_index = []\n",
    "        # read inputs\n",
    "        for dicom_index, each_dicom in enumerate(dicom_files):\n",
    "            \n",
    "            each_slice_label =  train_final_case_labels[dicom_index]\n",
    "            # count non_zeros value from the each slice label\n",
    "            num_nonzero = np.count_nonzero(each_slice_label)\n",
    "\n",
    "            if num_nonzero > 10:  # only consider the # of nonzeros values >10 to be the positive labels\n",
    "                each_slice_cls_label = 1\n",
    "                print(\"num_nonzero:\", num_nonzero)\n",
    "                None_zero_label_index.append(dicom_index+1)\n",
    "                print(\"each_slice_label shape:\", each_slice_label.shape)\n",
    "                print(\"each_slice label range before binarization: [{}, {}]\".format(each_slice_label.min(), each_slice_label.max()))\n",
    "\n",
    "                # binarize the mask label\n",
    "                each_slice_label[each_slice_label> 0] = 255\n",
    "                print(\"each_slice label range before after binarization: [{}, {}]\".format(each_slice_label.min(), each_slice_label.max()))\n",
    "\n",
    "                # genrate positive examples\n",
    "                my_positive_exmaple[each_dicom]=(each_slice_label, each_slice_cls_label)\n",
    "\n",
    "            else: # negative examples\n",
    "                each_slice_cls_label = 0\n",
    "                my_negative_exmaple[each_dicom]=(each_slice_label, each_slice_cls_label)\n",
    "\n",
    "            if dicom_index % 20 == 0:\n",
    "                    display.clear_output(wait=True)  \n",
    "\n",
    "            if dicom_index == len(dicom_files)-1:\n",
    "                print(\"total \" + str(len(dicom_files)) + \" checked ---------------------------------------------------------------------------->\")\n",
    "\n",
    "    #         print(\"my_example:\", my_exmaple)\n",
    "\n",
    "    #         # clear the output of the cell\n",
    "\n",
    "        # check the length for each case\n",
    "        print(\"generate positive {} positive samples at currently case step:\".format(len(my_positive_exmaple)))\n",
    "        print(\"generate positive {} negative samples at currently case step:\".format(len(my_negative_exmaple)))   \n",
    "    return my_positive_exmaple, my_negative_exmaple\n",
    "\n",
    "def make_samples_ubuntu(root_folder_path_for_cases, excel_loc, train_or_val=\"train\", marked_datatype= np.ubyte):\n",
    "    \n",
    "    \n",
    "    # get case_list\n",
    "    case_list = glob(root_folder_path_for_cases + \"*\")\n",
    "    # print(\"case_list:\", case_list)\n",
    "    print(\"len of case_list:\", len(case_list))\n",
    "    \n",
    "    \n",
    "    # get work_book of excel\n",
    "    train_wb = xlrd.open_workbook(excel_loc)\n",
    "    # get train filp colomn\n",
    "    sheet =  train_wb.sheet_by_index(0)\n",
    "    print(\"sheet:\", sheet)\n",
    "\n",
    "    # extract_number_of rows\n",
    "    print(sheet.nrows)\n",
    "    \n",
    "    case_names =  []\n",
    "    case_flips = []\n",
    "    for i in range(1, sheet.nrows):\n",
    "        case_names.append(sheet.cell_value(i, 0))\n",
    "        case_flips.append(sheet.cell_value(i, 1))\n",
    "\n",
    "    # convert flip notes into integer \n",
    "    print(\"case_flips:\", case_flips)\n",
    "    case_flips =[int(s) for s in case_flips] \n",
    "    print(\"case_names:\", case_names)\n",
    "    print(\"case_flips:\", case_flips)\n",
    "    print(\"len of case data:\", len(case_names))\n",
    "    print(\"len of flip data:\", len(case_flips))\n",
    "\n",
    "    # check case list\n",
    "    print(\"case list len:\", len(case_list))\n",
    "    assert len(case_names) ==  len(case_flips), \" len of cases names does not macthed with flip len in train excel \"\n",
    "    assert len(case_list) ==  len(case_names), \" len of cases in the train folder not macthed with cases in train excel \"\n",
    "\n",
    "    my_positive_exmaple ={}\n",
    "    my_negative_exmaple={}\n",
    "    \n",
    "    for case_index, each_case in enumerate(case_list):\n",
    "        print(each_case)\n",
    "        # get case name from case list:\n",
    "        each_case_str = os.path.split(each_case) \n",
    "        print(\"each_case_str:\", each_case_str)\n",
    "        case_name =  each_case_str[-1]\n",
    "        print(\"case name:\", case_name)\n",
    "        print(\"case index:\", i, \"case name:\", case_names[case_index])\n",
    "        # check case name match or not\n",
    "        assert case_name ==  case_names[case_index]\n",
    "\n",
    "        # read_labels for specified case name\n",
    "        raw_files = glob(each_case + \"/*.raw\")\n",
    "        dicom_files = glob(each_case + \"/*.DCM\")\n",
    "        # get label path\n",
    "        for each_path in raw_files:\n",
    "            if \"marked\" in each_path:\n",
    "                label_path =  each_path\n",
    "        print(\"label path:\", label_path)\n",
    "        # read labels\n",
    "        labels = np.fromfile(label_path, dtype = marked_datatype)\n",
    "        reshaped_labels =  labels.reshape([-1, 512, 512])\n",
    "        print(\"reshape labels shape:\", reshaped_labels.shape)\n",
    "        print(\"len of dicoms in this case:\", len(dicom_files))\n",
    "        assert  reshaped_labels.shape[0] ==  len(dicom_files)  , 'len of dicoms is not equal to the raw label values'\n",
    "\n",
    "       # check whether needs to flip the label\n",
    "        if case_flips[case_index] ==1: # flip the enitre label\n",
    "            train_final_case_labels = np.flip(reshaped_labels, 0)\n",
    "        else: \n",
    "            train_final_case_labels =  reshaped_labels\n",
    "\n",
    "        None_zero_label_index = []\n",
    "        # read inputs\n",
    "        for dicom_index, each_dicom in enumerate(dicom_files):\n",
    "#             print(each_dicom)\n",
    "            path_splits = os.path.split(each_dicom)\n",
    "            file_name = path_splits[-1]\n",
    "            print()\n",
    "            # replace the root folder for ubuntu\n",
    "            if train_or_val ==\"train\":\n",
    "                root =\"/media/ytx/Japan_Deep_Data/dataset/LeiSang/myTry/BleedingDataDCM/train\"\n",
    "                each_dicom =  root +\"/\"+ file_name\n",
    "            else:\n",
    "                root =\"/media/ytx/Japan_Deep_Data/dataset/LeiSang/myTry/BleedingDataDCM/val\"\n",
    "                each_dicom =  root +\"/\"+ file_name\n",
    "#             print(\"unbuntu dicom path:\",each_dicom)\n",
    "            \n",
    "            each_slice_label =  train_final_case_labels[dicom_index]\n",
    "            # count non_zeros value from the each slice label\n",
    "            num_nonzero = np.count_nonzero(each_slice_label)\n",
    "\n",
    "            if num_nonzero > 10:  # only consider the # of nonzeros values >10 to be the positive labels\n",
    "                each_slice_cls_label = 1\n",
    "                print(\"num_nonzero:\", num_nonzero)\n",
    "                None_zero_label_index.append(dicom_index+1)\n",
    "                print(\"each_slice_label shape:\", each_slice_label.shape)\n",
    "                print(\"each_slice label range before binarization: [{}, {}]\".format(each_slice_label.min(), each_slice_label.max()))\n",
    "\n",
    "                # binarize the mask label\n",
    "                each_slice_label[each_slice_label> 0] = 255\n",
    "                print(\"each_slice label range before after binarization: [{}, {}]\".format(each_slice_label.min(), each_slice_label.max()))\n",
    "\n",
    "                # genrate positive examples\n",
    "                my_positive_exmaple[each_dicom]=(each_slice_label, each_slice_cls_label)\n",
    "\n",
    "            else: # negative examples\n",
    "                each_slice_cls_label = 0\n",
    "                my_negative_exmaple[each_dicom]=(each_slice_label, each_slice_cls_label)\n",
    "\n",
    "#             if dicom_index % 20 == 0:\n",
    "#                     display.clear_output(wait=True)  \n",
    "\n",
    "            if dicom_index == len(dicom_files)-1:\n",
    "                print(\"total \" + str(len(dicom_files)) + \" checked ---------------------------------------------------------------------------->\")\n",
    "\n",
    "    #         print(\"my_example:\", my_exmaple)\n",
    "\n",
    "    #         # clear the output of the cell\n",
    "\n",
    "        # check the length for each case\n",
    "        print(\"generate positive {} positive samples at currently case step:\".format(len(my_positive_exmaple)))\n",
    "        print(\"generate positive {} negative samples at currently case step:\".format(len(my_negative_exmaple)))   \n",
    "    return my_positive_exmaple, my_negative_exmaple\n",
    "\n",
    "def Make_TFRecords(feature_samples_dict, file_name):\n",
    "    # start to write \n",
    "    print(\"writing tfrecords....\")\n",
    "    count_display = 0\n",
    "    # strat to write postive train dicoms\n",
    "    with tf.io.TFRecordWriter(file_name) as writer:\n",
    "        for dicom_path, labels in feature_samples_dict.items():\n",
    "            print(\"dicom_path:\", dicom_path)\n",
    "    #         dicom_path_string = open(dicom_path, 'rb').read()\n",
    "            seg_label = labels[0]\n",
    "            print(seg_label.shape)\n",
    "            cls_label = labels[1]\n",
    "            tf_example = image_example(dicom_path, seg_label, cls_label)  # need image_example function to genearte mesage for writing tf records\n",
    "            writer.write(tf_example.SerializeToString())\n",
    "            count_display+=1\n",
    "            if count_display >20:\n",
    "                display.clear_output(wait=True)  \n",
    "    print(\"Generation of TFRecords is finished.\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Train TFRECORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# for train\n",
    "train_root_folder_path_for_cases= \"E:/dataset/Leisang/myTry/BleedingDataDCM/train/\"\n",
    "# ## Make into functions for genrerate tf records\n",
    "# train_positive_samples_file = 'train_positive_samples.tfrecords'\n",
    "# train_negative_samples_file = 'train_negative_samples.tfrecords'\n",
    "# give the location of the file\n",
    "train_excel_loc = \"E:\\\\dataset\\\\Leisang\\\\myTry\\\\BleedingDataDCM/TrainfilpOrNot.xlsx\"\n",
    "\n",
    "# # for ubuntu:\n",
    "train_positive_samples_file = 'train_positive_samples_unbuntu.tfrecords'\n",
    "train_negative_samples_file = 'train_negative_samples_unbuntu.tfrecords'\n",
    "\n",
    "# make positive and negative samples:\n",
    "# train_positive_samples, train_negative_samples = make_samples(root_folder_path_for_cases=train_root_folder_path_for_cases,\n",
    "#                                                               excel_loc = train_excel_loc)\n",
    "train_positive_samples, train_negative_samples = make_samples_ubuntu(root_folder_path_for_cases=train_root_folder_path_for_cases,\n",
    "                                                              excel_loc = train_excel_loc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# make train_positive\n",
    "Make_TFRecords(feature_samples_dict=train_positive_samples, file_name=train_positive_samples_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# make train_negative\n",
    "Make_TFRecords(feature_samples_dict=train_negative_samples, file_name=train_negative_samples_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# check generated positive samples\n",
    "import tensorflow_io as tfio\n",
    "import matplotlib.pyplot as plt\n",
    "train_positive_dataset = tf.data.TFRecordDataset(train_positive_samples_file)\n",
    "train_negative_dataset = tf.data.TFRecordDataset(train_negative_samples_file)\n",
    "# Create a dictionary describing the features.\n",
    "image_feature_description = {\n",
    "        'dicom_path': tf.io.FixedLenFeature([], tf.string),\n",
    "        'seg_label': tf.io.FixedLenFeature([], tf.string), \n",
    "        'cls_label': tf.io.FixedLenFeature([], tf.int64),\n",
    "}\n",
    "\n",
    "\n",
    "def _parse_image_function(example_proto):\n",
    "    parsed_features = tf.io.parse_single_example(example_proto, image_feature_description)\n",
    "    # Parse the input tf.Example proto using the dictionary above.\n",
    "    \n",
    "    \n",
    "    # decode dicom\n",
    "    dicom_path = parsed_features[\"dicom_path\"]\n",
    "    image_bytes = tf.io.read_file(dicom_path)\n",
    "    input_image = tf.cast(tfio.image.decode_dicom_image(image_bytes, dtype=tf.uint16), tf.float32)\n",
    "    # decode mask\n",
    "    seg_label = tf.io.decode_raw(parsed_features['seg_label'], tf.uint8)\n",
    "    return dicom_path, input_image, seg_label,  parsed_features[\"cls_label\"]\n",
    "\n",
    "\n",
    "parsed_train_positive_dataset = train_positive_dataset.map(_parse_image_function)\n",
    "parsed_train_negative_dataset = train_negative_dataset.map(_parse_image_function)\n",
    "\n",
    "def check_dataset(dataset):\n",
    "    # window the input\n",
    "    def winwise(input,LB,HB):\n",
    "        # 20 ,380 for range (-32768, 32767)\n",
    "        # for tf input , (0, 65535)-? LB =  32788, 33148\n",
    "        input[input<LB] = LB # low boundary , if < LW , set to LW\n",
    "        input[input>HB] = HB # high boundary, if > Hw, Set to 255\n",
    "        return input\n",
    "\n",
    "    # print(len(parsed_train_positive_dataset))\n",
    "    BUFFER_SIZE =512\n",
    "    # random shuffle the train positive dagtaset\n",
    "    parsed_dataset = dataset.shuffle(BUFFER_SIZE)\n",
    "\n",
    "    for image_features in parsed_dataset.take(10):\n",
    "        dicom_path =  image_features[0]\n",
    "        input =  image_features[1]\n",
    "    #     dicom_path = image_features[0].numpy()\n",
    "        target = image_features[2]\n",
    "        cls_label = image_features[3]\n",
    "        print(\"dicom_path:\", dicom_path)\n",
    "        print(\"input_image.shape\", input.shape)\n",
    "    #     print(\"dicom_path\", dicom_path)\n",
    "    #     print(\"dicom_path:\", dicom_path)\n",
    "        print(\"seg_label\", target.shape)\n",
    "        print(\"cls_label\", cls_label)\n",
    "\n",
    "        # reshape label \n",
    "        target =  tf.reshape(target, input.shape)\n",
    "        fig, axes = plt.subplots(1,3, figsize=(20,20))\n",
    "\n",
    "        # \n",
    "        mask =  np.squeeze(target.numpy())\n",
    "        input_arr = np.squeeze(winwise(input.numpy(), 32788,33148 ))\n",
    "        masked_in = mask + input_arr\n",
    "        masked = np.ma.masked_where(mask == 0, mask)\n",
    "\n",
    "        axes[0].imshow(np.squeeze(winwise(input.numpy(), 32788,33148 )), cmap='gray')\n",
    "        axes[0].set_title('input range:[{}, {}]'.format((input.numpy().min()), np.max(input.numpy())))\n",
    "        axes[1].imshow(np.squeeze(target.numpy()), cmap='gray')\n",
    "        axes[1].set_title('target range:[{}, {}]'.format(np.min(target), np.max(target)))\n",
    "        axes[2].imshow(masked, 'jet', interpolation='none', alpha=0.7)\n",
    "        axes[2].set_title(\"maksed_input\")\n",
    "        print(dicom_path)\n",
    "\n",
    "        \n",
    "check_dataset(parsed_train_positive_dataset)\n",
    "check_dataset(parsed_train_negative_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Val TFRECORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# for val\n",
    "val_root_folder_path_for_cases= \"E:/dataset/Leisang/myTry/BleedingDataDCM/val/\"\n",
    "\n",
    "## Make into functions for genrerate tf records\n",
    "val_positive_samples_file = 'val_positive_samples.tfrecords'\n",
    "val_negative_samples_file = 'val_negative_samples.tfrecords'\n",
    "\n",
    "# excel file\n",
    "val_excel_loc = \"E:\\\\dataset\\\\Leisang\\\\myTry\\\\BleedingDataDCM/ValfilpOrNot.xlsx\"\n",
    "print(\"val_excel_loc:\", val_excel_loc)\n",
    "\n",
    "# # for ubuntu:\n",
    "val_positive_samples_file = 'val_positive_samples_unbuntu.tfrecords'\n",
    "val_negative_samples_file = 'val_negative_samples_unbuntu.tfrecords'\n",
    "\n",
    "# make positive and negative samples:\n",
    "# val_positive_samples, val_negative_samples = make_samples(root_folder_path_for_cases=val_root_folder_path_for_cases,\n",
    "#                                                               excel_loc = val_excel_loc)\n",
    "val_positive_samples, val_negative_samples = make_samples_ubuntu(root_folder_path_for_cases=val_root_folder_path_for_cases,\n",
    "                                                              excel_loc = val_excel_loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# make val_positive\n",
    "Make_TFRecords(feature_samples_dict=val_positive_samples, file_name=val_positive_samples_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# make val_positive\n",
    "Make_TFRecords(feature_samples_dict=val_negative_samples, file_name=val_negative_samples_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# check validation dataset\n",
    "val_positive_dataset = tf.data.TFRecordDataset(val_positive_samples_file)\n",
    "val_negative_dataset = tf.data.TFRecordDataset(val_negative_samples_file)\n",
    "parsed_val_positive_dataset = val_positive_dataset.map(_parse_image_function)\n",
    "parsed_val_negative_dataset = val_negative_dataset.map(_parse_image_function)\n",
    "\n",
    "check_dataset(parsed_val_positive_dataset)\n",
    "check_dataset(parsed_val_negative_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pydicom in e:\\projects\\intepreters\\anaconda\\envs\\tf21\\lib\\site-packages (1.4.2)\n",
      "Requirement already satisfied: xlrd in e:\\projects\\intepreters\\anaconda\\envs\\tf21\\lib\\site-packages (1.2.0)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "!pip install pydicom\n",
    "import pydicom\n",
    "from IPython import display\n",
    "!pip install xlrd\n",
    "import xlrd\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def _bytes_feature(value):\n",
    "  \"\"\"Returns a bytes_list from a string / byte.\"\"\"\n",
    "  if isinstance(value, type(tf.constant(0))):\n",
    "    value = value.numpy() # BytesList won't unpack a string from an EagerTensor.\n",
    "  return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "def _float_feature(value):\n",
    "  \"\"\"Returns a float_list from a float / double.\"\"\"\n",
    "  return tf.train.Feature(float_list=tf.train.FloatList(value=[value]))\n",
    "\n",
    "def _int64_feature(value):\n",
    "  \"\"\"Returns an int64_list from a bool / enum / int / uint.\"\"\"\n",
    "  return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n",
    "\n",
    "\n",
    "# Create a dictionary with features that may be relevant. # create tf.example message\n",
    "def image_example(each_dicom_path, seg_label, cls_label):\n",
    "    feature = {\n",
    "      \n",
    "        'dicom_path': _bytes_feature(each_dicom_path.encode()),\n",
    "        'seg_label': _bytes_feature(seg_label.tostring()), \n",
    "        'cls_label': _int64_feature(cls_label),\n",
    "        \n",
    "    }\n",
    "\n",
    "    return tf.train.Example(features=tf.train.Features(feature=feature))\n",
    "\n",
    "def make_samples(root_folder_path_for_cases, excel_loc, marked_datatype= np.ubyte):\n",
    "    \n",
    "    \n",
    "    # get case_list\n",
    "    case_list = glob(root_folder_path_for_cases + \"*\")\n",
    "    # print(\"case_list:\", case_list)\n",
    "    print(\"len of case_list:\", len(case_list))\n",
    "    \n",
    "    \n",
    "    # get work_book of excel\n",
    "    train_wb = xlrd.open_workbook(excel_loc)\n",
    "    # get train filp colomn\n",
    "    sheet =  train_wb.sheet_by_index(0)\n",
    "    print(\"sheet:\", sheet)\n",
    "\n",
    "    # extract_number_of rows\n",
    "    print(sheet.nrows)\n",
    "    \n",
    "    case_names =  []\n",
    "    case_flips = []\n",
    "    for i in range(1, sheet.nrows):\n",
    "        case_names.append(sheet.cell_value(i, 0))\n",
    "        case_flips.append(sheet.cell_value(i, 1))\n",
    "\n",
    "    # convert flip notes into integer \n",
    "    print(\"case_flips:\", case_flips)\n",
    "    case_flips =[int(s) for s in case_flips] \n",
    "    print(\"case_names:\", case_names)\n",
    "    print(\"case_flips:\", case_flips)\n",
    "    print(\"len of case data:\", len(case_names))\n",
    "    print(\"len of flip data:\", len(case_flips))\n",
    "\n",
    "    # check case list\n",
    "    print(\"case list len:\", len(case_list))\n",
    "    assert len(case_names) ==  len(case_flips), \" len of cases names does not macthed with flip len in train excel \"\n",
    "    assert len(case_list) ==  len(case_names), \" len of cases in the train folder not macthed with cases in train excel \"\n",
    "\n",
    "    my_positive_exmaple ={}\n",
    "    my_negative_exmaple={}\n",
    "    \n",
    "    for case_index, each_case in enumerate(case_list):\n",
    "        print(each_case)\n",
    "        # get case name from case list:\n",
    "        each_case_str = os.path.split(each_case) \n",
    "        print(\"each_case_str:\", each_case_str)\n",
    "        case_name =  each_case_str[-1]\n",
    "        print(\"case name:\", case_name)\n",
    "        print(\"case index:\", i, \"case name:\", case_names[case_index])\n",
    "        # check case name match or not\n",
    "        assert case_name ==  case_names[case_index]\n",
    "\n",
    "        # read_labels for specified case name\n",
    "        raw_files = glob(each_case + \"/*.raw\")\n",
    "        dicom_files = glob(each_case + \"/*.DCM\")\n",
    "        # get label path\n",
    "        for each_path in raw_files:\n",
    "            if \"marked\" in each_path:\n",
    "                label_path =  each_path\n",
    "        print(\"label path:\", label_path)\n",
    "        # read labels\n",
    "        labels = np.fromfile(label_path, dtype = marked_datatype)\n",
    "        reshaped_labels =  labels.reshape([-1, 512, 512])\n",
    "        print(\"reshape labels shape:\", reshaped_labels.shape)\n",
    "        print(\"len of dicoms in this case:\", len(dicom_files))\n",
    "        assert  reshaped_labels.shape[0] ==  len(dicom_files)  , 'len of dicoms is not equal to the raw label values'\n",
    "\n",
    "       # check whether needs to flip the label\n",
    "        if case_flips[case_index] ==1: # flip the enitre label\n",
    "            train_final_case_labels = np.flip(reshaped_labels, 0)\n",
    "        else: \n",
    "            train_final_case_labels =  reshaped_labels\n",
    "\n",
    "        None_zero_label_index = []\n",
    "        # read inputs\n",
    "        for dicom_index, each_dicom in enumerate(dicom_files):\n",
    "            \n",
    "            each_slice_label =  train_final_case_labels[dicom_index]\n",
    "            # count non_zeros value from the each slice label\n",
    "            num_nonzero = np.count_nonzero(each_slice_label)\n",
    "\n",
    "            if num_nonzero > 10:  # only consider the # of nonzeros values >10 to be the positive labels\n",
    "                each_slice_cls_label = 1\n",
    "                print(\"num_nonzero:\", num_nonzero)\n",
    "                None_zero_label_index.append(dicom_index+1)\n",
    "                print(\"each_slice_label shape:\", each_slice_label.shape)\n",
    "                print(\"each_slice label range before binarization: [{}, {}]\".format(each_slice_label.min(), each_slice_label.max()))\n",
    "\n",
    "                # binarize the mask label\n",
    "                each_slice_label[each_slice_label> 0] = 255\n",
    "                print(\"each_slice label range before after binarization: [{}, {}]\".format(each_slice_label.min(), each_slice_label.max()))\n",
    "\n",
    "                # genrate positive examples\n",
    "                my_positive_exmaple[each_dicom]=(each_slice_label, each_slice_cls_label)\n",
    "\n",
    "            else: # negative examples\n",
    "                each_slice_cls_label = 0\n",
    "                my_negative_exmaple[each_dicom]=(each_slice_label, each_slice_cls_label)\n",
    "\n",
    "            if dicom_index % 20 == 0:\n",
    "                    display.clear_output(wait=True)  \n",
    "\n",
    "            if dicom_index == len(dicom_files)-1:\n",
    "                print(\"total \" + str(len(dicom_files)) + \" checked ---------------------------------------------------------------------------->\")\n",
    "\n",
    "    #         print(\"my_example:\", my_exmaple)\n",
    "\n",
    "    #         # clear the output of the cell\n",
    "\n",
    "        # check the length for each case\n",
    "        print(\"generate positive {} positive samples at currently case step:\".format(len(my_positive_exmaple)))\n",
    "        print(\"generate positive {} negative samples at currently case step:\".format(len(my_negative_exmaple)))   \n",
    "    return my_positive_exmaple, my_negative_exmaple\n",
    "\n",
    "def Make_TFRecords(feature_samples_dict, file_name):\n",
    "    # start to write \n",
    "    print(\"writing tfrecords....\")\n",
    "    count_display = 0\n",
    "    # strat to write postive train dicoms\n",
    "    with tf.io.TFRecordWriter(file_name) as writer:\n",
    "        for dicom_path, labels in feature_samples_dict.items():\n",
    "            print(\"dicom_path:\", dicom_path)\n",
    "    #         dicom_path_string = open(dicom_path, 'rb').read()\n",
    "            seg_label = labels[0]\n",
    "            print(seg_label.shape)\n",
    "            cls_label = labels[1]\n",
    "            tf_example = image_example(dicom_path, seg_label, cls_label)  # need image_example function to genearte mesage for writing tf records\n",
    "            writer.write(tf_example.SerializeToString())\n",
    "            count_display+=1\n",
    "            if count_display >20:\n",
    "                display.clear_output(wait=True)  \n",
    "    print(\"Generation of TFRecords is finished.\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Train TFRECORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 1082 checked ---------------------------------------------------------------------------->\n",
      "generate positive 3035 positive samples at currently case step:\n",
      "generate positive 33588 negative samples at currently case step:\n"
     ]
    }
   ],
   "source": [
    "# for train\n",
    "train_root_folder_path_for_cases= \"E:/dataset/Leisang/myTry/BleedingDataDCM/train/\"\n",
    "## Make into functions for genrerate tf records\n",
    "train_positive_samples_file = 'train_positive_samples.tfrecords'\n",
    "train_negative_samples_file = 'train_negative_samples.tfrecords'\n",
    "# give the location of the file\n",
    "train_excel_loc = \"E:\\\\dataset\\\\Leisang\\\\myTry\\\\BleedingDataDCM/TrainfilpOrNot.xlsx\"\n",
    "\n",
    "# # for ubuntu:\n",
    "# # for train\n",
    "# train_root_folder_path_for_cases= \"/media/ytx/Japan_Deep_Data/dataset/LeiSang/myTry/BleedingDataDCM/train/\"\n",
    "# ## Make into functions for genrerate tf records\n",
    "# train_positive_samples_file = '/media/ytx/Japan_Deep_Data/dataset/LeiSang/myTry/BleedingDataDCM/train_positive_samples.tfrecords'\n",
    "# train_negative_samples_file = '/media/ytx/Japan_Deep_Data/dataset/LeiSang/myTry/BleedingDataDCM/train_negative_samples.tfrecords'\n",
    "# # give the location of the file\n",
    "# train_excel_loc = \"/media/ytx/Japan_Deep_Data/dataset/LeiSang/myTry/BleedingDataDCM/TrainfilpOrNot.xlsx\"\n",
    "\n",
    "# # make positive and negative samples:\n",
    "# train_positive_samples, train_negative_samples = make_samples(root_folder_path_for_cases=train_root_folder_path_for_cases,\n",
    "#                                                               excel_loc = train_excel_loc)\n",
    "\n",
    "train_positive_samples, train_negative_samples = make_samples(root_folder_path_for_cases=train_root_folder_path_for_cases,\n",
    "                                                              excel_loc = train_excel_loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make train_positive\n",
    "Make_TFRecords(feature_samples_dict=train_positive_samples, file_name=train_positive_samples_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make train_negative\n",
    "Make_TFRecords(feature_samples_dict=train_negative_samples, file_name=train_negative_samples_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check generated positive samples\n",
    "import tensorflow_io as tfio\n",
    "import matplotlib.pyplot as plt\n",
    "train_positive_dataset = tf.data.TFRecordDataset(train_positive_samples_file)\n",
    "train_negative_dataset = tf.data.TFRecordDataset(train_negative_samples_file)\n",
    "# Create a dictionary describing the features.\n",
    "image_feature_description = {\n",
    "        'dicom_path': tf.io.FixedLenFeature([], tf.string),\n",
    "        'seg_label': tf.io.FixedLenFeature([], tf.string), \n",
    "        'cls_label': tf.io.FixedLenFeature([], tf.int64),\n",
    "}\n",
    "\n",
    "\n",
    "def _parse_image_function(example_proto):\n",
    "    parsed_features = tf.io.parse_single_example(example_proto, image_feature_description)\n",
    "    # Parse the input tf.Example proto using the dictionary above.\n",
    "    \n",
    "    \n",
    "    # decode dicom\n",
    "    dicom_path = parsed_features[\"dicom_path\"]\n",
    "    image_bytes = tf.io.read_file(dicom_path)\n",
    "    input_image = tf.cast(tfio.image.decode_dicom_image(image_bytes, dtype=tf.uint16), tf.float32)\n",
    "    # decode mask\n",
    "    seg_label = tf.io.decode_raw(parsed_features['seg_label'], tf.uint8)\n",
    "    return dicom_path, input_image, seg_label,  parsed_features[\"cls_label\"]\n",
    "\n",
    "\n",
    "parsed_train_positive_dataset = train_positive_dataset.map(_parse_image_function)\n",
    "parsed_train_negative_dataset = train_negative_dataset.map(_parse_image_function)\n",
    "\n",
    "def check_dataset(dataset):\n",
    "    # window the input\n",
    "    def winwise(input,LB,HB):\n",
    "        # 20 ,380 for range (-32768, 32767)\n",
    "        # for tf input , (0, 65535)-? LB =  32788, 33148\n",
    "        input[input<LB] = LB # low boundary , if < LW , set to LW\n",
    "        input[input>HB] = HB # high boundary, if > Hw, Set to 255\n",
    "        return input\n",
    "\n",
    "    # print(len(parsed_train_positive_dataset))\n",
    "    BUFFER_SIZE =512\n",
    "    # random shuffle the train positive dagtaset\n",
    "    parsed_dataset = dataset.shuffle(BUFFER_SIZE)\n",
    "\n",
    "    for image_features in parsed_dataset.take(10):\n",
    "        dicom_path =  image_features[0]\n",
    "        input =  image_features[1]\n",
    "    #     dicom_path = image_features[0].numpy()\n",
    "        target = image_features[2]\n",
    "        cls_label = image_features[3]\n",
    "        print(\"dicom_path:\", dicom_path)\n",
    "        print(\"input_image.shape\", input.shape)\n",
    "    #     print(\"dicom_path\", dicom_path)\n",
    "    #     print(\"dicom_path:\", dicom_path)\n",
    "        print(\"seg_label\", target.shape)\n",
    "        print(\"cls_label\", cls_label)\n",
    "\n",
    "        # reshape label \n",
    "        target =  tf.reshape(target, input.shape)\n",
    "        fig, axes = plt.subplots(1,3, figsize=(20,20))\n",
    "\n",
    "        # \n",
    "        mask =  np.squeeze(target.numpy())\n",
    "        input_arr = np.squeeze(winwise(input.numpy(), 32788,33148 ))\n",
    "        masked_in = mask + input_arr\n",
    "        masked = np.ma.masked_where(mask == 0, mask)\n",
    "\n",
    "        axes[0].imshow(np.squeeze(winwise(input.numpy(), 32788,33148 )), cmap='gray')\n",
    "        axes[0].set_title('input range:[{}, {}]'.format((input.numpy().min()), np.max(input.numpy())))\n",
    "        axes[1].imshow(np.squeeze(target.numpy()), cmap='gray')\n",
    "        axes[1].set_title('target range:[{}, {}]'.format(np.min(target), np.max(target)))\n",
    "        axes[2].imshow(masked, 'jet', interpolation='none', alpha=0.7)\n",
    "        axes[2].set_title(\"maksed_input\")\n",
    "        print(dicom_path)\n",
    "\n",
    "        \n",
    "check_dataset(parsed_train_positive_dataset)\n",
    "check_dataset(parsed_train_negative_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Val TFRECORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for val\n",
    "val_root_folder_path_for_cases= \"E:/dataset/Leisang/myTry/BleedingDataDCM/val/\"\n",
    "\n",
    "## Make into functions for genrerate tf records\n",
    "val_positive_samples_file = 'val_positive_samples.tfrecords'\n",
    "val_negative_samples_file = 'val_negative_samples.tfrecords'\n",
    "\n",
    "# excel file\n",
    "val_excel_loc = \"E:\\\\dataset\\\\Leisang\\\\myTry\\\\BleedingDataDCM/ValfilpOrNot.xlsx\"\n",
    "print(\"val_excel_loc:\", val_excel_loc)\n",
    "\n",
    "\n",
    "# make positive and negative samples:\n",
    "val_positive_samples, val_negative_samples = make_samples(root_folder_path_for_cases=val_root_folder_path_for_cases,\n",
    "                                                              excel_loc = val_excel_loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make val_positive\n",
    "Make_TFRecords(feature_samples_dict=val_positive_samples, file_name=val_positive_samples_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make val_positive\n",
    "Make_TFRecords(feature_samples_dict=val_negative_samples, file_name=val_negative_samples_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check validation dataset\n",
    "val_positive_dataset = tf.data.TFRecordDataset('val_positive_samples.tfrecords')\n",
    "val_negative_dataset = tf.data.TFRecordDataset('val_negative_samples.tfrecords')\n",
    "parsed_val_positive_dataset = val_positive_dataset.map(_parse_image_function)\n",
    "parsed_val_negative_dataset = val_negative_dataset.map(_parse_image_function)\n",
    "\n",
    "check_dataset(parsed_val_positive_dataset)\n",
    "check_dataset(parsed_val_negative_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}