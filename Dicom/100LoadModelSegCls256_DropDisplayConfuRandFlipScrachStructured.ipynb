{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "from glob import glob\n",
    "!pip install -q tensorflow-io\n",
    "import tensorflow_io as tfio\n",
    "\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from IPython import display\n",
    "import imageio\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "import tensorflow.keras.preprocessing.image as prep\n",
    "\n",
    "\n",
    "from copy import copy\n",
    "import matplotlib.colors as colors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HYPERPARAMETERS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read dataset from train and validation tfrecords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paths for the tfrecords\n",
    "# # # for train\n",
    "train_positive =  \"E:\\\\dataset\\\\Leisang\\\\myTry\\\\BleedingDataDCM/train_positive_samples_win.tfrecords\"\n",
    "train_negative =  'E:\\\\dataset\\\\Leisang\\\\myTry\\\\BleedingDataDCM/train_negative_samples_win.tfrecords'\n",
    "\n",
    "# for val\n",
    "val_positive =  \"E:\\\\dataset\\\\Leisang\\\\myTry\\\\BleedingDataDCM/val_positive_samples_win.tfrecords\"\n",
    "val_negative =  'E:\\\\dataset\\\\Leisang\\\\myTry\\\\BleedingDataDCM/val_negative_samples_win.tfrecords'\n",
    "\n",
    "# for ubuntu\n",
    "\n",
    "# # for train\n",
    "# train_positive =  \"/media/ytx/Japan_Deep_Data/dataset/LeiSang/myTry/BleedingDataDCM/train_positive_samples_unbuntu.tfrecords\"\n",
    "# train_negative =  '/media/ytx/Japan_Deep_Data/dataset/LeiSang/myTry/BleedingDataDCM/train_negative_samples_unbuntu.tfrecords'\n",
    "\n",
    "# # for val\n",
    "# val_positive =  \"/media/ytx/Japan_Deep_Data/dataset/LeiSang/myTry/BleedingDataDCM/val_positive_samples_unbuntu.tfrecords\"\n",
    "# val_negative =  '/media/ytx/Japan_Deep_Data/dataset/LeiSang/myTry/BleedingDataDCM/val_negative_samples_unbuntu.tfrecords'\n",
    "\n",
    "train_positive_dataset = tf.data.TFRecordDataset(train_positive)\n",
    "train_negative_dataset = tf.data.TFRecordDataset(train_negative)\n",
    "\n",
    "val_positive_dataset = tf.data.TFRecordDataset(val_positive)\n",
    "val_negative_dataset = tf.data.TFRecordDataset(val_negative)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PREPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 400\n",
    "BATCH_SIZE = 2\n",
    "IMG_WIDTH = 256\n",
    "IMG_HEIGHT = 256\n",
    "\n",
    "# project_name\n",
    "project_name = \"6_100LoadModelSegCls256_DropDisplayConfuRandFlipScrach200Epoch/\"\n",
    "if not os.path.exists(project_name):\n",
    "    os.makedirs(project_name)\n",
    "# tb_log_name \n",
    "log_dir=project_name + \"AE_logs/\"\n",
    "\n",
    "# image_save name\n",
    "train_save_figure_path = project_name + \"AE_saves/train\"\n",
    "test_save_figure_path = project_name + \"AE_saves/test\"\n",
    "\n",
    "# training_checkpoint name\n",
    "checkpoint_dir = project_name + \"training_checkpoints\"\n",
    "\n",
    "# model architecture name\n",
    "encoder_path = project_name + \"Encoder.png\"\n",
    "decoder_path = project_name +\"Decoder.png\"\n",
    "autoencoder_path = project_name + \"Autoencoder.png\"\n",
    "new_model_path = project_name + \"new_model.png\"\n",
    "base_model_path = project_name + \"base_model.png\"\n",
    "classifier_path = project_name + \"classifier.png\"\n",
    "# gif save name\n",
    "anim_file = project_name + 'AE_saves.gif'\n",
    "\n",
    "# # dicom root \n",
    "# dicom_root = 'E:/dataset/Leisang/myTry/BleedingDataDCM'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math as m\n",
    "# import tensorflow_addons as tfa\n",
    "\n",
    "def resize(input, target):\n",
    "    print(input.shape)\n",
    "    print(target.shape)\n",
    "    resized_input = tf.image.resize(input, [IMG_HEIGHT, IMG_WIDTH],\n",
    "                                method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "    resized_target = tf.image.resize(target, [IMG_HEIGHT, IMG_WIDTH],\n",
    "                                method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "    resized_input =  tf.reshape(resized_input, [IMG_HEIGHT, IMG_WIDTH, 1])\n",
    "    resized_target =  tf.reshape(resized_target, [IMG_HEIGHT, IMG_WIDTH, 1])\n",
    "    return  resized_input, resized_target\n",
    "\n",
    "# @tf.function()\n",
    "def truncate(x, min, max):\n",
    "#     print(x.shape)\n",
    "    cliped =  tf.clip_by_value(x, min, max)\n",
    "    return cliped\n",
    " \n",
    "def norm(x, min, max):\n",
    "    # normalize_value = (value − min_value) / (max_value − min_value)\n",
    "    tensor = tf.math.divide(tf.subtract(x, min),\n",
    "                    tf.subtract(max, min))\n",
    "    return tensor\n",
    "\n",
    "def linear_normalization(input, min=30720.0, max=34816.0):\n",
    "    truncated_input = truncate(input, min, max)\n",
    "    norm_input = norm(truncated_input, min, max )\n",
    "    return  norm_input\n",
    "\n",
    "\n",
    "def random_crop(input_image, real_image):\n",
    "    stacked_image = tf.stack([input_image, real_image], axis=0)\n",
    "    cropped_image = tf.image.random_crop(\n",
    "      stacked_image, size=[2, IMG_HEIGHT, IMG_WIDTH, 1])  # crop to 256x256\n",
    "\n",
    "    return cropped_image[0], cropped_image[1]\n",
    "\n",
    "@tf.function()\n",
    "def random_jitter(input_image, real_image):\n",
    "    # randomly cropping to 256 x 256 x 3\n",
    "#     input_image, real_image = random_crop(input_image, real_image)\n",
    "\n",
    "    if tf.random.uniform(()) > 0.5:\n",
    "        # random mirroring\n",
    "        input_image = tf.image.flip_left_right(input_image)\n",
    "        real_image = tf.image.flip_left_right(real_image)\n",
    "\n",
    "    return input_image, real_image\n",
    "\n",
    "# @tf.function()\n",
    "# def Random_RotateInRnage(image, range_min=0, range_max = 30):\n",
    "    \n",
    "#     if tf.random.uniform(()) > 0.5: # rotate\n",
    "#         # angle\n",
    "#         PI = tf.constant(m.pi)\n",
    "# #         rotate_angel = random.random()/180*PI*angel\n",
    "#         angle = tf.random.uniform([], minval=range_min, maxval=range_max)\n",
    "# #         image = prep.random_rotation(image, rg=rotate_rg, \n",
    "# #                                      row_axis=0, col_axis=1, channel_axis=2)\n",
    "#         img =  tfa.image.rotate(image, angle)\n",
    "#     return img\n",
    "\n",
    "# def winwise(input,LB,HB):\n",
    "#         # 20 ,380 for range (-32768, 32767)\n",
    "#         # for tf input , (0, 65535)-? LB =  32788, 33148\n",
    "#         input[input<LB] = LB # low boundary , if < LW , set to LW\n",
    "#         input[input>HB] = HB # high boundary, if > Hw, Set to 255\n",
    "#         return input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parser the dataset to decode the features\n",
    "# Create a dictionary describing the features.\n",
    "image_feature_description = {\n",
    "        'dicom_path': tf.io.FixedLenFeature([], tf.string),\n",
    "        'seg_label': tf.io.FixedLenFeature([], tf.string), \n",
    "        'cls_label': tf.io.FixedLenFeature([], tf.int64),\n",
    "}\n",
    "\n",
    "@tf.function()\n",
    "def train__parse_image_function(example_proto):\n",
    "    # extract features # Parse the input tf.Example proto using the dictionary above.\n",
    "    parsed_features = tf.io.parse_single_example(example_proto, image_feature_description)\n",
    "       \n",
    "    # decode dicom\n",
    "    dicom_path = parsed_features[\"dicom_path\"]\n",
    "    image_bytes = tf.io.read_file(dicom_path)\n",
    "    input_image = tf.cast(tfio.image.decode_dicom_image(image_bytes, dtype=tf.uint16), tf.float32)\n",
    "    # decode mask\n",
    "    seg_label = tf.cast(tf.io.decode_raw(parsed_features['seg_label'], tf.uint8), tf.float32)\n",
    "    # reshape\n",
    "    seg_label = tf.reshape(seg_label, [-1, 512,512,1])\n",
    "    \n",
    "    \n",
    "    # preprocessing--->\n",
    "    # 1st resize\n",
    "    input_image, seg_label = resize(input_image, seg_label)\n",
    "    print(input_image.shape)\n",
    "    print(seg_label.shape)\n",
    "    \n",
    "    # 2nd random flip\n",
    "    input_image, seg_label = random_jitter(input_image, seg_label)\n",
    "    print(input_image.shape)\n",
    "    print(seg_label.shape)\n",
    "    \n",
    "    # Last normalize\n",
    "    input_image2 = linear_normalization(input_image) \n",
    "    seg_label =  seg_label/255.0\n",
    "    \n",
    "#     # 3nd random rotate\n",
    "#     rinput_imag2 = Random_RotateInRnage(input_image2)\n",
    "#     seg_label = Random_RotateInRnage(seg_label)\n",
    "    \n",
    "    #lei -window\n",
    "    input_w_image = linear_normalization(input_image,min=32788.0, max=33148.0)\n",
    "  \n",
    "    \n",
    "#     print(\"range of cliped_input: [{}, {}]\".format(np.min(input_image), np.max(input_image)))\n",
    "#     print(\"range of cliped_target: [{}, {}]\".format(np.min(seg_label), np.max(seg_label)))\n",
    "    \n",
    "    \n",
    "    return dicom_path, input_w_image ,input_image2, seg_label,  parsed_features[\"cls_label\"]\n",
    "\n",
    "@tf.function()\n",
    "def test__parse_image_function(example_proto):\n",
    "    # extract features # Parse the input tf.Example proto using the dictionary above.\n",
    "    parsed_features = tf.io.parse_single_example(example_proto, image_feature_description)\n",
    "       \n",
    "    # decode dicom\n",
    "    dicom_path = parsed_features[\"dicom_path\"]\n",
    "    image_bytes = tf.io.read_file(dicom_path)\n",
    "    input_image = tf.cast(tfio.image.decode_dicom_image(image_bytes, dtype=tf.uint16), tf.float32)\n",
    "    # decode mask\n",
    "    seg_label = tf.cast(tf.io.decode_raw(parsed_features['seg_label'], tf.uint8), tf.float32)\n",
    "    # reshape\n",
    "    seg_label = tf.reshape(seg_label, [-1, 512,512,1])\n",
    "    \n",
    "    \n",
    "    # preprocessing--->\n",
    "    # 1st resize\n",
    "    input_image, seg_label = resize(input_image, seg_label)\n",
    "    print(input_image.shape)\n",
    "    print(seg_label.shape)\n",
    "    \n",
    "#     # 2nd random flip\n",
    "#     input_image, seg_label = random_jitter(input_image, seg_label)\n",
    "#     print(input_image.shape)\n",
    "#     print(seg_label.shape)\n",
    "    \n",
    "    # Last normalize\n",
    "    input_image2 = linear_normalization(input_image) \n",
    "    seg_label =  seg_label/255.0\n",
    "    \n",
    "#     # 3nd random rotate\n",
    "#     rinput_imag2 = Random_RotateInRnage(input_image2)\n",
    "#     seg_label = Random_RotateInRnage(seg_label)\n",
    "    \n",
    "    #lei -window\n",
    "    input_w_image = linear_normalization(input_image,min=32788.0, max=33148.0)\n",
    "  \n",
    "    \n",
    "#     print(\"range of cliped_input: [{}, {}]\".format(np.min(input_image), np.max(input_image)))\n",
    "#     print(\"range of cliped_target: [{}, {}]\".format(np.min(seg_label), np.max(seg_label)))\n",
    "    \n",
    "    \n",
    "    return dicom_path, input_w_image ,input_image2, seg_label,  parsed_features[\"cls_label\"]\n",
    "\n",
    "\n",
    "# train: positive\n",
    "train_nb_pos = 3035\n",
    "train_nb_neg = 33588\n",
    "parsed_train_positive_dataset = train_positive_dataset.map(train__parse_image_function)\n",
    "parsed_train_negative_dataset = train_negative_dataset.map(train__parse_image_function)\n",
    "\n",
    "val_nb_pos = 238\n",
    "val_nb_neg = 4420\n",
    "parsed_val_positive_dataset = val_positive_dataset.map(test__parse_image_function)\n",
    "parsed_val_negative_dataset = val_negative_dataset.map(test__parse_image_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # check the dataset\n",
    "# def winwise(input,LB,HB):\n",
    "#         # 20 ,380 for range (-32768, 32767)\n",
    "#         # for tf input , (0, 65535)-? LB =  32788, 33148\n",
    "#         input[input<LB] = LB # low boundary , if < LW , set to LW\n",
    "#         input[input>HB] = HB # high boundary, if > Hw, Set to 255\n",
    "#         return input\n",
    "\n",
    "\n",
    "palette = copy(plt.cm.gray)\n",
    "palette.set_over('r', 1.0)\n",
    "for image_features in parsed_train_positive_dataset.take(2):\n",
    "    dicom_path =  image_features[0]\n",
    "    norm_input_w_image =  image_features[1]\n",
    "    norm_input =  image_features[2]\n",
    "    #     dicom_path = image_features[0].numpy()\n",
    "    norm_target = image_features[3]\n",
    "    cls_label = image_features[4]\n",
    "    print(\"dicom_path:\", dicom_path)\n",
    "    print(\"input_image.shape\", norm_input.shape)\n",
    "    print(\"seg_label\", norm_target.shape)\n",
    "    print(\"cls_label\", cls_label)\n",
    "    # reshape label \n",
    "#     norm_target =  tf.reshape(norm_target, norm_input.shape)\n",
    "    # # for lab ubuntu system\n",
    "    # input, target, image_path= load('/media/ytx/Japan_Deep_Data/dataset/LeiSang/myTry/BleedingDataDCM/train/ZA-006_000/00000001.DCM')\n",
    "    fig, axes = plt.subplots(2,2, figsize=(30,30))\n",
    "\n",
    "    axes[0, 0].imshow(np.squeeze(norm_input.numpy()), cmap='gray')\n",
    "    axes[0, 0].set_title('input range:[{}, {}]'.format((norm_input.numpy().min()), np.max(norm_input.numpy())))\n",
    "    axes[0, 1].imshow(np.squeeze(norm_target.numpy()), cmap='gray')\n",
    "    axes[0, 1].set_title('target range:[{}, {}]'.format(np.min(norm_target), np.max(norm_target)))\n",
    "    \n",
    "    axes[1, 0].imshow(np.squeeze(norm_input_w_image), cmap='gray')\n",
    "    axes[1, 0].set_title('Lei LB$HB：20.380：->  32788, 33148]')\n",
    "    \n",
    "    mask =  np.squeeze(norm_target.numpy())\n",
    "    masked_in = norm_target + norm_input\n",
    "    print('masked_in range:[{}, {}]'.format((masked_in.numpy().min()), masked_in.numpy().max()))\n",
    "\n",
    "    masked = np.ma.masked_where(norm_target==0, masked_in)  # this is only generated mask\n",
    "    print('masked range:[{}, {}]'.format(masked.min(), masked.max())) \n",
    "    axes[1, 1].imshow(np.squeeze(norm_input), cmap='gray')\n",
    "    axes[1, 1].imshow(np.squeeze(masked), palette, colors.Normalize(vmin=0, vmax=1), interpolation='none', alpha=0.4)\n",
    "    axes[1, 1].set_title('target range:[{}, {}]'.format(np.min(masked), np.max(masked)) + dicom_path)\n",
    "    fig.tight_layout()\n",
    "    \n",
    "    print(dicom_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DEFINE LOAD FUNCTION IN INPUT PIPELINE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## INPUT PIPELINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# balance the dataset\n",
    "BUFFER_SIZE = 1024\n",
    "BATCH_SIZE = 2\n",
    "train_dataset = tf.data.experimental.sample_from_datasets([parsed_train_positive_dataset, parsed_train_negative_dataset], weights=[0.5, 0.5])\n",
    "\n",
    "train_dataset = train_dataset.shuffle(BUFFER_SIZE)\n",
    "train_dataset = train_dataset.batch(BATCH_SIZE).prefetch(2)\n",
    "\n",
    "test_dataset = parsed_val_positive_dataset.concatenate(parsed_val_negative_dataset)\n",
    "# test_dataset = test_dataset.shuffle(BUFFER_SIZE)  # for the random check if not comment it\n",
    "test_dataset = test_dataset.batch(BATCH_SIZE).prefetch(2)\n",
    "\n",
    "\n",
    "# # test positive dataset\n",
    "# test_positive_dataset =  parsed_val_positive_dataset\n",
    "# test_positive_dataset = test_positive_dataset.batch(BATCH_SIZE).prefetch(2)\n",
    "# # test negative dataset\n",
    "# test_negative_dataset =  parsed_val_negative_dataset\n",
    "# test_negative_dataset = test_negative_dataset.batch(BATCH_SIZE).prefetch(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To use this dataset, you'll need the number of steps per epoch.\n",
    "\n",
    "The definition of \"epoch\" in this case is less clear. Say it's the number of batches required to see each positive example once:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resampled_steps_per_epoch = np.ceil(2.0*train_nb_pos/BATCH_SIZE)\n",
    "\n",
    "n_positive_test_batches = np.ceil(val_nb_pos/BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  check the dataset\n",
    "for image_features  in test_dataset.take(1):\n",
    "    dicom_path =  image_features[0][0]\n",
    "    norm_input_w_image =  image_features[1][0]\n",
    "    norm_input =  image_features[2][0]\n",
    "    #     dicom_path = image_features[0].numpy()\n",
    "    norm_target = image_features[3][0]\n",
    "    cls_label = image_features[4][0]\n",
    "    print(\"dicom_path:\", dicom_path)\n",
    "    print(\"input_image.shape\", norm_input.shape)\n",
    "    print(\"seg_label\", norm_target.shape)\n",
    "    print(\"cls_label\", cls_label)\n",
    "    \n",
    "    fig, axes = plt.subplots(2,2, figsize=(20,30))\n",
    "\n",
    "    axes[0, 0].imshow(np.squeeze(norm_input.numpy()), cmap='gray')\n",
    "    axes[0, 0].set_title('input range:[{}, {}]'.format((norm_input.numpy().min()), np.max(norm_input.numpy())))\n",
    "    axes[0, 1].imshow(np.squeeze(norm_target.numpy()), cmap='gray')\n",
    "    axes[0, 1].set_title('target range:[{}, {}]'.format(np.min(norm_target), np.max(norm_target)))\n",
    "    \n",
    "    axes[1, 0].imshow(np.squeeze(norm_input_w_image), cmap='gray')\n",
    "    axes[1, 0].set_title('Lei LB$HB：20.380：->  32788, 33148]')\n",
    "    \n",
    "    mask =  np.squeeze(norm_target.numpy())\n",
    "    masked_in = norm_target + norm_input\n",
    "    print('masked_in range:[{}, {}]'.format((masked_in.numpy().min()), masked_in.numpy().max()))\n",
    "\n",
    "    masked = np.ma.masked_where(norm_target==0, masked_in)  # this is only generated mask\n",
    "    print('masked range:[{}, {}]'.format(masked.min(), masked.max())) \n",
    "    axes[1, 1].imshow(np.squeeze(norm_input_w_image), cmap='gray')\n",
    "    axes[1, 1].imshow(np.squeeze(masked), palette, colors.Normalize(vmin=0, vmax=1), interpolation='none', alpha=0.4)\n",
    "    axes[1, 1].set_title('target range:[{}, {}]'.format(np.min(masked), np.max(masked)) + dicom_path)\n",
    "    fig.tight_layout()\n",
    "    \n",
    "    print(dicom_path)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DESIGN MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_CHANNELS = 1\n",
    "latent_dim =50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model input conponents\n",
    "en_inputs = tf.keras.layers.Input(shape=[IMG_HEIGHT,IMG_HEIGHT,1])\n",
    "de_inputs = tf.keras.layers.Input(shape=[latent_dim])\n",
    "# full model design and construct encoder, decoder , AE object\n",
    "# entire model\n",
    "\n",
    "# Define encoder part ---->\n",
    "x = en_inputs\n",
    "x = tf.keras.layers.Conv2D(\n",
    "                filters=16, kernel_size=3, strides=(1, 1), activation='relu', padding=\"same\")(x)\n",
    "x = tf.keras.layers.Conv2D(\n",
    "                filters=32, kernel_size=3, strides=(2, 2), activation='relu', padding=\"same\")(x)\n",
    "x = tf.keras.layers.Conv2D(\n",
    "                filters=64, kernel_size=3, strides=(2, 2), activation='relu', padding=\"SAME\")(x)\n",
    "x = tf.keras.layers.Conv2D(\n",
    "                filters=128, kernel_size=3, strides=(2, 2), activation='relu',padding=\"SAME\")(x)\n",
    "x = tf.keras.layers.Flatten()(x)\n",
    "# No activation\n",
    "latent_v = tf.keras.layers.Dense(latent_dim)(x)\n",
    "encoder = tf.keras.Model(inputs=en_inputs, outputs=latent_v, name='encoder')\n",
    "\n",
    "\n",
    "# Define decoder part ---->\n",
    "x = tf.keras.layers.Dense(units=131072, activation=tf.nn.relu)(de_inputs)\n",
    "x = tf.keras.layers.Reshape(target_shape=(32, 32, 128))(x)\n",
    "x = tf.keras.layers.Conv2DTranspose(\n",
    "      filters=64,\n",
    "      kernel_size=3,\n",
    "      strides=(2, 2),\n",
    "      padding=\"SAME\",\n",
    "      activation='relu')(x)\n",
    "x = tf.keras.layers.Conv2DTranspose(\n",
    "      filters=32,\n",
    "      kernel_size=3,\n",
    "      strides=(2, 2),\n",
    "      padding=\"SAME\",\n",
    "      activation='relu')(x)\n",
    "x = tf.keras.layers.Conv2DTranspose(\n",
    "      filters=16,\n",
    "      kernel_size=3,\n",
    "      strides=(2, 2),\n",
    "      padding=\"SAME\",\n",
    "      activation='relu')(x)\n",
    "\n",
    "decoded = tf.keras.layers.Conv2DTranspose(\n",
    "      filters=1, kernel_size=3, strides=(1, 1), padding=\"SAME\", use_bias=True)(x)\n",
    "# output with sigmoid\n",
    "decoded =  tf.sigmoid(decoded)\n",
    "decoder = tf.keras.Model(inputs=de_inputs, outputs=decoded, name='decoder')\n",
    "\n",
    "\n",
    "# Define AE model---->\n",
    "outputs = decoder(latent_v)\n",
    "autoencoder =  tf.keras.Model(inputs=en_inputs, outputs=outputs, name='AE')\n",
    "\n",
    "# a sperately decoder is struggle leave for the moment\n",
    "\n",
    "# # # create a placeholder for an encoded (32-dimensional) input\n",
    "# encoded_input = tf.keras.layers.Input(shape=(latent_dim))\n",
    "# # # retrieve the last layer of the autoencoder model\n",
    "# decoder_layer = autoencoder.layers[-1]\n",
    "# # # create the decoder model\n",
    "# decoder = tf.keras.Model(encoded_input, decoder_layer)\n",
    "             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.utils.plot_model(encoder, to_file=encoder_path, show_shapes=True, dpi=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.utils.plot_model(decoder, to_file=decoder_path, show_shapes=True, dpi=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.utils.plot_model(autoencoder, to_file=autoencoder_path, show_shapes=True, dpi=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OPTIMIZER AND OBJECTIVE LOSSES\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# BCE =  tf.keras.losses.BinaryCrossentropy() \n",
    "# Huber =  tf.keras.losses.Huber(delta=0.1)\n",
    "# @tf.function()\n",
    "# # define losses\n",
    "# def compute_loss(decoded_x, x):\n",
    "    \n",
    "#     # cross_entropy,  use reduce mean not sum, otherwise loss will be very big\n",
    "#     BCE_loss = BCE(y_true=x, y_pred=decoded_x) \n",
    "    \n",
    "    \n",
    "#     # L1 loss \n",
    "# #     L1_loss = tf.reduce_mean(tf.abs(x - decoded_x))\n",
    "    \n",
    "#     # Huber_loss\n",
    "#     Huber_loss = Huber(y_true=x, y_pred=decoded_x)\n",
    "#     total_loss = Huber_loss + BCE_loss\n",
    "#     return total_loss, BCE_loss, Huber_loss\n",
    "    \n",
    "# # appliy graidients  this is acutaully trianing step\n",
    "# # @tf.function()\n",
    "# # def compute_apply_gradients(model, x, optimizer, epoch):\n",
    "# #     decoded_x =  model(x)\n",
    "# #     with tf.GradientTape() as tape:\n",
    "# #         total_loss, CE_loss, L1_loss = compute_loss(decoded_x, x)\n",
    "# #     gradients = tape.gradient(total_loss, model.trainable_variables)\n",
    "# #     optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    \n",
    "# #     with summary_writer.as_default():\n",
    "# #         # write scalars to the tensorboard after each train step\n",
    "# #         tf.summary.scalar('total_loss', total_loss, step=epoch)\n",
    "# #         tf.summary.scalar('CE_loss', CE_loss, step=epoch)\n",
    "# #         tf.summary.scalar('L1_loss', L1_loss, step=epoch) \n",
    "# @tf.function()\n",
    "# def train_step(model, input, target, epoch):\n",
    "#     print(\"in trianing step\")\n",
    "#     with tf.GradientTape() as tape:  # very interesting\n",
    "#         decoded_img = model(input, training=True)\n",
    "#         total_loss, BCE_loss, Huber_loss = compute_loss(decoded_img, target)\n",
    "#     gradients = tape.gradient(total_loss, model.trainable_variables)\n",
    "#     optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "#     return total_loss, BCE_loss, Huber_loss\n",
    "\n",
    "# @tf.function()\n",
    "# def test_step(model, input, target, epoch):\n",
    "#     with tf.GradientTape() as tape:  # very interesting\n",
    "#         decoded_img = model(input, training=True)\n",
    "#         total_loss, BCE_loss, Huber_loss = compute_loss(decoded_img, target)\n",
    "#     return total_loss, BCE_loss, Huber_loss\n",
    "# #     train_avg_loss(train_loss)\n",
    "# #     train_avg_metric(metric)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # check the AE OUTPUT\n",
    "# print(\"inp.shape:\", cliped_norm_input.shape)\n",
    "# print(\"inp[tf.newaxis,...]:\", cliped_norm_input[tf.newaxis,...].shape)\n",
    "# AE_output = autoencoder(resized_input[tf.newaxis,...], training=False)  # inp is the image sample from cell code 6 ; \n",
    "# print(AE_output.shape)\n",
    "# plt.imshow(np.squeeze(AE_output[0,...]), cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training PREPARING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## get current working directory\n",
    "cwd = os.getcwd()\n",
    "print(\"current working directory:\", cwd)\n",
    "train_full_AE_saves =  os.path.join(cwd, train_save_figure_path)\n",
    "test_full_AE_saves =  os.path.join(cwd, test_save_figure_path)\n",
    "print(\"train_full_AE_saves:\", train_full_AE_saves)\n",
    "print(\"test_full_AE_saves:\", test_full_AE_saves)\n",
    "\n",
    "\n",
    "if not os.path.exists(train_full_AE_saves):\n",
    "    os.makedirs(train_full_AE_saves)\n",
    "\n",
    "if not os.path.exists(test_full_AE_saves):\n",
    "    os.makedirs(test_full_AE_saves)\n",
    "    \n",
    "    \n",
    "train_predictions_save_path =os.path.join(train_full_AE_saves, \"Predictions\")\n",
    "test_predictions_save_path =os.path.join(test_full_AE_saves, \"Predictions\")\n",
    "test_confusion_matrix =  os.path.join(test_full_AE_saves, \"Confusion_Matrix\")\n",
    "\n",
    "\n",
    "test_EpochValidation_save_path =os.path.join(test_full_AE_saves, \"EpochValidation\")\n",
    "test_EpochValidation_save_path_pred =os.path.join(test_EpochValidation_save_path, \"PredictionsOnly\")\n",
    "if not os.path.exists(train_predictions_save_path):\n",
    "    os.makedirs(train_predictions_save_path)\n",
    "if not os.path.exists(test_predictions_save_path):\n",
    "    os.makedirs(test_predictions_save_path)\n",
    "if not os.path.exists(test_EpochValidation_save_path):\n",
    "    os.makedirs(test_EpochValidation_save_path)\n",
    "if not os.path.exists(test_EpochValidation_save_path_pred):\n",
    "    os.makedirs(test_EpochValidation_save_path_pred)   \n",
    "if not os.path.exists(test_confusion_matrix):\n",
    "    os.makedirs(test_confusion_matrix)       \n",
    "    \n",
    "    \n",
    "def en_de_generate_images(en, de, norm_input, norm_input_w_image, norm_target, path, save=True, Train_or_not=True):\n",
    "#     prediction = model(test_input, training=True)\n",
    "    \n",
    "    laten_v = en(norm_input, training=False)\n",
    "    prediciton = de(laten_v)\n",
    "    print(\"prediciton shape:\", prediciton.shape)\n",
    "    \n",
    "    pred =  prediciton[0]\n",
    "    \n",
    "    norm_input =norm_input[0]\n",
    "    norm_input_w_image =norm_input_w_image[0]\n",
    "    norm_target =norm_target[0]\n",
    "    path = path[0]\n",
    "#     display_list = [np.squeeze(test_input[0]), np.squeeze(tar[0]), np.squeeze(prediction[0])]\n",
    "#     title = ['Input range:[{},{}]'.format(test_input[0].numpy().min(), test_input[0].numpy().max()), \n",
    "#            'GT range:[{},{}]'.format(tar[0].numpy().min(), tar[0].numpy().max()), 'en-to-de Pred. out {}'.format(prediction.shape)]\n",
    "    \n",
    "    fig, axes = plt.subplots(3,2, figsize =(20,30))\n",
    "    axes[0, 0].imshow(np.squeeze(norm_input.numpy()), cmap='gray')\n",
    "    axes[0, 0].set_title('input range:[{}, {}]'.format((norm_input.numpy().min()), np.max(norm_input.numpy())))\n",
    "    axes[0, 1].imshow(np.squeeze(norm_target.numpy()), cmap='gray')\n",
    "    axes[0, 1].set_title('target range:[{}, {}]'.format(np.min(norm_target), np.max(norm_target)))\n",
    "    \n",
    "    axes[1, 0].imshow(np.squeeze(norm_input_w_image), cmap='gray')\n",
    "    axes[1, 0].set_title('Lei LB$HB 20,380->  32788, 33148]')\n",
    "    \n",
    "    mask =  np.squeeze(norm_target.numpy())\n",
    "    masked_in = norm_target + norm_input\n",
    "    print('masked_in range:[{}, {}]'.format((masked_in.numpy().min()), masked_in.numpy().max()))\n",
    "\n",
    "    masked = np.ma.masked_where(norm_target==0, masked_in)  # this is only generated mask\n",
    "    print('masked range:[{}, {}]'.format(masked.min(), masked.max())) \n",
    "    axes[1, 1].imshow(np.squeeze(norm_input_w_image), cmap='gray')\n",
    "    \n",
    "    palette1 = copy(plt.cm.gray)\n",
    "    palette1.set_over('r', 1.0)\n",
    "    axes[1, 1].imshow(np.squeeze(masked), palette1, colors.Normalize(vmin=0, vmax=1), interpolation='none', alpha=0.4)\n",
    "    axes[1, 1].set_title('target range:[{}, {}]: \\n {}'.format(np.min(masked), np.max(masked), path))\n",
    "    \n",
    "    \n",
    "    \n",
    "    mask2 =  np.squeeze(pred.numpy())\n",
    "    axes[2, 0].imshow(mask2, cmap='gray')\n",
    "    axes[2, 0].set_title('Se Pred range:[{}, {}]; '.format(np.min(mask2), np.max(mask2)))\n",
    "    \n",
    "    \n",
    "    # binarize prediction\n",
    "    mask2[mask2>0.5] = 1\n",
    "    mask2[mask2<=0.5] = 0\n",
    "    \n",
    "    masked_in2 = mask2 + np.squeeze(norm_input)\n",
    "    print(\"norm_input_w_image.shape:\",norm_input_w_image.shape)\n",
    "    print(\"mask2 shape:\", mask2.shape)\n",
    "    print(\"masked_in2 shape:\", masked_in2.shape)\n",
    "    \n",
    "    print('masked_in range:[{}, {}]'.format((masked_in2.min()), masked_in2.max()))\n",
    "    \n",
    "    masked2 = np.ma.masked_where(mask2==0, masked_in2)  # this is only generated mask\n",
    "    print('masked range:[{}, {}]'.format(masked2.min(), masked2.max())) \n",
    "    axes[2, 1].imshow(np.squeeze(norm_input_w_image), cmap='gray')\n",
    "    palette2 = copy(plt.cm.gray)\n",
    "    palette2.set_over('y', 1.0)\n",
    "    axes[2, 1].imshow(np.squeeze(masked2), palette2, colors.Normalize(vmin=0, vmax=1), interpolation='none', alpha=0.4)\n",
    "    axes[2, 1].set_title('Seg Bi Pred range:[{}, {}]'.format(np.min(mask2), np.max(mask2)))\n",
    "    fig.tight_layout() \n",
    "    fig.subplots_adjust(wspace=0.0)\n",
    "#     axes.axis('off')\n",
    "    \n",
    "    \n",
    "#     fig.tight_layout()\n",
    "     \n",
    "    \n",
    "    \n",
    "    plt.figure(figsize=(8,8))\n",
    "    plt.title('Compare Pred(Yellow) & Label(red) ' )\n",
    "    plt.imshow(np.squeeze(norm_input_w_image), cmap='gray')\n",
    "    plt.imshow(np.squeeze(masked), palette1, colors.Normalize(vmin=0, vmax=1), interpolation='none', alpha=0.4)\n",
    "    palette.set_over('y', 1.0)\n",
    "    plt.imshow(np.squeeze(masked2), palette2, colors.Normalize(vmin=0, vmax=1), interpolation='none', alpha=0.4)\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    \n",
    "def new_generate_images(model, norm_input, norm_input_w_image, norm_target, cls_target, path, batch_idx, epoch, save=True, Train_or_not=True, Epoch_val =False):\n",
    "#     prediction = model(test_input, training=True)\n",
    "    \n",
    "    seg_out, class_sigmoid_output = model(norm_input, training=False)\n",
    "    print(\"seg_out shape:\", seg_out.shape)\n",
    "    \n",
    "    cls_pred =  np.argmax(class_sigmoid_output[0])\n",
    "    seg_pred =  seg_out[0]\n",
    "    \n",
    "    norm_input =norm_input[0]\n",
    "    norm_input_w_image =norm_input_w_image[0]\n",
    "    norm_target =norm_target[0]\n",
    "    cls_target = cls_target[0]\n",
    "    path = path[0]\n",
    "#     display_list = [np.squeeze(test_input[0]), np.squeeze(tar[0]), np.squeeze(prediction[0])]\n",
    "#     title = ['Input range:[{},{}]'.format(test_input[0].numpy().min(), test_input[0].numpy().max()), \n",
    "#            'GT range:[{},{}]'.format(tar[0].numpy().min(), tar[0].numpy().max()), 'en-to-de Pred. out {}'.format(prediction.shape)]\n",
    "    \n",
    "    fig, axes = plt.subplots(3,2, figsize =(20,30))\n",
    "    axes[0, 0].imshow(np.squeeze(norm_input.numpy()), cmap='gray')\n",
    "    axes[0, 0].set_title('input range:[{}, {}]'.format((norm_input.numpy().min()), np.max(norm_input.numpy())))\n",
    "    axes[0, 1].imshow(np.squeeze(norm_target.numpy()), cmap='gray')\n",
    "    axes[0, 1].set_title('target range:[{}, {}]'.format(np.min(norm_target), np.max(norm_target)))\n",
    "    \n",
    "    axes[1, 0].imshow(np.squeeze(norm_input_w_image), cmap='gray')\n",
    "    axes[1, 0].set_title('Lei LB$HB 20,380->  32788, 33148]')\n",
    "    \n",
    "    mask =  np.squeeze(norm_target.numpy())\n",
    "    masked_in = norm_target + norm_input\n",
    "    print('masked_in range:[{}, {}]'.format((masked_in.numpy().min()), masked_in.numpy().max()))\n",
    "\n",
    "    masked = np.ma.masked_where(norm_target==0, masked_in)  # this is only generated mask\n",
    "    print('masked range:[{}, {}]'.format(masked.min(), masked.max())) \n",
    "    axes[1, 1].imshow(np.squeeze(norm_input_w_image), cmap='gray')\n",
    "    \n",
    "    palette1 = copy(plt.cm.gray)\n",
    "    palette1.set_over('r', 1.0)\n",
    "    axes[1, 1].imshow(np.squeeze(masked), palette1, colors.Normalize(vmin=0, vmax=1), interpolation='none', alpha=0.4)\n",
    "    axes[1, 1].set_title('target range:[{}, {}]: \\n {}'.format(np.min(masked), np.max(masked), path))\n",
    "    \n",
    "    \n",
    "    \n",
    "    mask2 =  np.squeeze(seg_pred.numpy())\n",
    "    axes[2, 0].imshow(mask2, cmap='gray')\n",
    "    axes[2, 0].set_title('Seg Pred range:[{}, {}]; Cls Pred:{} Cls tar {}:'.format(np.min(mask2), np.max(mask2), cls_pred, cls_target.numpy() ))\n",
    "    \n",
    "    \n",
    "    # binarize prediction\n",
    "    mask2[mask2>0.5] = 1\n",
    "    mask2[mask2<=0.5] = 0\n",
    "    \n",
    "    masked_in2 = mask2 + np.squeeze(norm_input)\n",
    "    print(\"norm_input_w_image.shape:\",norm_input_w_image.shape)\n",
    "    print(\"mask2 shape:\", mask2.shape)\n",
    "    print(\"masked_in2 shape:\", masked_in2.shape)\n",
    "    \n",
    "    print('masked_in range:[{}, {}]'.format((masked_in2.min()), masked_in2.max()))\n",
    "    \n",
    "    masked2 = np.ma.masked_where(mask2==0, masked_in2)  # this is only generated mask\n",
    "    print('masked range:[{}, {}]'.format(masked2.min(), masked2.max())) \n",
    "    axes[2, 1].imshow(np.squeeze(norm_input_w_image), cmap='gray')\n",
    "    palette2 = copy(plt.cm.gray)\n",
    "    palette2.set_over('y', 1.0)\n",
    "    axes[2, 1].imshow(np.squeeze(masked2), palette2, colors.Normalize(vmin=0, vmax=1), interpolation='none', alpha=0.4)\n",
    "    axes[2, 1].set_title('Seg Bi Pred range:[{}, {}]; Cls Pred:{} Cls tar {}'.format(np.min(mask2), np.max(mask2), cls_pred,  cls_target.numpy()))\n",
    "    fig.tight_layout() \n",
    "    fig.subplots_adjust(wspace=0.0)\n",
    "#     axes.axis('off')\n",
    "    \n",
    "    \n",
    "#     fig.tight_layout()\n",
    "    \n",
    "    if save==True :\n",
    "        if Train_or_not:\n",
    "            fig.savefig(train_save_figure_path + \"/Train_image_at_epoch_{:04d}_batch_{}.png\".format(epoch,batch_idx))\n",
    "        else:\n",
    "            if Epoch_val:\n",
    "                fig.savefig(test_EpochValidation_save_path + \"/Test_image_at_epoch_{:04d}_batch_{}_idx.png\".format(epoch,batch_idx))\n",
    "            else:\n",
    "                fig.savefig(test_save_figure_path + \"/Test_image_at_epoch_{:04d}_batch_{}.png\".format(epoch,batch_idx))\n",
    "     \n",
    "    \n",
    "    \n",
    "    plt.figure(figsize=(12,12))\n",
    "    \n",
    "    plt.title('Compare Pred(Yellow) & Label(red) cls pred {}, cls tar {}\\n Path:{}'.format(cls_pred,  cls_target.numpy(), path) )\n",
    "    plt.imshow(np.squeeze(norm_input_w_image), cmap='gray')\n",
    "    plt.imshow(np.squeeze(masked), palette1, colors.Normalize(vmin=0, vmax=1), interpolation='none', alpha=0.4)\n",
    "    palette.set_over('y', 1.0)\n",
    "    plt.imshow(np.squeeze(masked2), palette2, colors.Normalize(vmin=0, vmax=1), interpolation='none', alpha=0.4)\n",
    "    plt.axis('off')\n",
    "    if save ==True:\n",
    "        if Train_or_not:\n",
    "            plt.savefig(train_save_figure_path + \"/Predictions/Train_pred_only_at_epoch_{:04d}_batch_{}.png\".format(epoch,batch_idx))\n",
    "        else:\n",
    "            if Epoch_val:\n",
    "                plt.savefig(test_EpochValidation_save_path_pred + \"/Test_pred_only_at_epoch_{:04d}_batch_{}.png\".format(epoch,batch_idx))\n",
    "            else:\n",
    "                plt.savefig(test_save_figure_path + \"/Predictions/Test_pred_only_at_epoch_{:04d}_batch_{}.png\".format(epoch,batch_idx))\n",
    "    \n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "epochs = 100\n",
    "# define opitmizer \n",
    "optimizer =  tf.keras.optimizers.Adam(1e-4)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# define check points\n",
    "\n",
    "\n",
    "# pre_saved_ckpt_path = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "# change to pre-saved model path: \n",
    "# 8 \n",
    "# pre_saved_ckpt_path = \"E:\\\\Projects\\\\logs\\dicoms\\\\fixedArranged\\\\fixed\\\\8Bleed_AE_Huber_BCE_Sigmoid_NormNarrow_ShuffleTr_RandWinTrTe_SaveBest\\\\training_checkpoints\\\\ckpt\"\n",
    "# # for ubuntu\n",
    "# pre_saved_ckpt_path = \"/media/ytx/Japan_Deep_Data/DicomProject2020/logs/NewFramework/fixed/8Bleed_AE_Huber_BCE_Sigmoid_NormNarrow_ShuffleTr_RandWinTrTe_SaveBest/training_checkpoints/ckpt\"\n",
    "# # the path for the new training\n",
    "# checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "\n",
    "# if not os.path.exists(pre_saved_ckpt_path):\n",
    "#     print(\"please check the pre-trained saved model path\")\n",
    "# #  contents of states to be saved as attributes on the checkpoint object\n",
    "# checkpoint_ob = tf.train.Checkpoint(step= tf.Variable(1),\n",
    "#                                     epoch=  tf.Variable(1),\n",
    "#                                     optimizer=optimizer,\n",
    "#                                  encoder=encoder,\n",
    "#                                  decoder =  decoder,\n",
    "#                                  autoencoder = autoencoder\n",
    "#                                  )\n",
    "# define restore checkpoint manager\n",
    "# manager =  tf.train.CheckpointManager(checkpoint_ob, pre_saved_ckpt_path, max_to_keep=1)\n",
    "\n",
    "# # define checkpoint manager for new training\n",
    "# manager =  tf.train.CheckpointManager(checkpoint_ob, checkpoint_prefix, max_to_keep=1)\n",
    "\n",
    "datetime_rec =  datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "train_summary_writer = tf.summary.create_file_writer(\n",
    "  log_dir + \"train\")\n",
    "val_summary_writer = tf.summary.create_file_writer(\n",
    "  log_dir + \"val\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start tensorboard\n",
    "# !kill 5032\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir {log_dir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load pre-trained model\n",
    "# checkpoint_ob.restore(restore_manager.latest_checkpoint)\n",
    "\n",
    "# step =  checkpoint_ob.step\n",
    "# epoch =  checkpoint_ob.epoch\n",
    "# print(int(step))\n",
    "# print(int(epoch))\n",
    "\n",
    "# AE=  checkpoint_ob.autoencoder\n",
    "# en = checkpoint_ob.encoder\n",
    "# de =  checkpoint_ob.decoder\n",
    "# print(AE)\n",
    "\n",
    "\n",
    "AE=  autoencoder\n",
    "en = encoder\n",
    "de =  decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## use the pre-saved encoder as the base model and design the new model\n",
    "base_model  = en\n",
    "print(base_model.trainable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model =  en # use encoder as the base model\n",
    "\n",
    "# check the loaded ae output\n",
    "for image_features in test_dataset.take(2):\n",
    "    dicom_path =  image_features[0]\n",
    "    norm_input_w_image =  image_features[1]\n",
    "    norm_input =  image_features[2]\n",
    "    #     dicom_path = image_features[0].numpy()\n",
    "    norm_target = image_features[3]\n",
    "    cls_label = image_features[4]\n",
    "    print(\"dicom_path:\", dicom_path)\n",
    "    print(\"input_image.shape\", norm_input.shape)\n",
    "    print(\"seg_label\", norm_target.shape)\n",
    "    print(\"cls_label\", cls_label)\n",
    "\n",
    "    \n",
    "    # check the loaded encoder and decoder: encoder,decoder, norm_input, norm_input_w_image, norm_target, path, batch_idx, epoch, save=True\n",
    "    en_de_generate_images(base_model, de,norm_input, norm_input_w_image,norm_target, dicom_path)\n",
    "\n",
    "# check the base base model traininable\n",
    "print(base_model.trainable)\n",
    "\n",
    "# \n",
    "trainable_or_not =  True\n",
    "\n",
    "if trainable_or_not == True:\n",
    "    base_model.trainable =  True # set encoder untranable\n",
    "print(\"after reset basemodel trainable property:\", base_model.trainable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the trainable model architecture\n",
    "base_model.summary()\n",
    "tf.keras.utils.plot_model(base_model, to_file=base_model_path, show_shapes=True, dpi=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add classification head for bleed exist or not\n",
    "print(\"# of layers in the base_model:\", len(base_model.layers))\n",
    "\n",
    "class_dropout = tf.keras.layers.Dropout(0.5)(base_model.layers[-1].output)\n",
    "class_head_nodes = tf.keras.layers.Dense(2)(class_dropout)\n",
    "class_softmax_output =  tf.nn.softmax(class_head_nodes)\n",
    "print(base_model.output)\n",
    "print(base_model.layers[-1].output)\n",
    "Classifier =  tf.keras.Model(inputs=en_inputs, outputs=class_softmax_output, name='classifier_head')\n",
    "# check the architecture\n",
    "Classifier.summary()\n",
    "tf.keras.utils.plot_model(Classifier, to_file=classifier_path, show_shapes=True, dpi=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set THE SETMENTATION PART\n",
    "# 1st take out the feature from the base_model\n",
    "base_feature =  base_model.layers[-3].output\n",
    "print(base_feature)\n",
    "\n",
    "\n",
    "# define down_sample block as the pix-2-pix from tensorflow official set\n",
    "def downsample(filters, size, apply_batchnorm=True):\n",
    "    initializer = tf.random_normal_initializer(0., 0.02)\n",
    "\n",
    "    result = tf.keras.Sequential()\n",
    "    result.add(\n",
    "      tf.keras.layers.Conv2D(filters, size, strides=2, padding='same',\n",
    "                             kernel_initializer=initializer, use_bias=False))\n",
    "\n",
    "    if apply_batchnorm:\n",
    "        result.add(tf.keras.layers.BatchNormalization())\n",
    "\n",
    "    result.add(tf.keras.layers.LeakyReLU())\n",
    "\n",
    "    return result\n",
    "\n",
    "def upsample(filters, size, apply_dropout=False):\n",
    "    initializer = tf.random_normal_initializer(0., 0.02)\n",
    "\n",
    "    result = tf.keras.Sequential()\n",
    "    result.add(\n",
    "    tf.keras.layers.Conv2DTranspose(filters, size, strides=2,\n",
    "                                    padding='same',\n",
    "                                    kernel_initializer=initializer,\n",
    "                                    use_bias=False))\n",
    "\n",
    "    result.add(tf.keras.layers.BatchNormalization())\n",
    "\n",
    "    if apply_dropout:\n",
    "        result.add(tf.keras.layers.Dropout(0.5))\n",
    "\n",
    "    result.add(tf.keras.layers.ReLU())\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "# def skips needed to be connected\n",
    "skips = []\n",
    "\n",
    "def en_for_seg():\n",
    "# new another path for the segmentation of the new model\n",
    "    x = downsample(64, 4, apply_batchnorm=False)(en_inputs) # output  (None, 128, 128, 64)\n",
    "    skips.append(x)\n",
    "    x = downsample(128, 4)(x)  # output (None, 64, 64, 128)\n",
    "    skips.append(x)\n",
    "    x = downsample(256, 4)(x)  # output (None, 32, 32, 256)\n",
    "    skips.append(x)\n",
    "    # if want seperate train the generator, return the Model object e.g. return tf.keras.Model(inputs=inputs, outputs=x)\n",
    "    return tf.keras.Model(inputs=en_inputs, outputs=x)\n",
    "\n",
    "en_coder_for_seg_out =  en_for_seg()\n",
    "print(en_coder_for_seg_out.output)\n",
    "tf.keras.utils.plot_model(en_coder_for_seg_out, to_file=\"encodre_for_seg.png\", show_shapes=True, dpi=128)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# concatenate the feature from base and feature from the new enocoder\n",
    "concate_features =  tf.keras.layers.Concatenate()([base_feature, en_coder_for_seg_out.output])\n",
    "print(concate_features)\n",
    "\n",
    "# start to one more step to encoder\n",
    "further_en = downsample(512, 4)(concate_features)  # output (None, 16, 16, 512)\n",
    "print(\"one more step encoding:\",further_en)\n",
    "\n",
    "# global pooling for that one more step encoding fetures\n",
    "GA  = tf.keras.layers.GlobalAveragePooling2D()(further_en)  # squeeze\n",
    "print(\"GA:\", GA)\n",
    "# RESHAPE GA  TO THE IMAGE FORMMAT\n",
    "RESHAPE_GA =  tf.reshape(GA, [-1,1,1, GA.shape[-1]])\n",
    "print(\"RESHAPE_GA:\", RESHAPE_GA)\n",
    "#resize to the feature maps size\n",
    "resize_GA =  tf.image.resize(RESHAPE_GA, (further_en.shape[1], further_en.shape[2]))\n",
    "print(\"resized GA:\", resize_GA)\n",
    "# concate future_en and reisze_GA\n",
    "bottom_concate =  tf.keras.layers.Concatenate()([resize_GA, further_en])\n",
    "print(\"bottom_concate:\", bottom_concate)\n",
    "\n",
    "print()\n",
    "print(\"start to upsample--------->\")\n",
    "# start to upsample and decoder from resize_GA and further_en\n",
    "up_feature1 =  upsample(512, 4) (bottom_concate) #  upsample/relarge ti [None, 32, 32, 1024] without dropout default false for drop\n",
    "print(\"up_feature1:\", up_feature1)\n",
    "up_concate1 =  tf.keras.layers.Concatenate()([up_feature1, skips[-1]])\n",
    "print(\"up_concate1:\", up_concate1)\n",
    "up_feature2 =  upsample(256, 4) (up_concate1) #  upsample/relarge ti [None, 64, 64, 256] without dropout default false for drop\n",
    "print(\"up_feature2:\", up_feature2)\n",
    "up_concate2 =  tf.keras.layers.Concatenate()([up_feature2, skips[-2]])\n",
    "print(\"up_concate2:\", up_concate2)\n",
    "up_feature3 =  upsample(128, 4) (up_concate2) #  upsample/relarge ti [None, 128, 128, 128] without dropout default false for drop\n",
    "print(\"up_feature3:\", up_feature2)\n",
    "up_concate3 =  tf.keras.layers.Concatenate()([up_feature3, skips[-3]])\n",
    "print(\"up_concate3:\", up_concate3)\n",
    "\n",
    "# last layer output\n",
    "initializer = tf.random_normal_initializer(0., 0.02)\n",
    "last = tf.keras.layers.Conv2DTranspose(OUTPUT_CHANNELS, 4,\n",
    "                                         strides=2, # stride 2\n",
    "                                         padding='same',\n",
    "                                         kernel_initializer=initializer,\n",
    "                                         activation='sigmoid') # (bs, 256, 256, 3)\n",
    "seg_out  =  last(up_concate3)\n",
    "\n",
    "Final_seg_model = tf.keras.Model(inputs=en_inputs, outputs=[seg_out, class_softmax_output])\n",
    "tf.keras.utils.plot_model(Final_seg_model, to_file=new_model_path, show_shapes=True, dpi=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check new model\n",
    "# check the AE OUTPUT\n",
    "# print(\"inp.shape:\", cliped_norm_input.shape)\n",
    "# print(\"inp[tf.newaxis,...]:\", cliped_norm_input[tf.newaxis,...].shape)\n",
    "# AE_output = autoencoder(resized_input[tf.newaxis,...], training=False)  # inp is the image sample from cell code 6 ; \n",
    "# print(AE_output.shape)\n",
    "# plt.imshow(np.squeeze(AE_output[0,...]), cmap=\"gray\")\n",
    "\n",
    "# check the loaded ae output\n",
    "for image_features in test_dataset.take(2):\n",
    "    dicom_path =  image_features[0]\n",
    "    norm_input_w_image =  image_features[1]\n",
    "    norm_input =  image_features[2]\n",
    "    #     dicom_path = image_features[0].numpy()\n",
    "    norm_target = image_features[3]\n",
    "    cls_label = image_features[4]\n",
    "    print(\"dicom_path:\", dicom_path)\n",
    "    print(\"input_image.shape\", norm_input.shape)\n",
    "    print(\"seg_label\", norm_target.shape)\n",
    "    print(\"cls_label\", cls_label)\n",
    "\n",
    " \n",
    "    \n",
    "    new_model_output =  Final_seg_model(norm_input)\n",
    "    # outputs=[seg_out, class_sigmoid_output]\n",
    "    output1=   new_model_output[0]\n",
    "    output2=   new_model_output[1]\n",
    "    print(\"output1 shape:\", output1.shape)  \n",
    "    print(\"output2 shape:\", output2.shape)  \n",
    "#     test_total_loss, test_BCE_loss, test_Huber_loss = test_step(AE, TEST_input, TEST_target, epoch) \n",
    "#     generate_images(AE, TEST_input, TEST_target,TEST_dicom_path,test_total_loss.numpy(), int(step), int(epoch), save=False)\n",
    "    plt.imshow(np.squeeze(output1[0,...]), cmap=\"gray\")\n",
    "#     # check the loaded encoder and decoder\n",
    "#     en_de_generate_images(en, de,TEST_input, TEST_target,TEST_dicom_path, int(step), int(epoch), save=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# design new loss for the new model as new_train_step, new_test_step\n",
    "new_optimizer =  tf.keras.optimizers.Adam(1e-4)\n",
    "# from tf.keras.utils import to_categorical\n",
    "BCE =  tf.keras.losses.BinaryCrossentropy() \n",
    "Huber =  tf.keras.losses.Huber(delta=0.1)\n",
    "SCC = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "@tf.function()\n",
    "# define losses\n",
    "def new_compute_loss(pred_seg, gt_seg, pred_cls, gt_cls):\n",
    "    \n",
    "    #seg  cross_entropy,  use reduce mean not sum, otherwise loss will be very big\n",
    "    seg_BCE_loss = BCE(y_true=gt_seg, y_pred=pred_seg) \n",
    "    \n",
    "    \n",
    "    # L1 loss \n",
    "#     L1_loss = tf.reduce_mean(tf.abs(x - decoded_x))\n",
    "    \n",
    "    #seg Huber_loss\n",
    "    seg_Huber_loss = Huber(y_true=gt_seg, y_pred=pred_seg)\n",
    "    \n",
    "    seg_total_loss = seg_Huber_loss + seg_BCE_loss\n",
    "    \n",
    "    \n",
    "    # use tf.keras.losses.SparseCategoricalCrossentropy instead of BCE , no need one-hot tranfermat\n",
    "    cls_SCC_loss=  SCC(y_true=gt_cls, y_pred=pred_cls)\n",
    "    \n",
    "    # total_loss = cls_BCE_loss + seg_loss\n",
    "    total_loss = cls_SCC_loss + seg_total_loss\n",
    "    return total_loss, cls_SCC_loss, seg_total_loss\n",
    "    \n",
    "\n",
    "# def mask_to_categorical(cls_tar, num_cls=2):\n",
    "#     tar = tf.one_hot(tf.cast(cls_tar, tf.int32), num_cls)\n",
    "#     tar = tf.cast(tar, tf.float32)\n",
    "#     return tar\n",
    "\n",
    "@tf.function()\n",
    "def new_train_step(model, input, seg_target, cls_target,epoch, training):\n",
    "    # change cls labe 0 or 1 into  [1, 0] [0, 1]\n",
    "    print(cls_target)\n",
    "#     cls_target = mask_to_categorical(cls_target)\n",
    "#     print(\"in trianing step\")\n",
    "    with tf.GradientTape() as tape:  # very interesting\n",
    "        # outputs=[seg_out, class_sigmoid_output] \n",
    "        seg_out, class_softmax_output= model(input, training=training)\n",
    "        total_loss, cls_SCC_loss, seg_total_loss = new_compute_loss(pred_seg=seg_out,\n",
    "                                                            gt_seg=seg_target, \n",
    "                                                            pred_cls=class_softmax_output, \n",
    "                                                            gt_cls=cls_target)\n",
    "    gradients = tape.gradient(total_loss, model.trainable_variables)\n",
    "    new_optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    return total_loss, cls_SCC_loss, seg_total_loss\n",
    "\n",
    "@tf.function()\n",
    "def new_test_step(model2, input2, seg_target2, cls_target2, epoch2, training2):\n",
    "    with tf.GradientTape() as tape:  # very interesting\n",
    "        seg_out2, class_softmax_output2= model2(input2, training=training2)\n",
    "        total_loss2, cls_SCC_loss2, seg_total_loss2 = new_compute_loss(pred_seg=seg_out2,\n",
    "                                                            gt_seg=seg_target2, \n",
    "                                                            pred_cls=class_softmax_output2, \n",
    "                                                            gt_cls=cls_target2)\n",
    "    return total_loss2, cls_SCC_loss2, seg_total_loss2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion matrix define\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install scikit-learn\n",
    "import sklearn.metrics\n",
    "import itertools\n",
    "def plot_confusion_matrix(cm, class_names, epoch, save=True):\n",
    "    \"\"\"\n",
    "    Returns a matplotlib figure containing the plotted confusion matrix.\n",
    "\n",
    "    Args:\n",
    "    cm (array, shape = [n, n]): a confusion matrix of integer classes\n",
    "    class_names (array, shape = [n]): String names of the integer classes e.g, [\"dog\", \"cat\"]\n",
    "    \"\"\"\n",
    "    figure = plt.figure(figsize=(8, 8))\n",
    "#     plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    plt.title(\"Confusion matrix(range[{}, {}])\".format(cm.min(), cm.max()))\n",
    "    \n",
    "     # Normalize the confusion matrix.\n",
    "    norm_cm = np.around(cm.astype('float') / cm.sum(axis=1)[:, np.newaxis], decimals=2)\n",
    "    plt.imshow(norm_cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "     # Use white text if squares are dark; otherwise black.\n",
    "    threshold = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        color = \"white\" if cm[i, j] > threshold else \"black\"\n",
    "        plt.text(j, i, \"{}({})\".format(cm[i, j],norm_cm[i, j]), horizontalalignment=\"center\", color=color)\n",
    "    plt.colorbar()  # rember the jupyter lab will remember the plt color operation so the coloar may related to the normalized value if not clean \n",
    "    tick_marks = np.arange(len(class_names))\n",
    "    plt.xticks(tick_marks, class_names, rotation=45)\n",
    "    plt.yticks(tick_marks, class_names, rotation=45)\n",
    "\n",
    "#     # Normalize the confusion matrix.\n",
    "#     cm = np.around(cm.astype('float') / cm.sum(axis=1)[:, np.newaxis], decimals=2)\n",
    "\n",
    "#     # Use white text if squares are dark; otherwise black.\n",
    "#     threshold = cm.max() / 2.\n",
    "#     for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "#         color = \"white\" if cm[i, j] > threshold else \"black\"\n",
    "#         plt.text(j, i, cm[i, j], horizontalalignment=\"center\", color=color)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label', labelpad=0)\n",
    "    plt.xlabel('Predicted label')\n",
    "    if save:    \n",
    "        plt.savefig(test_confusion_matrix + \"/Test_cm_at_epoch_{:04d}.png\".format(epoch))\n",
    "    else:\n",
    "        pass\n",
    "    return figure\n",
    "\n",
    "# define confusion matrix for classification displaying\n",
    "def log_confusion_matrix_all(model,test_inputs, class_names,cls_tar, epoch, save):\n",
    "    # Use the model to predict the values from the validation dataset.\n",
    "    seg_out, class_softmax_output = model.predict(test_inputs)\n",
    "    test_pred = np.argmax(class_softmax_output, axis=1)\n",
    "    \n",
    "    \n",
    "    # Calculate the confusion matrix.\n",
    "    cm = sklearn.metrics.confusion_matrix(cls_tar, test_pred)\n",
    "    # Log the confusion matrix as an image summary.\n",
    "\n",
    "    figure = plot_confusion_matrix(cm, class_names, epoch, batch_indx, save)\n",
    "#     cm_image = plot_to_image(figure)\n",
    "\n",
    "\n",
    "def return_cls_preds_tars(single_cls_pred, single_cls_tars):\n",
    "    print(\"single_cls_pred:\", single_cls_pred)\n",
    "    print(\"single_cls_tars:\", single_cls_tars)\n",
    "\n",
    "# define confusion matrix for classification displaying\n",
    "def log_confusion_matrix_without_model(cls_preds, class_names,cls_tars, epoch, save):\n",
    "    # Use the model to predict the values from the validation dataset.\n",
    "#     seg_out, class_softmax_output = model.predict(test_inputs)\n",
    "    cls_preds_ints = np.argmax(cls_preds, axis=1)\n",
    "    print(\"cls_preds_ints:\", cls_preds_ints, cls_preds_ints.shape)\n",
    "    print(\"cls_tars:\", cls_tars, cls_tars.shape)\n",
    "    \n",
    "    # Calculate the confusion matrix.\n",
    "    cm = sklearn.metrics.confusion_matrix(cls_tars, cls_preds_ints,  labels=[0, 1])\n",
    "    print(\"cm:\", cm)\n",
    "    # Log the confusion matrix as an image summary.\n",
    "\n",
    "    figure = plot_confusion_matrix(cm, class_names, epoch, save)\n",
    "#     cm_image = plot_to_image(figure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_losses_tb(writer, avg_losses, epoch):\n",
    "    \"\"\"\n",
    "     avg_losses = [total_loss, seg_loss, cls_loss]\n",
    "    \"\"\"\n",
    "    with writer.as_default():\n",
    "        print(\"writing train logs to tensorboard...\")                                                       \n",
    "        # write scalars to the tensorboard after each train step\n",
    "        tf.summary.scalar('total_loss', avg_losses[0].result(), step=epoch)\n",
    "        tf.summary.scalar('seg_total_loss', avg_losses[1].result(), step=epoch)\n",
    "        tf.summary.scalar('cls_SCE_loss', avg_losses[2].result(), step=epoch)\n",
    "\n",
    "\n",
    "def train_display_save_at(model, Tr_norm_input, Tr_norm_input_w_image, Tr_norm_target, Tr_cls_label, Tr_dicom_path,  step, epoch, freq_step =100, save=True, Train_or_not=True):\n",
    "    if step % freq_step == 0:\n",
    "        print(\"train image checking----------------------------------------------------------------------------------->\")\n",
    "        new_generate_images(model, Tr_norm_input, Tr_norm_input_w_image, Tr_norm_target, Tr_cls_label, Tr_dicom_path,  step, epoch, save=save, Train_or_not=Train_or_not)\n",
    "\n",
    "#         print('Epoch{}: {}/{}: total_loss: {}; cls_SCE_loss: {}; seg_total_loss: {} '.format(epoch, step, resampled_steps_per_epoch, total_loss, cls_SCE_loss, seg_total_loss))\n",
    "        \n",
    "def test_display_save_at(val_dataset, model, step, epoch, freq_step= 100, save=True, Train_or_not=False):\n",
    "    # every 100 batch step show the generate test image to check.\n",
    "    if int(step) % freq_step == 0:\n",
    "        display.clear_output(wait=True)\n",
    "           \n",
    "        # take the 1st batch from the validation dataset\n",
    "        for image_features2 in val_dataset.take(1):\n",
    "            TE_dicom_path2 =  image_features2[0]\n",
    "            TE_norm_input_w_image2 =  image_features2[1]\n",
    "            TE_norm_input2 =  image_features2[2]\n",
    "            TE_norm_target2 = image_features2[3]\n",
    "            TE_cls_label2 = image_features2[4]\n",
    "#           \n",
    "            print(\"The 1st batch test image checking----------------------------------------------------------------------------------->\")\n",
    "            new_generate_images(model, TE_norm_input2, TE_norm_input_w_image2, TE_norm_target2, TE_cls_label2, TE_dicom_path2,  step, epoch, save=save, Train_or_not=Train_or_not)\n",
    "            # every 400 steps save the genrate images\n",
    "\n",
    "\n",
    "\n",
    "def test_at_each_epoch(val_dataset, model, step, epoch, temp_loss, save=True, training=False):\n",
    "    # initialize \n",
    "    epoch_val_flag = True\n",
    "    # initialize for evaluating average metric\n",
    "    test_total_loss_mean = tf.keras.metrics.Mean()\n",
    "    test_seg_total_loss_mean = tf.keras.metrics.Mean()\n",
    "    test_cls_SCE_loss_mean = tf.keras.metrics.Mean()\n",
    "\n",
    "    # \n",
    "    cls_preds_entire = None\n",
    "    cls_tar_entire = None\n",
    "    \n",
    "    #\n",
    "\n",
    "\n",
    "    idx = 0\n",
    "    # start to go through entire validation dataset\n",
    "    for  image_features3 in val_dataset.take(10):     # just for debug   $$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
    "                                          \n",
    "#     for  idx, image_features3 in enumerate(val_dataset):\n",
    "        # feature extraction\n",
    "        TE_dicom_path3 =  image_features3[0]\n",
    "        TE_norm_input_w_image3 =  image_features3[1]\n",
    "        TE_norm_input3 =  image_features3[2]\n",
    "        TE_norm_target3 = image_features3[3]\n",
    "        TE_cls_label3 = image_features3[4]\n",
    "        \n",
    "        # one loss evluation : new_test_step(model, input, seg_target, cls_target, epoch)\n",
    "        test_total_loss, test_cls_SCE_loss, test_seg_total_loss = new_test_step(model, TE_norm_input3, TE_norm_target3, TE_cls_label3, epoch, training2=training)\n",
    "        \n",
    "        test_total_loss_mean(test_total_loss)\n",
    "        test_seg_total_loss_mean(test_seg_total_loss)\n",
    "        test_cls_SCE_loss_mean(test_cls_SCE_loss)\n",
    "\n",
    "        # save epoch validation image in the folder every 10 batches save\n",
    "        \n",
    "#         if idx >10:  # just for debug   $$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
    "#             break    # just for debug   $$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
    "        \n",
    "        if idx > n_positive_test_batches:  # when  evaluating negative val dataset using larger spacing\n",
    "            \n",
    "            if idx % 100 ==0:\n",
    "                new_generate_images(model, TE_norm_input3, TE_norm_input_w_image3, TE_norm_target3, TE_cls_label3, TE_dicom_path3,  \n",
    "                                    idx, epoch, save=save, Train_or_not=training, Epoch_val=epoch_val_flag)\n",
    "           \n",
    "        else:\n",
    "            new_generate_images(model, TE_norm_input3, TE_norm_input_w_image3, TE_norm_target3, TE_cls_label3, TE_dicom_path3,  idx, \n",
    "                                epoch, save=save, Train_or_not=training, Epoch_val=epoch_val_flag)                                               \n",
    "        display.clear_output(wait=True)\n",
    "\n",
    "         # Test input--------------batch input and targets to evaluate confusion matrix====================》\n",
    "        _, cls_preds =model.predict(TE_norm_input3) # predicts has shape [batch size, len(classes)]\n",
    "        print(\"single batch preds:\", cls_preds.shape)\n",
    "        if cls_preds_entire is None:\n",
    "            cls_preds_entire = cls_preds\n",
    "            cls_tar_entire = TE_cls_label3.numpy()\n",
    "#                   \n",
    "        else: \n",
    "            cls_preds_entire =  np.concatenate((cls_preds_entire, cls_preds), axis=0)\n",
    "            cls_tar_entire = np.concatenate((cls_tar_entire, TE_cls_label3.numpy()), axis=0)\n",
    "            \n",
    "        idx +=1       # just for debug   $$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
    "            \n",
    "    print(\"preds_entire.shape\", cls_preds_entire)\n",
    "    print(\"cls_entire.shape\", cls_tar_entire)   # https://www.tensorflow.org/tensorboard/image_summaries\n",
    "#                 return_cls_preds_tars(single_cls_pred, single_cls_tars)\n",
    "#             # check initial confusion matrix\n",
    "\n",
    "    log_confusion_matrix_without_model(cls_preds=cls_preds_entire, class_names=[\"No bleed\", \"bleed\"], cls_tars=cls_tar_entire, epoch=epoch, save=save)\n",
    "    #\n",
    "\n",
    "    print('Epoch: {}, Test total_loss set loss: {},  seg_loss: {},  cls_loss: {}'.format(epoch,\n",
    "                                                test_total_loss_mean.result(), test_seg_total_loss_mean.result(), test_cls_SCE_loss_mean.result()))\n",
    "\n",
    "    epoch_val_flag = False # one epoch finish so set flag back\n",
    "    \n",
    "    \n",
    "    # write logs to tensorboard:    avg_losses = [total_loss, seg_loss, cls_loss]\n",
    "    # write test losses to the tensorboard\n",
    "    print(\"writing test logs to tensorboard...\") \n",
    "    write_losses_tb(val_summary_writer, [test_total_loss_mean, test_seg_total_loss_mean, test_cls_SCE_loss_mean], epoch)\n",
    "\n",
    "  \n",
    "    # check whether needs to save the model\n",
    "    if test_total_loss_mean.result() < temp_loss:\n",
    "        save_path =  new_manager.save() # save the checkpoint and return the save path\n",
    "        print(\"Saved checkpoint for epoch {}-  step {}: {}\".format(epoch, step, save_path))\n",
    "        test_avg_tmp_loss =  test_total_loss_mean.result()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_process(train_dataset, val_dataset, step, start_epoch, total_epochs):\n",
    "    # some initial parameters\n",
    "    # test average_loss = for saving the test results\n",
    "    test_avg_tmp_loss = 100\n",
    "       \n",
    "    \n",
    "    \n",
    "    for epoch in range(int(start_epoch), total_epochs+1):\n",
    "        # initializations at each epoch\n",
    "        \n",
    "        train_total_loss_mean = tf.keras.metrics.Mean()\n",
    "        train_seg_total_loss_mean = tf.keras.metrics.Mean()\n",
    "        train_cls_SCC_loss_mean = tf.keras.metrics.Mean()\n",
    "     \n",
    "        \n",
    "        for image_features0 in train_dataset:\n",
    "            # train sample extraction\n",
    "            TR_dicom_path =  image_features0[0]\n",
    "            TR_norm_input_w_image =  image_features0[1]\n",
    "            TR_norm_input =  image_features0[2]\n",
    "            TR_norm_target = image_features0[3]\n",
    "            TR_cls_label = image_features0[4]\n",
    "\n",
    "            # one train step loss calculation and optimization\n",
    "            total_loss, cls_SCE_loss, seg_total_loss = new_train_step(Final_seg_model, TR_norm_input, TR_norm_target, TR_cls_label, epoch, training=True)\n",
    "            train_total_loss_mean(total_loss)\n",
    "            train_seg_total_loss_mean(seg_total_loss)\n",
    "            train_cls_SCC_loss_mean(cls_SCE_loss)\n",
    "            \n",
    "\n",
    "            # display and save training image with results\n",
    "            train_display_save_at(Final_seg_model, TR_norm_input, TR_norm_input_w_image, TR_norm_target, TR_cls_label, TR_dicom_path,  int(step), epoch, freq_step=100, save=True, Train_or_not=True )\n",
    "\n",
    "            \n",
    "            # test check at some step\n",
    "            test_display_save_at(val_dataset, Final_seg_model, int(step), epoch, freq_step= 1000, save=True, Train_or_not=False)\n",
    "                \n",
    "            \n",
    "            # if step > positive number of samples, training epoch finished else, continous for the current epoch training and add the step with 1\n",
    "            if step > 5:                                 # just for debug   $$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
    "                print(\"one epoch finihsed\")              # just for debug   $$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$        \n",
    "                step.assign_add(1)                       # just for debug   $$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
    "                break\n",
    "    #         if step > resampled_steps_per_epoch:\n",
    "                \n",
    "    #             break\n",
    "            else:\n",
    "                print(\"one epoch finihsed\")\n",
    "                step.assign_add(1)\n",
    "         \n",
    "        \n",
    "            \n",
    "            \n",
    "        # finihsed epoch and start epoch validaion : save images for the entire validation dataset(including all positives, but some negatives), tensorboard log writing, whether to save the model\n",
    "        test_at_each_epoch(val_dataset, Final_seg_model, int(step), epoch, temp_loss=test_avg_tmp_loss, save=True, training = False)\n",
    "        \n",
    "        \n",
    "        # write train epoch average loss to the tensorboard\n",
    "        print(\"writing train logs to tensorboard...\") \n",
    "        write_losses_tb(train_summary_writer, [train_total_loss_mean, train_seg_total_loss_mean, train_cls_SCE_loss_mean], epoch)\n",
    "            \n",
    "      \n",
    "        # when epoch fisnihed add epoch counter and reset the batch step \n",
    "        start_epoch.assign_add(1)     \n",
    "        step.assign(1)\n",
    "        \n",
    "        \n",
    "    # when training finihsed\n",
    "    print(\"training finished\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "\n",
    "# build_new check point manager\n",
    "new_ckpt_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "if not os.path.exists(new_ckpt_prefix):\n",
    "    os.makedirs(new_ckpt_prefix)\n",
    "#  contents of states to be saved as attributes on the checkpoint object\n",
    "new_ckpt_ob = tf.train.Checkpoint(step= tf.Variable(1),\n",
    "                                    epoch=  tf.Variable(1),\n",
    "                                    optimizer=new_optimizer,\n",
    "                                     new_model =  Final_seg_model\n",
    "                                 )\n",
    "# define checkpoint manager\n",
    "new_manager =  tf.train.CheckpointManager(new_ckpt_ob, new_ckpt_prefix, max_to_keep=1)\n",
    "\n",
    "# check the whether there is a checkpoint in the checkpoint folder, if it is restore from it\n",
    "if new_manager.latest_checkpoint:\n",
    "    print(\"Restored from {}\".format(new_manager.latest_checkpoint))\n",
    "    new_ckpt_ob.restore(new_manager.latest_checkpoint)\n",
    "else:\n",
    "    print(\"Initializing from scratch.\")\n",
    "\n",
    "# reset checkcpoint.step for each epoch\n",
    "step =  new_ckpt_ob.step\n",
    "ckpt_epoch =  new_ckpt_ob.epoch  \n",
    "\n",
    "\n",
    "epochs =  200\n",
    "# start train_process\n",
    "train_process(train_dataset, test_dataset, step, ckpt_epoch, total_epochs=epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
