{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "print(tf.test.is_gpu_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "from glob import glob\n",
    "# !pip install -q tensorflow-io\n",
    "import tensorflow_io as tfio\n",
    "\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from IPython import display\n",
    "import imageio\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "import tensorflow.keras.preprocessing.image as prep\n",
    "\n",
    "\n",
    "from copy import copy\n",
    "import matplotlib.colors as colors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HYPERPARAMETERS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read dataset from train and validation tfrecords (change to MNIST dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # paths for the tfrecords\n",
    "# # # # # for train\n",
    "# train_positive =  \"E:\\\\dataset\\\\Leisang\\\\myTry\\\\BleedingDataDCM/train_positive_samples_win.tfrecords\"\n",
    "# train_negative =  'E:\\\\dataset\\\\Leisang\\\\myTry\\\\BleedingDataDCM/train_negative_samples_win.tfrecords'\n",
    "\n",
    "# # for val\n",
    "# val_positive =  \"E:\\\\dataset\\\\Leisang\\\\myTry\\\\BleedingDataDCM/val_positive_samples_win.tfrecords\"\n",
    "# val_negative =  'E:\\\\dataset\\\\Leisang\\\\myTry\\\\BleedingDataDCM/val_negative_samples_win.tfrecords'\n",
    "\n",
    "# # # # for windows\n",
    "train_positive_file_name =  \"E:\\\\dataset\\\\Leisang\\\\myTry\\\\BleedingDataDCM/train_positive_sample_pairs_win.tfrecords\"\n",
    "train_negative_file_name =  'E:\\\\dataset\\\\Leisang\\\\myTry\\\\BleedingDataDCM/train_negative_sample_pairs_win.tfrecords'\n",
    "\n",
    "val_positive_file_name =  \"E:\\\\dataset\\\\Leisang\\\\myTry\\\\BleedingDataDCM/val_positive_sample_pairs_win.tfrecords\"\n",
    "val_negative_file_name =  'E:\\\\dataset\\\\Leisang\\\\myTry\\\\BleedingDataDCM/val_negative_sample_pairs_win.tfrecords'\n",
    "\n",
    "# # # # for windows\n",
    "# train_positive_file_name =  \"F:\\\\dataset\\\\Leisang\\\\myTry\\\\BleedingDataDCM/train_positive_sample_pairs_winLab.tfrecords\"\n",
    "# train_negative_file_name =  'F:\\\\dataset\\\\Leisang\\\\myTry\\\\BleedingDataDCM/train_negative_sample_pairs_winLab.tfrecords'\n",
    "\n",
    "# val_positive_file_name =  \"F:\\\\dataset\\\\Leisang\\\\myTry\\\\BleedingDataDCM/val_positive_sample_pairs__winLab.tfrecords\"\n",
    "# val_negative_file_name =  'F:\\\\dataset\\\\Leisang\\\\myTry\\\\BleedingDataDCM/val_negative_sample_pairs__winLab.tfrecords'\n",
    "# # for ubuntu\n",
    "\n",
    "# train_positive_file_name =  \"/media/ytx/Japan_Deep_Data/dataset/LeiSang/myTry/BleedingDataDCM/train_positive_sample_pairs_ubuntu.tfrecords\"\n",
    "# train_negative_file_name =  '/media/ytx/Japan_Deep_Data/dataset/LeiSang/myTry/BleedingDataDCM/train_negative_sample_pairs_ubuntu.tfrecords'\n",
    " \n",
    "# val_positive_file_name =  \"/media/ytx/Japan_Deep_Data/dataset/LeiSang/myTry/BleedingDataDCM/val_positive_sample_pairs_ubuntu.tfrecords\"\n",
    "# val_negative_file_name =  '/media/ytx/Japan_Deep_Data/dataset/LeiSang/myTry/BleedingDataDCM/val_negative_sample_pairs_ubuntu.tfrecords'\n",
    "\n",
    "\n",
    "train_positive_dataset = tf.data.TFRecordDataset(train_positive_file_name)\n",
    "train_negative_dataset = tf.data.TFRecordDataset(train_negative_file_name)\n",
    "\n",
    "val_positive_dataset = tf.data.TFRecordDataset(val_positive_file_name)\n",
    "val_negative_dataset = tf.data.TFRecordDataset(val_negative_file_name)\n",
    "\n",
    "\n",
    "# fashion_mnist = tf.keras.datasets.fashion_mnist\n",
    "\n",
    "# (train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(train_images.shape)\n",
    "# print(test_images.shape)\n",
    "\n",
    "# print(\"initial range:[{},{}]\".format(train_images.min(), train_images.max()))\n",
    "# # directly normalize\n",
    "# train_images = train_images/255.0\n",
    "# print(\"normalized range:[{},{}]\".format(train_images.min(), train_images.max()))\n",
    "# test_images = test_images/255.0\n",
    "\n",
    "\n",
    "# print(\"range of targest[{}, {}]\".format(train_labels.min(), train_labels.max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset = tf.data.Dataset.from_tensor_slices((train_images, train_labels))\n",
    "# test_dataset = tf.data.Dataset.from_tensor_slices((test_images, test_labels))\n",
    "\n",
    "# for tx_input, tx_target in train_dataset.take(1):\n",
    "#     print(\"range:[{},{}]\".format(tx_input.numpy().min(), tx_input.numpy().max()))\n",
    "#     print(tx_input.shape)\n",
    "#     plt.figure()\n",
    "#     plt.imshow(tx_input)\n",
    "#     plt.colorbar()\n",
    "#     plt.grid(False)\n",
    "#     plt.title(str(tx_target.numpy()))\n",
    "#     plt.show()\n",
    "    \n",
    "# for tx_input, tx_target in test_dataset.take(1):\n",
    "#     print(\"range:[{},{}]\".format(tx_input.numpy().min(), tx_input.numpy().max()))\n",
    "#     print(tx_input.shape)\n",
    "#     plt.figure()\n",
    "#     plt.imshow(tx_input)\n",
    "#     plt.colorbar()\n",
    "#     plt.grid(False)\n",
    "#     plt.title(str(tx_target.numpy()))\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PREPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 2048\n",
    "BATCH_SIZE = 2\n",
    "IMG_WIDTH = 256\n",
    "IMG_HEIGHT = 256\n",
    "# IMG_WIDTH = 28\n",
    "# IMG_HEIGHT = 28\n",
    "# project_name\n",
    "project_name = \"300Cls256_OneShotShareRandFlipScrachStructuredBleedClassONlyOutMidBreak2NegNumIter/\"\n",
    "\n",
    "# class names for the classification\n",
    "# Class_names= ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "#                'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
    "Class_names = [\"No bleed\", \"bleed\"]\n",
    "\n",
    "if not os.path.exists(project_name):\n",
    "    os.makedirs(project_name)\n",
    "# tb_log_name \n",
    "log_dir=project_name + \"AE_logs/\"\n",
    "\n",
    "# image_save name\n",
    "train_save_figure_path = project_name + \"AE_saves/train\"\n",
    "test_save_figure_path = project_name + \"AE_saves/test\"\n",
    "\n",
    "# training_checkpoint name\n",
    "checkpoint_dir = project_name + \"training_checkpoints\"\n",
    "\n",
    "# model architecture name\n",
    "encoder_path = project_name + \"Encoder.png\"\n",
    "decoder_path = project_name +\"Decoder.png\"\n",
    "autoencoder_path = project_name + \"Autoencoder.png\"\n",
    "new_model_path = project_name + \"new_model.png\"\n",
    "base_model_path = project_name + \"base_model.png\"\n",
    "classifier_path = project_name + \"classifier.png\"\n",
    "# gif save name\n",
    "anim_file = project_name + 'AE_saves.gif'\n",
    "\n",
    "# # dicom root \n",
    "# dicom_root = 'E:/dataset/Leisang/myTry/BleedingDataDCM'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math as m\n",
    "# import tensorflow_addons as tfa\n",
    "\n",
    "def resize(input, target):\n",
    "    print(input.shape)\n",
    "    print(target.shape)\n",
    "    resized_input = tf.image.resize(input, [IMG_HEIGHT, IMG_WIDTH],\n",
    "                                method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "    resized_target = tf.image.resize(target, [IMG_HEIGHT, IMG_WIDTH],\n",
    "                                method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "    resized_input =  tf.reshape(resized_input, [IMG_HEIGHT, IMG_WIDTH, 1])\n",
    "    resized_target =  tf.reshape(resized_target, [IMG_HEIGHT, IMG_WIDTH, 1])\n",
    "    return  resized_input, resized_target\n",
    "\n",
    "# @tf.function()\n",
    "def truncate(x, min, max):\n",
    "#     print(x.shape)\n",
    "    cliped =  tf.clip_by_value(x, min, max)\n",
    "    return cliped\n",
    " \n",
    "def norm(x, min, max):\n",
    "    # normalize_value = (value − min_value) / (max_value − min_value)\n",
    "    tensor = tf.math.divide(tf.subtract(x, min),\n",
    "                    tf.subtract(max, min))\n",
    "    return tensor\n",
    "\n",
    "def linear_normalization(input, min=30720.0, max=34816.0):\n",
    "    truncated_input = truncate(input, min, max)\n",
    "    norm_input = norm(truncated_input, min, max )\n",
    "    return  norm_input\n",
    "\n",
    "\n",
    "def random_crop(input_image, real_image):\n",
    "    stacked_image = tf.stack([input_image, real_image], axis=0)\n",
    "    cropped_image = tf.image.random_crop(\n",
    "      stacked_image, size=[2, IMG_HEIGHT, IMG_WIDTH, 1])  # crop to 256x256\n",
    "\n",
    "    return cropped_image[0], cropped_image[1]\n",
    "\n",
    "@tf.function()\n",
    "def random_jitter(input_image_tuple, seg_labels_tuple):\n",
    "    # randomly cropping to 256 x 256 x 3\n",
    "#     input_image, real_image = random_crop(input_image, real_image)\n",
    "\n",
    "    if tf.random.uniform(()) > 0.5:\n",
    "        # random mirroring\n",
    "        input_image1 = tf.image.flip_left_right(input_image_tuple[0])\n",
    "        input_image2 = tf.image.flip_left_right(input_image_tuple[1])\n",
    "        input_image3 = tf.image.flip_left_right(input_image_tuple[2])\n",
    "        seg_label1 = tf.image.flip_left_right(seg_labels_tuple[0])\n",
    "        seg_label2 = tf.image.flip_left_right(seg_labels_tuple[1])\n",
    "        seg_label3 = tf.image.flip_left_right(seg_labels_tuple[2])\n",
    "    \n",
    "        processed_inputs =  (input_image1, input_image2, input_image3)\n",
    "        processed_labels =  (seg_label1, seg_label2, seg_label3)\n",
    "    \n",
    "    else:\n",
    "        processed_inputs = input_image_tuple\n",
    "        processed_labels = seg_labels_tuple\n",
    "    return processed_inputs, processed_labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parser the dataset to decode the features\n",
    "# Create a dictionary describing the features.\n",
    "image_feature_description = {\n",
    "    \n",
    "        'dicom_path1': tf.io.FixedLenFeature([], tf.string),\n",
    "        'seg_label1': tf.io.FixedLenFeature([], tf.string), \n",
    "        'cls_label1': tf.io.FixedLenFeature([], tf.int64),\n",
    "        \n",
    "        'dicom_path2': tf.io.FixedLenFeature([], tf.string),\n",
    "        'seg_label2': tf.io.FixedLenFeature([], tf.string), \n",
    "        'cls_label2': tf.io.FixedLenFeature([], tf.int64),\n",
    "        \n",
    "        'dicom_path3': tf.io.FixedLenFeature([], tf.string),\n",
    "        'seg_label3': tf.io.FixedLenFeature([], tf.string), \n",
    "        'cls_label3': tf.io.FixedLenFeature([], tf.int64),\n",
    "}\n",
    "\n",
    "def read_inputs_seg_labels(parsed_features, dicom_path_key, seg_label_key):\n",
    "    # decode dicom\n",
    "    dicom_path = parsed_features[dicom_path_key]\n",
    "    image_bytes = tf.io.read_file(dicom_path)\n",
    "    input_image = tf.cast(tfio.image.decode_dicom_image(image_bytes, dtype=tf.uint16), tf.float32)\n",
    "    # decode mask\n",
    "    seg_label = tf.cast(tf.io.decode_raw(parsed_features[seg_label_key], tf.uint8), tf.float32)\n",
    "    seg_label = tf.reshape(seg_label, [-1, 512,512,1])\n",
    "    \n",
    "    return dicom_path, input_image, seg_label\n",
    "\n",
    "def train_prerpocessing(input_image_tuple, seg_labels_tuple):\n",
    "    preproced_inputs = []\n",
    "    preproced_input_w_images = []\n",
    "    preproced_labels = []\n",
    "    \n",
    "      # 1nd random flip\n",
    "    input_image_tuple, seg_labels_tuple = random_jitter(input_image_tuple, seg_labels_tuple)\n",
    "\n",
    "    \n",
    "    for i in range(len(input_image_tuple)):\n",
    "    \n",
    "        input_image, seg_label = resize(input_image_tuple[i], seg_labels_tuple[i])\n",
    "        print(input_image.shape)\n",
    "        print(seg_label.shape)\n",
    "        \n",
    "        # Last normalize\n",
    "        norm_input_image = linear_normalization(input_image) \n",
    "        seg_label =  seg_label/255.0\n",
    "\n",
    "\n",
    "        #lei -window for showing only\n",
    "        input_w_image = linear_normalization(input_image, min=32788.0, max=33148.0)\n",
    "        \n",
    "        preproced_inputs.append(norm_input_image)\n",
    "        preproced_input_w_images.append(input_w_image)\n",
    "        preproced_labels.append(seg_label)\n",
    "        \n",
    "        \n",
    "    \n",
    "    \n",
    "    return preproced_inputs, preproced_input_w_images, preproced_labels\n",
    "\n",
    "def test_prerpocessing(input_image_tuple, seg_labels_tuple):\n",
    "    preproced_inputs = []\n",
    "    preproced_input_w_images = []\n",
    "    preproced_labels = []\n",
    "    for i in range(len(input_image_tuple)):\n",
    "        \n",
    "    \n",
    "        input_image, seg_label = resize(input_image_tuple[i], seg_labels_tuple[i])\n",
    "        print(input_image.shape)\n",
    "        print(seg_label.shape)\n",
    "        \n",
    "        # Last normalize\n",
    "        norm_input_image = linear_normalization(input_image) \n",
    "        seg_label =  seg_label/255.0\n",
    "\n",
    "\n",
    "        #lei -window for showing only\n",
    "        input_w_image = linear_normalization(input_image, min=32788.0, max=33148.0)\n",
    "        \n",
    "        preproced_inputs.append(norm_input_image)\n",
    "        preproced_input_w_images.append(input_w_image)\n",
    "        preproced_labels.append(seg_label)\n",
    "        \n",
    "    return preproced_inputs, preproced_input_w_images, preproced_labels\n",
    "\n",
    "# # @tf.function()\n",
    "def train__parse_image_function(example_proto):\n",
    "    # extract features # Parse the input tf.Example proto using the dictionary above.\n",
    "    parsed_features = tf.io.parse_single_example(example_proto, image_feature_description)\n",
    "       \n",
    "  # for input 1 --->\n",
    "#     # decode dicom\n",
    "#     dicom_path1 = parsed_features[\"dicom_path1\"]\n",
    "#     image_bytes1 = tf.io.read_file(dicom_path1)\n",
    "#     input_image1 = tf.cast(tfio.image.decode_dicom_image(image_bytes1, dtype=tf.uint16), tf.float32)\n",
    "#     # decode mask\n",
    "#     seg_label1 = tf.cast(tf.io.decode_raw(parsed_features['seg_label1'], tf.uint8), tf.float32)\n",
    "#     seg_label1 = tf.reshape(seg_label1, [-1, 512,512,1])\n",
    "    dicom_path1, input_image1, seg_label1 = read_inputs_seg_labels(parsed_features, \"dicom_path1\", \"seg_label1\")\n",
    "    \n",
    "    # for input 2 --->\n",
    "    dicom_path2, input_image2, seg_label2 = read_inputs_seg_labels(parsed_features, \"dicom_path2\", \"seg_label2\")\n",
    "    \n",
    "    # for input 3 --->\n",
    "    dicom_path3, input_image3, seg_label3 = read_inputs_seg_labels(parsed_features, \"dicom_path3\", \"seg_label3\")\n",
    "    \n",
    "    \n",
    "    inputs = (input_image1, input_image2, input_image3)\n",
    "    seg_labels = (seg_label1, seg_label2, seg_label3)\n",
    "    \n",
    "    # preprocessing--->\n",
    "    # for input 1 ------->\n",
    "    preproced_inputs, preproced_input_w_image, preproced_labels = train_prerpocessing(inputs, seg_labels)\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    return (dicom_path1, dicom_path2, dicom_path3), tuple(preproced_inputs), tuple(preproced_input_w_image), tuple(preproced_labels), (parsed_features[\"cls_label1\"], parsed_features[\"cls_label2\"], parsed_features[\"cls_label3\"])\n",
    "\n",
    "# @tf.function()\n",
    "def test__parse_image_function(example_proto):\n",
    "    # extract features # Parse the input tf.Example proto using the dictionary above.(dicom_path1, dicom_path2, dicom_path3), (input_image1, input_image2, input_image3), (seg_label1, seg_label2, seg_label3), (parsed_features[\"cls_label1\"], parsed_features[\"cls_label2\"], parsed_features[\"cls_label3\"]\n",
    "    parsed_features = tf.io.parse_single_example(example_proto, image_feature_description)\n",
    "    \n",
    "    # for input 1 --->\n",
    "    dicom_path1, input_image1, seg_label1 = read_inputs_seg_labels(parsed_features, \"dicom_path1\", \"seg_label1\")\n",
    "    \n",
    "    # for input 2 --->\n",
    "    dicom_path2, input_image2, seg_label2 = read_inputs_seg_labels(parsed_features, \"dicom_path2\", \"seg_label2\")\n",
    "    \n",
    "    # for input 3 --->\n",
    "    dicom_path3, input_image3, seg_label3 = read_inputs_seg_labels(parsed_features, \"dicom_path3\", \"seg_label3\")\n",
    "    \n",
    "    \n",
    "    # preprocessing--->\n",
    "    inputs = (input_image1, input_image2, input_image3)\n",
    "    seg_labels = (seg_label1, seg_label2, seg_label3)\n",
    "    \n",
    "    preproced_inputs, preproced_input_w_image, preproced_labels = test_prerpocessing(inputs, seg_labels)\n",
    "    \n",
    "    \n",
    "    return (dicom_path1, dicom_path2, dicom_path3), tuple(preproced_inputs), tuple(preproced_input_w_image), tuple(preproced_labels), (parsed_features[\"cls_label1\"], parsed_features[\"cls_label2\"], parsed_features[\"cls_label3\"])\n",
    "\n",
    "\n",
    "# train: positive\n",
    "# train_nb_pos = 3049\n",
    "# train_nb_neg = 33588\n",
    "train_nb_pos = 3048\n",
    "train_nb_neg = 33515\n",
    "parsed_train_positive_dataset = train_positive_dataset.map(train__parse_image_function)\n",
    "parsed_train_negative_dataset = train_negative_dataset.map(train__parse_image_function)\n",
    "\n",
    "# val_nb_pos = 238\n",
    "# val_nb_neg = 4420\n",
    "val_nb_pos = 238\n",
    "val_nb_neg = 4214\n",
    "parsed_val_positive_dataset = val_positive_dataset.map(test__parse_image_function)\n",
    "parsed_val_negative_dataset = val_negative_dataset.map(test__parse_image_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # check the dataset\n",
    "def plot_one_input(dicom_path, norm_input, norm_w_input, seg_label, cls_label):\n",
    "    palette = copy(plt.cm.gray)\n",
    "    palette.set_over('r', 1.0)\n",
    "    print(\"dicom_path:\", dicom_path)\n",
    "    print(\"input_image.shape\", norm_input.shape)\n",
    "    print(\"seg_label\", seg_label.shape)\n",
    "    print(\"cls_label\", cls_label)\n",
    "    num_nonzeros=np.count_nonzero(seg_label)\n",
    "    # reshape label \n",
    "    target =  tf.reshape(seg_label, norm_input.shape)\n",
    "    fig, axes = plt.subplots(1,2, figsize=(20,20))\n",
    "    \n",
    "    norm_mask =  np.squeeze(target.numpy())\n",
    "    \n",
    "    norm_input = np.squeeze(norm_input.numpy())\n",
    "    norm_w_input= np.squeeze(norm_w_input.numpy())\n",
    "\n",
    "    print()\n",
    "    axes[0].imshow(norm_input, cmap='gray')\n",
    "    axes[0].set_title('input range:[{}, {}]'.format((norm_input.min()), np.max(norm_input)))\n",
    "    axes[1].imshow(norm_mask, cmap='gray')\n",
    "    axes[1].set_title('seg target range:[{}, {}] \\n cls_tar: {} # of nonzeros: {}'.format(np.min(norm_mask), np.max(norm_mask), cls_label, num_nonzeros))\n",
    "    \n",
    "    print(\"norm_mask\", norm_mask.shape)\n",
    "    print(\"norm_input\", norm_input.shape)\n",
    "    masked_in = norm_w_input + norm_mask\n",
    "    print('masked_in range:[{}, {}]'.format((masked_in.min()), masked_in.max()))\n",
    "\n",
    "    masked = np.ma.masked_where(norm_mask==0, masked_in)  # this is only generated mask\n",
    "    print('masked range:[{}, {}]'.format(masked.min(), masked.max())) \n",
    "\n",
    "    \n",
    "    fig2, axes2 = plt.subplots(1,2, figsize=(20,20))\n",
    "    \n",
    "    axes2[0].imshow(norm_w_input, cmap='gray')\n",
    "    axes2[0].set_title('input range:[{}, {}]'.format((norm_w_input.min()), np.max(norm_w_input)))\n",
    "    \n",
    "    axes2[1].imshow(norm_w_input, cmap='gray')\n",
    "    axes2[1].set_title('fused range:[{}, {}] \\n path: {}'.format(np.min(masked), np.max(masked), dicom_path) )\n",
    "    axes2[1].imshow(masked, palette, colors.Normalize(vmin=0, vmax=1), interpolation='none', alpha=0.4)\n",
    "   \n",
    "\n",
    "for dicom_paths, norm_inputs, norm_w_inputs, seg_labels, cls_labels in parsed_train_positive_dataset.take(1):\n",
    "    for i in range(len(dicom_paths)):\n",
    "        print(\"input range: [{} {}]\".format(norm_inputs[i].numpy().min(), norm_inputs[i].numpy().max()))\n",
    "        plot_one_input(dicom_paths[i], norm_inputs[i], norm_w_inputs[i],seg_labels[i], cls_labels[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DEFINE LOAD FUNCTION IN INPUT PIPELINE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## INPUT PIPELINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# balance the dataset\n",
    "# lets keep the positive and negative training samples \n",
    "# positive and negative dataset will be asked by turn of batchs\n",
    "train_nb_pos = 3048\n",
    "train_nb_neg = 33515\n",
    "rep_num = np.ceil(train_nb_neg/train_nb_pos)\n",
    "print(\"train_nb_pos:\", train_nb_pos)\n",
    "print(\"train_nb_neg:\", train_nb_neg)\n",
    "print(\"rep_num:\", rep_num)\n",
    "# \n",
    "# parsed_train_positive_dataset = parsed_train_positive_dataset.shuffle(train_nb_pos).repeat() # https://www.tensorflow.org/tutorials/structured_data/imbalanced_data#oversample_the_minority_class\n",
    "# parsed_train_negative_dataset = parsed_train_negative_dataset.shuffle(BUFFER_SIZE).repeat()\n",
    "# train_dataset = tf.data.experimental.sample_from_datasets([parsed_train_positive_dataset, parsed_train_negative_dataset], weights=[0.5, 0.5])\n",
    "# train_dataset = train_dataset.batch(BATCH_SIZE).prefetch(4)\n",
    "# train_positive_dataset =  parsed_train_positive_dataset\n",
    "# train_negative_dataset =  parsed_train_negative_dataset\n",
    "\n",
    "train_positive_dataset = train_positive_dataset.shuffle(train_nb_pos, reshuffle_each_iteration=True).repeat().batch(BATCH_SIZE).prefetch(4)\n",
    "\n",
    "train_negative_dataset = train_negative_dataset.shuffle(BUFFER_SIZE, reshuffle_each_iteration=True).repeat().batch(BATCH_SIZE).prefetch(4)\n",
    "\n",
    "\n",
    "test_dataset = parsed_val_positive_dataset.concatenate(parsed_val_negative_dataset)\n",
    "# test_dataset = test_dataset.shuffle(BUFFER_SIZE)  # for the random check if not comment it\n",
    "test_dataset = test_dataset.batch(BATCH_SIZE).prefetch(4)\n",
    "\n",
    "\n",
    "# # test positive dataset\n",
    "# test_positive_dataset =  parsed_val_positive_dataset\n",
    "# test_positive_dataset = test_positive_dataset.batch(BATCH_SIZE).prefetch(2)\n",
    "# # test negative dataset\n",
    "# test_negative_dataset =  parsed_val_negative_dataset\n",
    "# test_negative_dataset = test_negative_dataset.batch(BATCH_SIZE).prefetch(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To use this dataset, you'll need the number of steps per epoch.\n",
    "\n",
    "The definition of \"epoch\" in this case is less clear. Say it's the number of batches required to see each positive example once:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_pos_train_batches = np.ceil(train_nb_pos*rep_num/BATCH_SIZE)\n",
    "n_neg_train_batches = np.ceil(train_nb_neg/BATCH_SIZE)\n",
    "\n",
    "val_nb_pos = 238\n",
    "val_nb_neg = 4214\n",
    "n_val_batches = np.ceil((val_nb_pos+val_nb_neg)/BATCH_SIZE)\n",
    "\n",
    "\n",
    "n_positive_test_batches = np.ceil(val_nb_pos/BATCH_SIZE)\n",
    "\n",
    "print(\"n_pos_train_batches:\", n_pos_train_batches)\n",
    "print(\"n_neg_train_batches:\", n_neg_train_batches)\n",
    "\n",
    "\n",
    "print(\"n_test_batches:\", n_val_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dicom_paths, norm_inputs, norm_w_inputs, seg_labels, cls_labels in test_dataset.take(1): #[([batch,...]), (), ...]\n",
    "    print(\"test dicom_path shape:\", dicom_paths[0].shape)\n",
    "    print(\"test norm_inputs shape:\", norm_inputs[0].shape)\n",
    "    for i in range(len(dicom_paths)):\n",
    "        print(\"input range: [{} {}]\".format(norm_inputs[i][0].numpy().min(), norm_inputs[i][0].numpy().max()))\n",
    "        plot_one_input(dicom_paths[i][0], norm_inputs[i][0], norm_w_inputs[i][0],seg_labels[i][0], cls_labels[i][0])\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DESIGN MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_CHANNELS = 1\n",
    "latent_dim =50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Flatten, Dense, Concatenate, Dot, Lambda, Input\n",
    "from tensorflow.python.keras import backend as K\n",
    "# model input components\n",
    "input1 = tf.keras.layers.Input(shape=[IMG_HEIGHT,IMG_HEIGHT,1])\n",
    "input2 = tf.keras.layers.Input(shape=[IMG_HEIGHT,IMG_HEIGHT,1])\n",
    "input3 = tf.keras.layers.Input(shape=[IMG_HEIGHT,IMG_HEIGHT,1])\n",
    "\n",
    "# define down_sample block as the pix-2-pix from tensorflow official set\n",
    "def downsample(filters, size, apply_batchnorm=True):\n",
    "    initializer = tf.random_normal_initializer(0., 0.02)\n",
    "\n",
    "    result = tf.keras.Sequential()\n",
    "    result.add(\n",
    "      tf.keras.layers.Conv2D(filters, size, strides=2, padding='same',\n",
    "                             kernel_initializer=initializer, use_bias=False))\n",
    "\n",
    "    if apply_batchnorm:\n",
    "        result.add(tf.keras.layers.BatchNormalization())\n",
    "\n",
    "    result.add(tf.keras.layers.LeakyReLU())\n",
    "\n",
    "    return result\n",
    "\n",
    "def upsample(filters, size, apply_dropout=False):\n",
    "    initializer = tf.random_normal_initializer(0., 0.02)\n",
    "\n",
    "    result = tf.keras.Sequential()\n",
    "    result.add(\n",
    "    tf.keras.layers.Conv2DTranspose(filters, size, strides=2,\n",
    "                                    padding='same',\n",
    "                                    kernel_initializer=initializer,\n",
    "                                    use_bias=False))\n",
    "\n",
    "    result.add(tf.keras.layers.BatchNormalization())\n",
    "\n",
    "    if apply_dropout:\n",
    "        result.add(tf.keras.layers.Dropout(0.5))\n",
    "\n",
    "    result.add(tf.keras.layers.ReLU())\n",
    "\n",
    "    return result\n",
    "\n",
    "def feature_extractor_backbone(feb_input):\n",
    "    x = downsample(64, 3, apply_batchnorm=False)(feb_input) # output  (None, 128, 128, 64)\n",
    "#     skips.append(x)\n",
    "    x = downsample(128, 3)(x)  # output (None, 64, 64, 128)\n",
    "#     skips.append(x)\n",
    "    x = downsample(256, 3)(x)  # output (None, 32, 32, 256)\n",
    "    \n",
    "    x = downsample(512, 3)(x)  # output (None, 16, 16, 512)\n",
    "    x = downsample(1024, 3)(x)  # output (None, 8, 8, 1024)\n",
    "    \n",
    "    # 1x1 convol\n",
    "    x = Flatten()(x)\n",
    "    \n",
    "#     # dense layer\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    \n",
    "#     skips.append(x)\n",
    "    # if want seperate train the generator, return the Model object e.g. return tf.keras.Model(inputs=inputs, outputs=x)\n",
    "    return x\n",
    "\n",
    "\n",
    "# extractor input 1\n",
    "en1 = feature_extractor_backbone(input1)\n",
    "en2 = feature_extractor_backbone(input2)\n",
    "en3 = feature_extractor_backbone(input3)\n",
    "\n",
    "# define euclidean_distance for dense output vectors\n",
    "def euclidean_distance(vects):\n",
    "    x1, x2, x3 = vects\n",
    "    sum_square1 = K.sum(K.square(x1 - x2), axis=1, keepdims=True)\n",
    "    sum_square3 = K.sum(K.square(x3 - x2), axis=1, keepdims=True)\n",
    "    return K.sqrt(K.maximum(sum_square1, K.epsilon())), K.sqrt(K.maximum(sum_square3, K.epsilon()))  #1e-07\n",
    "\n",
    "# # merge dense outputs from feature extractor backbones\n",
    "merge_layer1,  merge_layer3= Lambda(euclidean_distance)([en1, en2, en3])\n",
    "\n",
    "dense_d1 = Dense(64, activation=\"relu\")(merge_layer1)\n",
    "dense_d3 = Dense(64, activation=\"relu\")(merge_layer3)\n",
    "\n",
    "concat_features = Concatenate()([en1, en2, en3, dense_d1, dense_d3])\n",
    "#\n",
    "# concat_features = Concatenate()([en1, en2, en3])\n",
    "\n",
    "# dense_out1 = Dense(1, activation=\"sigmoid\")(concat_features)\n",
    "dense_out2 = Dense(1, activation=\"sigmoid\")(concat_features)\n",
    "# dense_out3 = Dense(1, activation=\"sigmoid\")(concat_features)\n",
    "\n",
    "classifier = tf.keras.Model(inputs=[input1, input2, input3], outputs=dense_out2)\n",
    "             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.keras.utils.plot_model(en1, to_file=encoder_path, show_shapes=True, dpi=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.keras.utils.plot_model(en2, to_file=decoder_path, show_shapes=True, dpi=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.keras.utils.plot_model(en3, to_file=autoencoder_path, show_shapes=True, dpi=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.utils.plot_model(classifier, to_file=classifier_path, show_shapes=True, dpi=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OPTIMIZER AND OBJECTIVE LOSSES\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training PREPARING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## get current working directory\n",
    "cwd = os.getcwd()\n",
    "print(\"current working directory:\", cwd)\n",
    "train_full_AE_saves =  os.path.join(cwd, train_save_figure_path)\n",
    "test_full_AE_saves =  os.path.join(cwd, test_save_figure_path)\n",
    "print(\"train_full_AE_saves:\", train_full_AE_saves)\n",
    "print(\"test_full_AE_saves:\", test_full_AE_saves)\n",
    "\n",
    "\n",
    "if not os.path.exists(train_full_AE_saves):\n",
    "    os.makedirs(train_full_AE_saves)\n",
    "\n",
    "if not os.path.exists(test_full_AE_saves):\n",
    "    os.makedirs(test_full_AE_saves)\n",
    "    \n",
    "    \n",
    "train_predictions_save_path =os.path.join(train_full_AE_saves, \"Predictions\")\n",
    "train_clsDistr_save_path =os.path.join(train_full_AE_saves, \"ClassDistruibution\")\n",
    "\n",
    "test_predictions_save_path =os.path.join(test_full_AE_saves, \"Predictions\")\n",
    "\n",
    "test_confusion_matrix =  os.path.join(test_full_AE_saves, \"Confusion_Matrix\")\n",
    "test_cm_diags =  os.path.join(test_full_AE_saves, \"Confusion_Matrix_diagnoise_with_Epoch\")\n",
    "\n",
    "test_batch_losses =  os.path.join(test_full_AE_saves, \"Test_batch_losses\")\n",
    "\n",
    "test_EpochValidation_save_path =os.path.join(test_full_AE_saves, \"EpochValidation\")\n",
    "test_EpochValidation_save_path_pred =os.path.join(test_EpochValidation_save_path, \"PredictionsOnly\")\n",
    "if not os.path.exists(train_predictions_save_path):\n",
    "    os.makedirs(train_predictions_save_path)\n",
    "if not os.path.exists(train_clsDistr_save_path):\n",
    "    os.makedirs(train_clsDistr_save_path)\n",
    "    \n",
    "if not os.path.exists(test_predictions_save_path):\n",
    "    os.makedirs(test_predictions_save_path)\n",
    "if not os.path.exists(test_EpochValidation_save_path):\n",
    "    os.makedirs(test_EpochValidation_save_path)\n",
    "if not os.path.exists(test_EpochValidation_save_path_pred):\n",
    "    os.makedirs(test_EpochValidation_save_path_pred)   \n",
    "if not os.path.exists(test_confusion_matrix):\n",
    "    os.makedirs(test_confusion_matrix)       \n",
    "if not os.path.exists(test_cm_diags):\n",
    "    os.makedirs(test_cm_diags) \n",
    "if not os.path.exists(test_batch_losses):\n",
    "    os.makedirs(test_batch_losses)   \n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "def new_generate_images(model, dicom_paths, norm_inputs,norm_input_w_images, norm_seg_targets, cls_targets, batch_idx, epoch, save=True, Train_or_not=True, Epoch_val =False):\n",
    "#     prediction = model(test_input, training=True)\n",
    "  \n",
    "    \n",
    "    class_sigmoid_outputs = model.predict(norm_inputs)\n",
    "#     print(\"class_sigmoid_output shape:\", class_sigmoid_output)\n",
    "    cls_pred =  class_sigmoid_outputs[0]\n",
    "#     cls_out1 =  class_sigmoid_outputs[0]\n",
    "#     cls_out2 =  class_sigmoid_outputs[1]\n",
    "#     cls_out3 =  class_sigmoid_outputs[2]\n",
    "#     cls_pred =  np.argmax(class_sigmoid_output[0])\n",
    "    fig, axes = plt.subplots(1,3, figsize=(15,15))\n",
    "    for i in range(len(dicom_paths)):\n",
    "        \n",
    "        \n",
    "        dicom_path = dicom_paths[i][0]\n",
    "        print(\"dicom_path\", dicom_path.numpy())\n",
    "        dicom_path = str(dicom_path.numpy(), 'utf-8')\n",
    "        base_name =  os.path.basename(dicom_path)\n",
    "        case_name = os.path.basename((os.path.dirname(dicom_path)))\n",
    "        print(\"basename:\", base_name)\n",
    "        print(\"dri_name:\", case_name)\n",
    "        \n",
    "        \n",
    "        norm_input =norm_inputs[i][0]\n",
    "        norm_w_input =norm_input_w_images[i][0]\n",
    "        seg_label =norm_seg_targets[i][0]\n",
    "        cls_label = cls_targets[i][0]\n",
    "        \n",
    "        palette = copy(plt.cm.gray)\n",
    "        palette.set_over('r', 1.0)\n",
    "        print(\"dicom_path:\", dicom_path)\n",
    "        print(\"input_image.shape\", norm_input.shape)\n",
    "        print(\"seg_label\", seg_label.shape)\n",
    "        print(\"cls_label\", cls_label)\n",
    "        num_nonzeros=np.count_nonzero(seg_label)\n",
    "        # reshape label \n",
    "        target =  tf.reshape(seg_label, norm_input.shape)\n",
    "        \n",
    "#         fig, axes = plt.subplots(1,3,i+1, figsize=(20,20))\n",
    "\n",
    "        norm_mask =  np.squeeze(target.numpy())\n",
    "\n",
    "        norm_input = np.squeeze(norm_input.numpy())\n",
    "        norm_w_input= np.squeeze(norm_w_input.numpy())\n",
    "\n",
    "        print(\"norm_mask\", norm_mask.shape)\n",
    "        print(\"norm_input\", norm_input.shape)\n",
    "        masked_in = norm_w_input + norm_mask\n",
    "        print('masked_in range:[{}, {}]'.format((masked_in.min()), masked_in.max()))\n",
    "\n",
    "        masked = np.ma.masked_where(norm_mask==0, masked_in)  # this is only generated mask\n",
    "        print('masked range:[{}, {}]'.format(masked.min(), masked.max())) \n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "      \n",
    "\n",
    "        axes[i].imshow(norm_w_input, cmap='gray')\n",
    "        axes[i].set_title('input range:[{}, {}] \\n case name: {} \\n dicom: {} \\n cls_tar: {} cls_pred: {}'.format(np.min(norm_input), np.max(norm_input), case_name, base_name, cls_label, cls_pred))\n",
    "        axes[i].imshow(masked, palette, colors.Normalize(vmin=0, vmax=1), interpolation='none', alpha=0.4)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save==True :\n",
    "        if Train_or_not:\n",
    "            fig.savefig(train_save_figure_path + \"/Train_image_at_epoch_{:04d}_batch_{}.png\".format(epoch,batch_idx))\n",
    "        else:\n",
    "            if Epoch_val:\n",
    "                fig.savefig(test_EpochValidation_save_path + \"/Test_image_at_epoch_{:04d}_batch_{}_idx.png\".format(epoch,batch_idx))\n",
    "            else:\n",
    "                fig.savefig(test_save_figure_path + \"/Test_image_at_epoch_{:04d}_batch_{}.png\".format(epoch,batch_idx))\n",
    "     \n",
    "    \n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "# for tensorboard writers\n",
    "datetime_rec =  datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "train_summary_writer = tf.summary.create_file_writer(\n",
    "  log_dir + \"train\")\n",
    "val_summary_writer = tf.summary.create_file_writer(\n",
    "  log_dir + \"val\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start tensorboard\n",
    "# !kill 5032\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir {log_dir}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## check the mornitoring the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base_model =  en # use encoder as the base model\n",
    "\n",
    "# # check the loaded ae output\n",
    "for dicom_paths, norm_inputs, norm_w_inputs, seg_labels, cls_labels in test_dataset.take(1):\n",
    "    new_generate_images(classifier,dicom_paths, norm_inputs, norm_w_inputs, seg_labels, cls_labels,  1, 1, save=False, Train_or_not=False, Epoch_val =False)\n",
    "\n",
    "# check the base base model traininable\n",
    "# print(base_model.trainable)\n",
    "\n",
    "# # \n",
    "# trainable_or_not =  True\n",
    "\n",
    "# if trainable_or_not == True:\n",
    "#     base_model.trainable =  True # set encoder untranable\n",
    "# print(\"after reset basemodel trainable property:\", base_model.trainable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# design new loss for the new model as new_train_step, new_test_step\n",
    "# new_optimizer =  tf.keras.optimizers.Adam(1e-4)\n",
    "# from tf.keras.utils import to_categorical\n",
    "BCE =  tf.keras.losses.BinaryCrossentropy() \n",
    "Huber =  tf.keras.losses.Huber(delta=0.1)\n",
    "SCC = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "@tf.function()\n",
    "# define losses\n",
    "def new_compute_loss(pred, cls_tar):\n",
    "    \n",
    "#     #seg  cross_entropy,  use reduce mean not sum, otherwise loss will be very big\n",
    "#     cls_BCE_loss1 = BCE(y_true=cls_tars[0], y_pred=preds[0]) \n",
    "#     cls_BCE_loss2 = BCE(y_true=cls_tars[1], y_pred=preds[1]) \n",
    "    cls_BCE_loss = BCE(y_true=cls_tar, y_pred=pred) \n",
    "    total_loss = cls_BCE_loss\n",
    "#     # L1 loss \n",
    "# #     L1_loss = tf.reduce_mean(tf.abs(x - decoded_x))\n",
    "    \n",
    "#     #seg Huber_loss\n",
    "#     seg_Huber_loss = Huber(y_true=gt_seg, y_pred=pred_seg)\n",
    "    \n",
    "#     seg_total_loss = seg_Huber_loss + seg_BCE_loss\n",
    "    \n",
    "    \n",
    "#     # use tf.keras.losses.SparseCategoricalCrossentropy instead of BCE , no need one-hot tranfermat\n",
    "#     cls_SCC_loss=  SCC(y_true=gt_cls, y_pred=pred_cls)\n",
    "    \n",
    "#     # total_loss = cls_BCE_loss + seg_loss\n",
    "#     total_loss = cls_SCC_loss \n",
    "    return total_loss\n",
    "    \n",
    "\n",
    "# def mask_to_categorical(cls_tar, num_cls=2):\n",
    "#     tar = tf.one_hot(tf.cast(cls_tar, tf.int32), num_cls)\n",
    "#     tar = tf.cast(tar, tf.float32)\n",
    "#     return tar\n",
    "\n",
    "# @tf.function()\n",
    "def new_train_step(model, optimizer, inputs, cls_target, training):\n",
    "    # change cls labe 0 or 1 into  [1, 0] [0, 1]\n",
    "#     print(cls_target)\n",
    "#     cls_target = mask_to_categorical(cls_target)\n",
    "#     print(\"in trianing step\")\n",
    "    with tf.GradientTape() as tape:  # very interesting\n",
    "        # outputs=[seg_out, class_sigmoid_output] \n",
    "        pred= model(inputs, training=training)\n",
    "        total_loss = new_compute_loss(pred=pred, cls_tar=cls_target)\n",
    "    gradients = tape.gradient(total_loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    return total_loss\n",
    "\n",
    "# @tf.function()\n",
    "def new_test_step(model2, inputs, cls_target, training):\n",
    "#     with tf.GradientTape() as tape:  # very interesting\n",
    "    pred= model2(inputs, training=training)\n",
    "    total_loss= new_compute_loss(pred=pred, cls_tar=cls_target)\n",
    "    return total_loss, pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion matrix define\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install scikit-learn\n",
    "!pip install pandas\n",
    "import sklearn.metrics\n",
    "import itertools\n",
    "# import seaborn as sns\n",
    "import pandas as pd\n",
    "def plot_confusion_matrix(cm, class_names, num_s, final_step, epoch, save=True):\n",
    "    \"\"\"\n",
    "    Returns a matplotlib figure containing the plotted confusion matrix.\n",
    "\n",
    "    Args:\n",
    "    cm (array, shape = [n, n]): a confusion matrix of integer classes\n",
    "    class_names (array, shape = [n]): String names of the integer classes e.g, [\"dog\", \"cat\"]\n",
    "    \"\"\"\n",
    "    figure = plt.figure(figsize=(12, 12))\n",
    "#     plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    plt.title(\"Confusion matrix(range[{}, {}] \\n # of samples: {} toatal train steps:{})\".format(cm.min(), cm.max(), num_s, final_step))\n",
    "    \n",
    "     # Normalize the confusion matrix.\n",
    "    norm_cm = np.around(cm.astype('float') / cm.sum(axis=1)[:, np.newaxis], decimals=2)\n",
    "    plt.imshow(norm_cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "     # Use white text if squares are dark; otherwise black.\n",
    "    threshold = norm_cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        color = \"white\" if norm_cm[i, j] > threshold else \"black\"\n",
    "        plt.text(j, i, \"{}({})\".format(cm[i, j],norm_cm[i, j]), horizontalalignment=\"center\", color=color)\n",
    "    plt.colorbar()  # rember the jupyter lab will remember the plt color operation so the coloar may related to the normalized value if not clean \n",
    "    tick_marks = np.arange(len(class_names))\n",
    "    plt.xticks(tick_marks, class_names, rotation=45)\n",
    "    plt.yticks(tick_marks, class_names, rotation=45)\n",
    "\n",
    "#     # Normalize the confusion matrix.\n",
    "#     cm = np.around(cm.astype('float') / cm.sum(axis=1)[:, np.newaxis], decimals=2)\n",
    "\n",
    "#     # Use white text if squares are dark; otherwise black.\n",
    "#     threshold = cm.max() / 2.\n",
    "#     for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "#         color = \"white\" if cm[i, j] > threshold else \"black\"\n",
    "#         plt.text(j, i, cm[i, j], horizontalalignment=\"center\", color=color)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label', labelpad=0)\n",
    "    plt.xlabel('Predicted label')\n",
    "    if save:    \n",
    "        plt.savefig(test_confusion_matrix + \"/Test_cm_at_epoch_{:04d}.png\".format(epoch))\n",
    "    else:\n",
    "        pass\n",
    "    return figure\n",
    "\n",
    "\n",
    "\n",
    "# # define calculate cm\n",
    "# def log_confusion_matrix_all(model,test_inputs, cls_tar, epoch, save):\n",
    "#     # Use the model to predict the values from the validation dataset.\n",
    "#     class_softmax_output = model.predict(test_inputs)\n",
    "#     test_pred = np.argmax(class_softmax_output, axis=1)\n",
    "    \n",
    "    \n",
    "#     # Calculate the confusion matrix.\n",
    "#     cm = sklearn.metrics.confusion_matrix(cls_tar, test_pred)\n",
    "\n",
    "\n",
    "# plot diagonal of confusion matrix with respect to epoch\n",
    "def log_plot_confusion_matrix_at_epochs(cm_concate_axis0, class_names, epoch, at_epoch, save=True):\n",
    "    if epoch % at_epoch ==0:\n",
    "        figure = plt.figure(figsize=(8, 8))\n",
    "    #     plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "        plt.title(\"Confusion matrix(range[{}, {}]) at epoch {}(total {} epochs)\".format(cm_concate_axis0.min(), cm_concate_axis0.max(), at_epoch, cm_concate_axis0.shape[0]))\n",
    "\n",
    "        print(\"concate cm shape:\",cm_concate_axis0.shape)\n",
    "        # plot cm_diag vs epoch\n",
    "    #     plt.plot(range(cm_concate_axis0.shape[0]), )\n",
    "        for i in range(cm_concate_axis0.shape[1]):\n",
    "            plt.plot(range(1, cm_concate_axis0.shape[0]+1), cm_concate_axis0[:,i], \"o--\", label = '{}'.format(class_names[i]))\n",
    "\n",
    "        plt.legend(loc='best')\n",
    "        plt.tight_layout()\n",
    "        plt.ylabel('True diagonal probabilies', labelpad=0)\n",
    "        plt.xlabel('Epochs')\n",
    "        if save:    \n",
    "            plt.savefig(test_cm_diags + \"/Test_cm_diag_at_epoch_{:04d}.png\".format(epoch))\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "def plot_train_class_distrution(class_list, step, epoch,freq = 10, save=True):\n",
    "    if step % freq ==0:\n",
    "        fig, axes = plt.subplots(1,2, figsize=(20,20))\n",
    "        axes[0].scatter(np.array(range(1, len(class_list)+1)), np.array(class_list))\n",
    "        axes[0].set_title(\"target class vs epochs\")\n",
    "        axes[0].set_xlabel(\"epochs\")\n",
    "        axes[0].set_ylabel(\"train tar class\")\n",
    "        \n",
    "        axes[1].hist(class_list, bins=2, normed=0, facecolor='blue', alpha=0.5) \n",
    "#         axes[1].hist(class_list, bins = 2)\n",
    "        axes[1].set_title(\"train target disribution\")\n",
    "        axes[1].set_xlabel(\"train tar class\")\n",
    "        axes[1].set_ylabel(\"Count\")\n",
    "        \n",
    "        if save:    \n",
    "            plt.savefig(train_clsDistr_save_path + \"/Test_cm_diag_at_epoch_{:04d}_step_{:04d}.png\".format(epoch, step))\n",
    "        else:\n",
    "            pass\n",
    "    \n",
    "def plot_batch_test_loss(test_batch_loss_catches, epoch):\n",
    "    figure = plt.figure(figsize=(8, 8))\n",
    "#     #     plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "#     plt.title(\"test loss(range[{%.2f}, {%.2f}]) at epoch {} /n total batches:{}\".format(test_batch_loss_catches.min(), test_batch_loss_catches.max(), epoch, test_batch_loss_catches.shape[1]))\n",
    "#     epoch_x =  range(1, epoch+1)\n",
    "#     data = {\"epochs\": } \n",
    "    \n",
    "    data={}\n",
    "#     sns.sns.boxplot(data = test_batch_loss_catches\n",
    "#             ,x = 'score'\n",
    "#             )\n",
    "    # plot cm_diag vs epoch\n",
    "    #     plt.plot(range(cm_concate_axis0.shape[0]), )\n",
    "\n",
    "    # build dictory data for sns box plot\n",
    "    for i in range(test_batch_loss_catches.shape[0]):\n",
    "        data[str(i+1)] = test_batch_loss_catches[i, :]\n",
    "#         x_axis =  []\n",
    "#         x_axis += test_batch_loss_catches.shape[1] *[epoch]\n",
    "#         print(\"x_axis:\", x_axis)\n",
    "#         plt.plot(x_axis, test_batch_loss_catches[i], \"o--\", label = '{}'.format(epoch))\n",
    "#         plt.plot(x_axis[0], np.mean(test_batch_loss_catches), label = 'mean {%.2f} at epoch {}'.format(np.mean(test_batch_loss_catches), epoch))\n",
    "#     plt.legend(loc='best')\n",
    "#     plt.tight_layout()\n",
    "#     plt.ylabel('Test batch loss', labelpad=0)\n",
    "#     plt.xlabel('Epochs', labelpad=0)\n",
    "       \n",
    "#     plt.savefig(test_batch_losses + \"/Test_loss_at_epoch_{:04d}.png\".format(epoch))\n",
    "    print(\"test loss data:\", data)\n",
    "    df = pd.DataFrame(data)\n",
    "    df.plot.box(title=\"test batch losses per epoch\")\n",
    "    plt.savefig(test_batch_losses + \"/Test_loss_at_epoch_{:04d}.png\".format(epoch))\n",
    "    \n",
    "# define confusion matrix for classification displaying\n",
    "def log_confusion_matrix_all(model,test_inputs, class_names,cls_tar, epoch, save):\n",
    "    # Use the model to predict the values from the validation dataset.\n",
    "    class_softmax_output = model.predict(test_inputs)\n",
    "#     test_pred = np.argmax(class_softmax_output, axis=1)\n",
    "    \n",
    "    \n",
    "    # Calculate the confusion matrix.\n",
    "    cm = sklearn.metrics.confusion_matrix(cls_tar, test_pred)\n",
    "    # Log the confusion matrix as an image summary.\n",
    "\n",
    "    figure = plot_confusion_matrix(cm, class_names, epoch, batch_indx, save)\n",
    "    \n",
    "    return cm\n",
    "#     cm_image = plot_to_image(figure)\n",
    "\n",
    "\n",
    "# def return_cls_preds_tars(single_cls_pred, single_cls_tars):\n",
    "#     print(\"single_cls_pred:\", single_cls_pred)\n",
    "#     print(\"single_cls_tars:\", single_cls_tars)\n",
    "\n",
    "# define confusion matrix for classification displaying\n",
    "def log_confusion_matrix_without_model(cls_preds, class_names,cls_tars, final_step, epoch, save):\n",
    "    # Use the model to predict the values from the validation dataset.\n",
    "#     seg_out, class_softmax_output = model.predict(test_inputs)\n",
    "    \n",
    "    # check the number of samples\n",
    "    num_s =  cls_preds.shape[0]\n",
    "    # binarize the preds\n",
    "    cls_preds = cls_preds>0.5\n",
    "    \n",
    "    \n",
    "#     cls_preds_ints = np.argmax(cls_preds, axis=1)\n",
    "#     print(\"cls_preds_ints:\", cls_preds_ints, cls_preds_ints.shape)\n",
    "#     print(\"cls_tars:\", cls_tars, cls_tars.shape)\n",
    "    \n",
    "    # Calculate the confusion matrix.\n",
    "#     cm = sklearn.metrics.confusion_matrix(cls_tars, cls_preds_ints,  labels=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
    "    cm = sklearn.metrics.confusion_matrix(cls_tars, cls_preds,  labels=[0, 1])  # for binary classification : cls_preds should not be probability ,has  to be either 1s or 0\n",
    "    print(\"cm:\", cm)\n",
    "    # Log the confusion matrix as an image summary.\n",
    "\n",
    "    figure = plot_confusion_matrix(cm, class_names, num_s,final_step, epoch, save)\n",
    "    \n",
    "    # calcualte norm cm for the later diagnal plots\n",
    "    norm_cm = np.around(cm.astype('float') / cm.sum(axis=1)[:, np.newaxis], decimals=2)\n",
    "    \n",
    "    return norm_cm\n",
    "#     cm_image = plot_to_image(figure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_losses_tb(writer, avg_losses, epoch):\n",
    "    \"\"\"\n",
    "     avg_losses = [total_loss, seg_loss, cls_loss]\n",
    "    \"\"\"\n",
    "    with writer.as_default():\n",
    "        print(\"writing train logs to tensorboard...\")                                                       \n",
    "        # write scalars to the tensorboard after each train step\n",
    "        tf.summary.scalar('total_loss', avg_losses[0].result(), step=epoch)\n",
    "        tf.summary.scalar('cls_BCE_loss1', avg_losses[1].result(), step=epoch)\n",
    "        tf.summary.scalar('cls_BCE_loss2', avg_losses[2].result(), step=epoch) \n",
    "        tf.summary.scalar('cls_BCE_loss3', avg_losses[3].result(), step=epoch)\n",
    "        \n",
    "\n",
    "def write_single_total_loss_tb(writer, avg_losses, name, epoch):\n",
    "    \"\"\"\n",
    "     avg_losses = [total_loss, seg_loss, cls_loss]\n",
    "    \"\"\"\n",
    "    with writer.as_default():\n",
    "        print(\"writing train logs to tensorboard...\")                                                       \n",
    "        # write scalars to the tensorboard after each train step\n",
    "        tf.summary.scalar(name, avg_losses.result(), step=epoch)\n",
    "     \n",
    "    \n",
    "def write_single_scalar_tb(writer, avg_losses, name, epoch):\n",
    "    \"\"\"\n",
    "     avg_losses = [total_loss, seg_loss, cls_loss]\n",
    "    \"\"\"\n",
    "    with writer.as_default():\n",
    "        print(\"writing train logs to tensorboard...\")                                                       \n",
    "        # write scalars to the tensorboard after each train step\n",
    "        tf.summary.scalar(name, avg_losses, step=epoch)\n",
    "        \n",
    "def train_display_save_at(model, Tr_dicom_paths, Tr_norm_input, Tr_norm_input_w_image, Tr_norm_seg_label,Tr_cls_label,   step, epoch, freq_step =200, save=True, Train_or_not=True):\n",
    "    if step % freq_step == 0 or step % (freq_step+1) ==0:\n",
    "        print(\"train image checking----------------------------------------------------------------------------------->\")\n",
    "#        (model, dicom_paths, norm_inputs,norm_input_w_images, norm_seg_targets, cls_targets, batch_idx, epoch, save=True, Train_or_not=True, Epoch_val =False\n",
    "        new_generate_images(model, Tr_dicom_paths, Tr_norm_input,Tr_norm_input_w_image, Tr_norm_seg_label,Tr_cls_label,  step, epoch, save=save, Train_or_not=Train_or_not)\n",
    "#         print('Epoch{}: {}/{}: total_loss: {}; cls_SCE_loss: {}; seg_total_loss: {} '.format(epoch, step, resampled_steps_per_epoch, total_loss, cls_SCE_loss, seg_total_loss))\n",
    "        \n",
    "# def test_display_save_at(val_dataset, model, step, epoch, freq_step= 100, save=True, Train_or_not=False):\n",
    "#     # every 100 batch step show the generate test image to check.\n",
    "#     if int(step) % freq_step == 0:\n",
    "#         display.clear_output(wait=True)\n",
    "           \n",
    "#         # take the 1st batch from the validation dataset\n",
    "#         for image_featuresx in val_dataset.take(1):\n",
    "\n",
    "# #         print(\"test input:\", image_features3)\n",
    "#         # feature extraction\n",
    "# #             TE_dicom_path3 =  image_features3[0]\n",
    "#             Tex_input_w_image =  image_featuresx[1]\n",
    "#             tex_input =  image_featuresx[2]\n",
    "#             Tex_seg_label = image_featuresx[3]\n",
    "#             tex_target = image_featuresx[4]\n",
    "# #         for tex_input, tex_target in test_dataset.take(1):\n",
    " \n",
    "#             #         plt.figure()\n",
    "#             #         plt.imshow(np.squeeze(tex_input[0]))\n",
    "#             #         plt.colorbar()\n",
    "#             #         plt.grid(False)\n",
    "#             #         plt.title(str(tex_target[0].numpy()))\n",
    "#             #         plt.show()\n",
    "# #           \n",
    "#             print(\"The 1st batch test image checking----------------------------------------------------------------------------------->\")\n",
    "#             new_generate_images(model, tex_input,Tex_input_w_image, Tex_seg_label, tex_target,step, epoch, save=save, Train_or_not=Train_or_not)\n",
    "            # every 400 steps save the genrate images\n",
    "\n",
    "\n",
    "\n",
    "def test_at_each_epoch(val_dataset, model, step, epoch, temp_loss,save=True, training=False):\n",
    "    # initialize \n",
    "    epoch_val_flag = True\n",
    "    # initialize for evaluating average metric\n",
    "    test_total_loss_mean = tf.keras.metrics.Mean()\n",
    "#     test_cls_BCE_loss1_mean = tf.keras.metrics.Mean()\n",
    "#     test_cls_BCE_loss2_mean = tf.keras.metrics.Mean()\n",
    "#     test_cls_BCE_loss3_mean = tf.keras.metrics.Mean()\n",
    "    # \n",
    "    cls_preds_entire = None\n",
    "    cls_tar_entire = None\n",
    "    \n",
    "#     #\n",
    "    test_loss_catches = []\n",
    "#     print(loss_catches.shape)\n",
    "\n",
    "\n",
    "    # start to go through entire validation dataset\n",
    "#     for  image_features3 in val_dataset.take(10):     # just for debug   $$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
    "#     for tex_input, tex_target in test_dataset.take(1):  \n",
    "    count =0\n",
    "    for dicom_paths, norm_inputs, norm_w_inputs, seg_labels, cls_labels in val_dataset:\n",
    "        count+=1\n",
    "        # one train step loss calculation and optimization  model2, inputs, cls_target, training\n",
    "        te_total_loss, pred = new_test_step(model, norm_inputs, cls_labels[1], training=False)\n",
    "        test_total_loss_mean(te_total_loss)\n",
    "        test_loss_catches.append(te_total_loss)\n",
    "#         test_cls_BCE_loss1_mean(te_cls_BCE_loss1)\n",
    "#         test_cls_BCE_loss2_mean(te_cls_BCE_loss2)\n",
    "#         test_cls_BCE_loss3_mean(te_cls_BCE_loss3)\n",
    "    \n",
    "        print('[Test]Epoch: {} batch:{} \\n  total_loss set loss: {}'.format(epoch, count, test_total_loss_mean.result()))\n",
    "        \n",
    "        if count %25 ==0:\n",
    "            display.clear_output(wait=True)\n",
    "        \n",
    "     \n",
    "        \n",
    "        # save epoch validation image in the folder every 10 batches save\n",
    "        \n",
    "#         if idx >10:  # just for debug   $$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
    "#             break    # just for debug   $$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
    "        \n",
    "        if count > n_positive_test_batches:  # when  evaluating negative val dataset using larger spacing\n",
    "            \n",
    "            if count % 500 ==0:\n",
    "                # (model, dicom_paths, norm_inputs,norm_input_w_images, norm_seg_targets, cls_targets, batch_idx, epoch, save=True, Train_or_not=True, Epoch_val =False\n",
    "                new_generate_images(model, dicom_paths, norm_inputs, norm_w_inputs, seg_labels, cls_labels , count,\n",
    "                                    epoch, save=save, Train_or_not=training, Epoch_val=epoch_val_flag) \n",
    "           \n",
    "        else:\n",
    "            new_generate_images(model, dicom_paths,norm_inputs,  norm_w_inputs, seg_labels, cls_labels ,count, \n",
    "                                epoch, save=save, Train_or_not=training, Epoch_val=epoch_val_flag)                                               \n",
    "#         display.clear_output(wait=True)\n",
    "\n",
    "         # Test input--------------batch input and targets to evaluate confusion matrix====================》\n",
    "        cls_preds = pred # predicts has shape [batch size, len(classes)]\n",
    "#         print(\"single batch targets shape:\", cls_labels.shape)\n",
    "#         print(\"single batch preds shape:\", cls_preds.shape)\n",
    "        if cls_preds_entire is None:\n",
    "            cls_preds_entire = cls_preds\n",
    "            cls_tar_entire = cls_labels[1].numpy()\n",
    "# #                   \n",
    "        else: \n",
    "            cls_preds_entire =  np.concatenate((cls_preds_entire, cls_preds), axis=0)\n",
    "            cls_tar_entire = np.concatenate((cls_tar_entire, cls_labels[1].numpy()), axis=0)\n",
    "            \n",
    "# #         idx +=1       # just for debug   $$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
    "            \n",
    "    print(\"preds_entire.shape\", cls_preds_entire, cls_preds_entire.shape)\n",
    "    print(\"cls_entire.shape\", cls_tar_entire, cls_tar_entire.shape)   # https://www.tensorflow.org/tensorboard/image_summaries\n",
    "\n",
    "    \n",
    "    # log the norm\n",
    "    norm_cm = log_confusion_matrix_without_model(cls_preds=cls_preds_entire, class_names=Class_names, cls_tars=cls_tar_entire, final_step =step, epoch=epoch, save=save)\n",
    "\n",
    "    epoch_val_flag = False # one epoch finish so set flag back\n",
    "    \n",
    "    \n",
    "#     # write logs to tensorboard:    avg_losses = [total_loss, seg_loss, cls_loss]\n",
    "#     # write test losses to the tensorboard\n",
    "    print(\"writing test logs to tensorboard...\") \n",
    "    write_single_total_loss_tb(val_summary_writer, test_total_loss_mean, \"total_loss_per_epoch\", epoch)\n",
    "\n",
    "  \n",
    "    # check whether needs to save the model\n",
    "    if test_total_loss_mean.result() < temp_loss:\n",
    "        save_path =  new_manager.save() # save the checkpoint and return the save path\n",
    "        print(\"Saved checkpoint for epoch {}-  step {}: {}\".format(epoch, step, save_path))\n",
    "        temp_loss =  test_total_loss_mean.result()\n",
    "\n",
    "    return norm_cm,   np.array(test_loss_catches).reshape((1,-1))\n",
    "\n",
    "# defin one epoch training for different dataset\n",
    "def train_one_epoch(train_pos_dataset_iter, train_neg_dataset_iter, cls_tar_check_distr,model, optimizer, step, epoch):\n",
    "    #  initializations at each epoch\n",
    "    train_total_loss_mean = tf.keras.metrics.Mean()\n",
    "    \n",
    "    for i in range(int(2*n_neg_train_batches)):\n",
    "        \n",
    "        if step % 2 ==0:\n",
    "            dicom_paths, norm_inputs, norm_w_inputs, seg_labels, cls_labels  =  next(train_neg_dataset_iter)     \n",
    "        else: \n",
    "            dicom_paths, norm_inputs, norm_w_inputs, seg_labels, cls_labels  =  next(train_pos_dataset_iter)\n",
    "            \n",
    "        cls_tar_check_distr.append(cls_labels[1][0])\n",
    "#         if step > 2 * n_neg_train_batches:  # due to positive and negative is sampled by turns , in order to view all the negative training samples.\n",
    "#             break\n",
    "        \n",
    "        \n",
    "        # one train step loss calculation and optimization\n",
    "        tr_total_loss = new_train_step(model, optimizer, norm_inputs, cls_labels[1], training=True)\n",
    "        train_total_loss_mean(tr_total_loss)\n",
    "\n",
    "        if optimizer.iterations.numpy() % 10 == 0: \n",
    "            write_single_total_loss_tb(train_summary_writer, train_total_loss_mean, \"train_total_loss_per_batch\", optimizer.iterations.numpy())\n",
    "            write_single_scalar_tb(train_summary_writer,  optimizer._decayed_lr(var_dtype=tf.float32), \"Optimizer Lr\", optimizer.iterations.numpy())\n",
    "\n",
    "        print('Epoch: {} batch:{} \\n Train total_loss set loss: {}'.format(epoch, int(step), train_total_loss_mean.result()))\n",
    "        \n",
    "        # check the current trained class distribution\n",
    "        plot_train_class_distrution(cls_tar_check_distr, int(step), epoch, freq = 50, save=True)\n",
    "        \n",
    "        #  model, Tr_dicom_paths, Tr_norm_input, Tr_norm_input_w_image, Tr_norm_seg_label,Tr_cls_label,   step, epoch, freq_step =100, save=True, Train_or_not=True\n",
    "        train_display_save_at(model, dicom_paths, norm_inputs, norm_w_inputs, seg_labels, cls_labels,  int(step), epoch, freq_step=200, save=True, Train_or_not=True )\n",
    "\n",
    "        if step % 25 ==0:\n",
    "            display.clear_output(wait=True)\n",
    "\n",
    "\n",
    "        step.assign_add(1)\n",
    "        \n",
    "    \n",
    "     # write train epoch average loss to the tensorboard\n",
    "    print(\"writing train logs to tensorboard...\") \n",
    "    write_single_total_loss_tb(train_summary_writer, train_total_loss_mean, \"total_loss_per_epoch\", epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take_new_train_batches =  3035\n",
    "def train_process(train_pos_dataset, train_neg_dataset, val_dataset, model, optimizer, step, start_epoch, total_epochs):\n",
    "    # some initial parameters\n",
    "    # test average_loss = for saving the test results\n",
    "    test_avg_tmp_loss = 100\n",
    "    norm_cm_diags = None   \n",
    "    cls_tar_check_distr = []\n",
    "    test_loss_epochs = None\n",
    "    tr_pos_iter= iter(train_pos_dataset)\n",
    "    tr_neg_iter= iter(train_neg_dataset)\n",
    "    for epoch in range(int(start_epoch), total_epochs+1):\n",
    "      \n",
    "    \n",
    "        # train one epoch， including loss calculation and optimization\n",
    "        train_one_epoch(tr_pos_iter, tr_neg_iter, cls_tar_check_distr, model, optimizer, step, epoch)\n",
    "        \n",
    "        # one training epoch ended start to evaluate the performance through entire test dataset\n",
    "        each_epoch_norm_cm, test_loss_catches_per_epoch = test_at_each_epoch(val_dataset, model, int(step)-1, epoch, temp_loss=test_avg_tmp_loss, save=True, training = False)\n",
    "        \n",
    "        # calculate epoch diagnals of confucion matrix\n",
    "        print(\"test_loss_catches_per_epoch.shape:\", test_loss_catches_per_epoch.shape)\n",
    "        norm_cm_diag_each_epoch = np.reshape(np.diag(each_epoch_norm_cm),(1, -1))\n",
    "        \n",
    "        if norm_cm_diags is None:\n",
    "            norm_cm_diags =  norm_cm_diag_each_epoch\n",
    "        else:\n",
    "            norm_cm_diags=  np.concatenate((norm_cm_diags, norm_cm_diag_each_epoch ), axis = 0)\n",
    "            \n",
    "        log_plot_confusion_matrix_at_epochs(norm_cm_diags, Class_names, epoch, at_epoch=7, save=True)\n",
    "        \n",
    "        # accumulated epoch loss\n",
    "        if test_loss_epochs is  None:\n",
    "            test_loss_epochs = test_loss_catches_per_epoch\n",
    "        else:\n",
    "            test_loss_epochs=  np.concatenate((test_loss_epochs,test_loss_catches_per_epoch ), axis = 0)\n",
    "            \n",
    "        # plot tepoch test losses\n",
    "        plot_batch_test_loss(test_loss_epochs, epoch)\n",
    "        \n",
    "        \n",
    "            \n",
    "      \n",
    "        # when epoch fisnihed add epoch counter and reset the batch step \n",
    "        start_epoch.assign_add(1)     \n",
    "        step.assign(1)\n",
    "        \n",
    "        \n",
    "    # when training finihsed\n",
    "    print(\"training finished\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "#     init_lr = 1e-3\n",
    "#     learning_rate_fn = tf.keras.optimizers.schedules.PolynomialDecay(\n",
    "#       initial_learning_rate=init_lr,\n",
    "#       decay_steps=1000,\n",
    "#       end_learning_rate=0.0)\n",
    "    learning_rate_fn = 1e-5\n",
    "    new_optimizer =  tf.keras.optimizers.Adam(learning_rate_fn)\n",
    "    # build_new check point manager\n",
    "    new_ckpt_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "    if not os.path.exists(new_ckpt_prefix):\n",
    "        os.makedirs(new_ckpt_prefix)\n",
    "    #  contents of states to be saved as attributes on the checkpoint object\n",
    "    new_ckpt_ob = tf.train.Checkpoint(step= tf.Variable(1),\n",
    "                                        epoch=  tf.Variable(1),\n",
    "                                        optimizer=new_optimizer,\n",
    "                                         model =  classifier\n",
    "                                     )\n",
    "    # define checkpoint manager\n",
    "    new_manager =  tf.train.CheckpointManager(new_ckpt_ob, new_ckpt_prefix, max_to_keep=1)\n",
    "\n",
    "    # check the whether there is a checkpoint in the checkpoint folder, if it is restore from it\n",
    "    if new_manager.latest_checkpoint:\n",
    "        print(\"Restored from {}\".format(new_manager.latest_checkpoint))\n",
    "        new_ckpt_ob.restore(new_manager.latest_checkpoint)\n",
    "    else:\n",
    "        print(\"Initializing from scratch.\")\n",
    "\n",
    "    # reset checkcpoint.step for each epoch\n",
    "    step =  new_ckpt_ob.step\n",
    "    ckpt_epoch =  new_ckpt_ob.epoch  \n",
    "    optimizer = new_ckpt_ob.optimizer\n",
    "    model = new_ckpt_ob.model\n",
    "    epochs = 10\n",
    "    # start train_process\n",
    "    train_process(train_pos_dataset, train_neg_dataset, test_dataset, model, optimizer, step, ckpt_epoch, total_epochs=epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
