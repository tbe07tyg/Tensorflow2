{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## autoenocder with own dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D, Reshape, Dropout, UpSampling2D\n",
    "from tensorflow.keras.callbacks import LambdaCallback\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "from glob import glob\n",
    "import math\n",
    "from tensorflow import io\n",
    "\n",
    "# import pydicom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_images_root:  ['/media/ytx/Japan_Deep_Data/dataset/LeiSang/myTry/BleedingDataDCM/train/ZA-006_000', '/media/ytx/Japan_Deep_Data/dataset/LeiSang/myTry/BleedingDataDCM/train/ZA-008_000', '/media/ytx/Japan_Deep_Data/dataset/LeiSang/myTry/BleedingDataDCM/train/ZA-012_000', '/media/ytx/Japan_Deep_Data/dataset/LeiSang/myTry/BleedingDataDCM/train/ZA-015_001', '/media/ytx/Japan_Deep_Data/dataset/LeiSang/myTry/BleedingDataDCM/train/ZA-017_001', '/media/ytx/Japan_Deep_Data/dataset/LeiSang/myTry/BleedingDataDCM/train/ZA-019_001', '/media/ytx/Japan_Deep_Data/dataset/LeiSang/myTry/BleedingDataDCM/train/ZA-020_002', '/media/ytx/Japan_Deep_Data/dataset/LeiSang/myTry/BleedingDataDCM/train/ZA-021_001', '/media/ytx/Japan_Deep_Data/dataset/LeiSang/myTry/BleedingDataDCM/train/ZA-022_001', '/media/ytx/Japan_Deep_Data/dataset/LeiSang/myTry/BleedingDataDCM/train/ZA-023_001', '/media/ytx/Japan_Deep_Data/dataset/LeiSang/myTry/BleedingDataDCM/train/ZA-024_001', '/media/ytx/Japan_Deep_Data/dataset/LeiSang/myTry/BleedingDataDCM/train/ZA-027_001', '/media/ytx/Japan_Deep_Data/dataset/LeiSang/myTry/BleedingDataDCM/train/ZA-029_001', '/media/ytx/Japan_Deep_Data/dataset/LeiSang/myTry/BleedingDataDCM/train/ZA-031_001', '/media/ytx/Japan_Deep_Data/dataset/LeiSang/myTry/BleedingDataDCM/train/ZA-032_001', '/media/ytx/Japan_Deep_Data/dataset/LeiSang/myTry/BleedingDataDCM/train/ZA-033_001', '/media/ytx/Japan_Deep_Data/dataset/LeiSang/myTry/BleedingDataDCM/train/ZA-035_001', '/media/ytx/Japan_Deep_Data/dataset/LeiSang/myTry/BleedingDataDCM/train/ZA-036_001', '/media/ytx/Japan_Deep_Data/dataset/LeiSang/myTry/BleedingDataDCM/train/ZA-037_001', '/media/ytx/Japan_Deep_Data/dataset/LeiSang/myTry/BleedingDataDCM/train/ZA-038_001']\n",
      "val_images_root:  ['/media/ytx/Japan_Deep_Data/dataset/LeiSang/myTry/BleedingDataDCM/val/ZA-039_001', '/media/ytx/Japan_Deep_Data/dataset/LeiSang/myTry/BleedingDataDCM/val/ZA-040_001', '/media/ytx/Japan_Deep_Data/dataset/LeiSang/myTry/BleedingDataDCM/val/ZA-041_001']\n",
      "train paths--------------------------------------------------------------->\n",
      "root: /media/ytx/Japan_Deep_Data/dataset/LeiSang/myTry/BleedingDataDCM/train/ZA-006_000\n",
      "one_root_paths: 1251\n",
      "\n",
      "train_all_image_paths: 1251\n",
      "root: /media/ytx/Japan_Deep_Data/dataset/LeiSang/myTry/BleedingDataDCM/train/ZA-008_000\n",
      "one_root_paths: 1182\n",
      "\n",
      "train_all_image_paths: 2433\n",
      "root: /media/ytx/Japan_Deep_Data/dataset/LeiSang/myTry/BleedingDataDCM/train/ZA-012_000\n",
      "one_root_paths: 665\n",
      "\n",
      "train_all_image_paths: 3098\n",
      "root: /media/ytx/Japan_Deep_Data/dataset/LeiSang/myTry/BleedingDataDCM/train/ZA-015_001\n",
      "one_root_paths: 1132\n",
      "\n",
      "train_all_image_paths: 4230\n",
      "root: /media/ytx/Japan_Deep_Data/dataset/LeiSang/myTry/BleedingDataDCM/train/ZA-017_001\n",
      "one_root_paths: 1226\n",
      "\n",
      "train_all_image_paths: 5456\n",
      "root: /media/ytx/Japan_Deep_Data/dataset/LeiSang/myTry/BleedingDataDCM/train/ZA-019_001\n",
      "one_root_paths: 1176\n",
      "\n",
      "train_all_image_paths: 6632\n",
      "root: /media/ytx/Japan_Deep_Data/dataset/LeiSang/myTry/BleedingDataDCM/train/ZA-020_002\n",
      "one_root_paths: 1251\n",
      "\n",
      "train_all_image_paths: 7883\n",
      "root: /media/ytx/Japan_Deep_Data/dataset/LeiSang/myTry/BleedingDataDCM/train/ZA-021_001\n",
      "one_root_paths: 1219\n",
      "\n",
      "train_all_image_paths: 9102\n",
      "root: /media/ytx/Japan_Deep_Data/dataset/LeiSang/myTry/BleedingDataDCM/train/ZA-022_001\n",
      "one_root_paths: 1251\n",
      "\n",
      "train_all_image_paths: 10353\n",
      "root: /media/ytx/Japan_Deep_Data/dataset/LeiSang/myTry/BleedingDataDCM/train/ZA-023_001\n",
      "one_root_paths: 869\n",
      "\n",
      "train_all_image_paths: 11222\n",
      "root: /media/ytx/Japan_Deep_Data/dataset/LeiSang/myTry/BleedingDataDCM/train/ZA-024_001\n",
      "one_root_paths: 1113\n",
      "\n",
      "train_all_image_paths: 12335\n",
      "root: /media/ytx/Japan_Deep_Data/dataset/LeiSang/myTry/BleedingDataDCM/train/ZA-027_001\n",
      "one_root_paths: 1232\n",
      "\n",
      "train_all_image_paths: 13567\n",
      "root: /media/ytx/Japan_Deep_Data/dataset/LeiSang/myTry/BleedingDataDCM/train/ZA-029_001\n",
      "one_root_paths: 1251\n",
      "\n",
      "train_all_image_paths: 14818\n",
      "root: /media/ytx/Japan_Deep_Data/dataset/LeiSang/myTry/BleedingDataDCM/train/ZA-031_001\n",
      "one_root_paths: 1169\n",
      "\n",
      "train_all_image_paths: 15987\n",
      "root: /media/ytx/Japan_Deep_Data/dataset/LeiSang/myTry/BleedingDataDCM/train/ZA-032_001\n",
      "one_root_paths: 1013\n",
      "\n",
      "train_all_image_paths: 17000\n",
      "root: /media/ytx/Japan_Deep_Data/dataset/LeiSang/myTry/BleedingDataDCM/train/ZA-033_001\n",
      "one_root_paths: 1076\n",
      "\n",
      "train_all_image_paths: 18076\n",
      "root: /media/ytx/Japan_Deep_Data/dataset/LeiSang/myTry/BleedingDataDCM/train/ZA-035_001\n",
      "one_root_paths: 1182\n",
      "\n",
      "train_all_image_paths: 19258\n",
      "root: /media/ytx/Japan_Deep_Data/dataset/LeiSang/myTry/BleedingDataDCM/train/ZA-036_001\n",
      "one_root_paths: 1751\n",
      "\n",
      "train_all_image_paths: 21009\n",
      "root: /media/ytx/Japan_Deep_Data/dataset/LeiSang/myTry/BleedingDataDCM/train/ZA-037_001\n",
      "one_root_paths: 1213\n",
      "\n",
      "train_all_image_paths: 22222\n",
      "root: /media/ytx/Japan_Deep_Data/dataset/LeiSang/myTry/BleedingDataDCM/train/ZA-038_001\n",
      "one_root_paths: 1013\n",
      "\n",
      "train_all_image_paths: 23235\n",
      "val paths--------------------------------------------------------------->\n",
      "root: /media/ytx/Japan_Deep_Data/dataset/LeiSang/myTry/BleedingDataDCM/val/ZA-039_001\n",
      "one_root_paths: 1344\n",
      "\n",
      "root: /media/ytx/Japan_Deep_Data/dataset/LeiSang/myTry/BleedingDataDCM/val/ZA-040_001\n",
      "one_root_paths: 1369\n",
      "\n",
      "root: /media/ytx/Japan_Deep_Data/dataset/LeiSang/myTry/BleedingDataDCM/val/ZA-041_001\n",
      "one_root_paths: 17\n",
      "\n",
      "Found 23235 training images\n",
      "Found 2730 validation images\n"
     ]
    }
   ],
   "source": [
    "#  set dataset path\n",
    "train_images_root = sorted(glob('/media/ytx/Japan_Deep_Data/dataset/LeiSang/myTry/BleedingDataDCM/train/*'))\n",
    "# train_masks = sorted(glob('I:/dataset/infaredSublingualVein/train/tongue_labels/*'))\n",
    "\n",
    "val_images_root = sorted(glob('/media/ytx/Japan_Deep_Data/dataset/LeiSang/myTry/BleedingDataDCM/val/*'))\n",
    "# val_masks = sorted(glob('I:/dataset/infaredSublingualVein/validation/tongue_labels/*'))\n",
    "print(\"train_images_root: \", train_images_root)\n",
    "print(\"val_images_root: \", val_images_root)\n",
    "\n",
    "train_all_image_paths =  []\n",
    "val_all_image_paths = []\n",
    "\n",
    "print(\"train paths--------------------------------------------------------------->\")\n",
    "for each in train_images_root:\n",
    "    print(\"root:\", each)\n",
    "    one_root_paths =  sorted(glob(each +'/*.DCM'))\n",
    "    print(\"one_root_paths:\", len(one_root_paths))\n",
    "    print()\n",
    "    train_all_image_paths = train_all_image_paths+ one_root_paths\n",
    "    print(\"train_all_image_paths:\", len(train_all_image_paths))\n",
    "\n",
    "print(\"val paths--------------------------------------------------------------->\")\n",
    "for each in val_images_root:\n",
    "    print(\"root:\", each)\n",
    "    one_root_paths =  sorted(glob(each +'/*.DCM'))\n",
    "    print(\"one_root_paths:\", len(one_root_paths))\n",
    "    print()\n",
    "    val_all_image_paths = val_all_image_paths+ one_root_paths\n",
    "print(f'Found {len(train_all_image_paths)} training images')\n",
    "print(f'Found {len(val_all_image_paths)} validation images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch size: 2\n",
      "total_num_batches per epoch: 11618\n",
      "input image_size: 512\n"
     ]
    }
   ],
   "source": [
    "# hyperparameter\n",
    "encoding_size = 32\n",
    "batch_size =  2\n",
    "image_size = 512\n",
    "img_width = img_height = image_size \n",
    "total_num_batches_per_epoch = math.ceil(len(train_all_image_paths) / batch_size)\n",
    "\n",
    "total_num_batches_per_val = math.ceil(len(val_all_image_paths) / batch_size)\n",
    "print(\"batch size:\", batch_size)\n",
    "print(\"total_num_batches per epoch:\", total_num_batches_per_epoch)\n",
    "print(\"input image_size:\", image_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dcm \n",
    "# !pip install -q tensorflow-io-nightly\n",
    "\n",
    "import tensorflow_io as tfio\n",
    "import matplotlib.pyplot as plt\n",
    "def load_dcm(dcm_path, dcm=False):\n",
    "   \n",
    "    if dcm:\n",
    "        \n",
    "        print(\"read dcm data\")\n",
    "        _bytes = tf.io.read_file(dcm_path)\n",
    "        img = tfio.image.decode_dicom_image( _bytes)  # defualt on_error= \"strict\": throw an error if can not throw one except; scale =  perserve defulat means keeps the value they are\n",
    "        print(img.shape)  # (1, 512, 512) after decode image shape is this, WHICH CAN BE SEEN FROM PLTDicom.ipynb\n",
    "        \n",
    "        img =  tf.squeeze(img) # squeez the dimension to (512, 512)\n",
    "        img =  tf.expand_dims(img, axis =-1) # expand the last dimension to 1, as (512, 512, 1)\n",
    "        img.set_shape([None, None, 1])  # need to set the shape because the shape will becomes unknown with preprocessing function load\n",
    "    else:\n",
    "        \n",
    "       raise \"please choose dcm as input formart\"\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing\n",
    "def resize(image):\n",
    "    print(image.shape)\n",
    "    resized_image = tf.image.resize(image, size=[image_size, image_size], method='bilinear')\n",
    "    return resized_image\n",
    "\n",
    "def std_norm(image):\n",
    "    image = tf.image.per_image_standardization(image)\n",
    "    return image\n",
    "\n",
    "\n",
    "def random_flip_auto(image):\n",
    "    flip = tf.random.uniform(\n",
    "        shape=[1, ], minval=0, maxval=2, dtype=tf.int32)[0]\n",
    "    image = tf.case([\n",
    "        (tf.greater(flip, 0), lambda: tf.image.flip_left_right(image))\n",
    "    ], default=lambda: image)\n",
    "    return image\n",
    "\n",
    "@tf.function()\n",
    "def train_preprocess_inputs_auto(image_path):\n",
    "    print(image_path)\n",
    "    with tf.device('/cpu:0'):\n",
    "      \n",
    "        # image = load_image(image_path) # infraed image input. there for 8 bit input\n",
    "        image = tf.cast(load_dcm(image_path, dcm=True), tf.float32)  # infraed image input. there for 8 bit input\n",
    "        \n",
    "        print(\"load image shape:\", image.shape)\n",
    "#         mask = load_image(mask_path, mask=True)\n",
    "#         mask = tf.cast(mask > 0, dtype=tf.float32)\n",
    "        print(image)\n",
    "#         image = resize(image)\n",
    "        # image, mask = random_scale(image, mask) # random resize\n",
    "        image = std_norm(image)  # norm before padding and crop_pad\n",
    "        # image, mask = pad_inputs(image, mask)  # and pad to raw size\n",
    "        # image, mask = random_crop(image, mask)  #\n",
    "        image = random_flip_auto(image)\n",
    "        print(\"prepro image shape:\", image.shape)\n",
    "        return image, image\n",
    "\n",
    "#         image = resize(image)import tensorflow_io as tfio\n",
    "@tf.function()\n",
    "def val_preprocess_inputs_auto(image_path):\n",
    "    print(image_path)\n",
    "    with tf.device('/cpu:0'):\n",
    "      \n",
    "        # image = load_image(image_path) # infraed image input. there for 8 bit input\n",
    "        image = tf.cast(load_dcm(image_path, dcm=True), tf.float32)  # infraed image input. there for 8 bit input\n",
    "        \n",
    "        print(\"load image shape:\", image.shape)\n",
    "#         mask = load_image(mask_path, mask=True)\n",
    "#         mask = tf.cast(mask > 0, dtype=tf.float32)\n",
    "        print(image)\n",
    "#         image = resize(image)\n",
    "        # image, mask = random_scale(image, mask) # random resize\n",
    "        image = std_norm(image)  # norm before padding and crop_pad\n",
    "        # image, mask = pad_inputs(image, mask)  # and pad to raw size\n",
    "        # image, mask = random_crop(image, mask)  #\n",
    "        image = random_flip_auto(image)\n",
    "        print(\"prepro image shape:\", image.shape)\n",
    "        return image, image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The entire training dataset is: 23235\n",
      "The entire validation dataset is: 2730\n"
     ]
    }
   ],
   "source": [
    "print(\"The entire training dataset is:\", len(train_all_image_paths))\n",
    "print(\"The entire validation dataset is:\", len(val_all_image_paths))\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(train_all_image_paths)\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices(val_all_image_paths)\n",
    "# for idx, data in enumerate(train_dataset):\n",
    "#     print(idx)\n",
    "#     print(data)\n",
    "    \n",
    "# train_dataset = train_dataset.shuffle(1024)\n",
    "train_dataset = train_dataset.map(map_func=train_preprocess_inputs_auto,\n",
    "                                  num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "train_dataset = train_dataset.batch(batch_size=batch_size, drop_remainder=True) # drop reminder... if true batch= 6 otherwise =7\n",
    "# train_dataset = train_dataset.repeat(1000)\n",
    "train_dataset = train_dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "val_dataset = val_dataset.map(map_func=val_preprocess_inputs_auto,\n",
    "                                  num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "val_dataset = val_dataset.batch(batch_size=batch_size, drop_remainder=True) # drop reminder... if true batch= 6 otherwise =7\n",
    "# train_dataset = train_dataset.repeat(1000)\n",
    "val_dataset = val_dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "# for idx, data in enumerate(train_dataset):\n",
    "#     print(idx)\n",
    "#     print(data[0].shape)\n",
    "\n",
    "\n",
    "# for idx, data in enumerate(val_dataset):\n",
    "#     print(idx)\n",
    "#     print(data[0].shape)\n",
    "# print(\"train dataset is ok\")\n",
    "# print(\"val dataset is ok\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 512, 512, 1)]     0         \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 512, 512, 64)      640       \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 512, 512, 64)      36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 256, 256, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 256, 256, 128)     73856     \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 256, 256, 128)     147584    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 128, 128, 128)     0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 128, 128, 256)     295168    \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 128, 128, 256)     590080    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 64, 64, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 64, 64, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 64, 64, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 64, 64, 512)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 32, 32, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 32, 32, 1024)      4719616   \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 32, 32, 1024)      9438208   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 32, 32, 1024)      0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d (UpSampling2D) (None, 64, 64, 1024)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 64, 64, 512)       2097664   \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 64, 64, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 64, 64, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "up_sampling2d_1 (UpSampling2 (None, 128, 128, 512)     0         \n",
      "_________________________________________________________________\n",
      "conv2d_13 (Conv2D)           (None, 128, 128, 256)     524544    \n",
      "_________________________________________________________________\n",
      "conv2d_14 (Conv2D)           (None, 128, 128, 256)     590080    \n",
      "_________________________________________________________________\n",
      "conv2d_15 (Conv2D)           (None, 128, 128, 256)     590080    \n",
      "_________________________________________________________________\n",
      "up_sampling2d_2 (UpSampling2 (None, 256, 256, 256)     0         \n",
      "_________________________________________________________________\n",
      "conv2d_16 (Conv2D)           (None, 256, 256, 128)     131200    \n",
      "_________________________________________________________________\n",
      "conv2d_17 (Conv2D)           (None, 256, 256, 128)     147584    \n",
      "_________________________________________________________________\n",
      "conv2d_18 (Conv2D)           (None, 256, 256, 128)     147584    \n",
      "_________________________________________________________________\n",
      "up_sampling2d_3 (UpSampling2 (None, 512, 512, 128)     0         \n",
      "_________________________________________________________________\n",
      "conv2d_19 (Conv2D)           (None, 512, 512, 64)      32832     \n",
      "_________________________________________________________________\n",
      "conv2d_20 (Conv2D)           (None, 512, 512, 64)      36928     \n",
      "_________________________________________________________________\n",
      "conv2d_21 (Conv2D)           (None, 512, 512, 64)      36928     \n",
      "_________________________________________________________________\n",
      "conv2d_22 (Conv2D)           (None, 512, 512, 2)       1154      \n",
      "_________________________________________________________________\n",
      "conv2d_23 (Conv2D)           (None, 512, 512, 1)       3         \n",
      "=================================================================\n",
      "Total params: 27,898,245\n",
      "Trainable params: 27,898,245\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Failed to import pydot. You must install pydot and graphviz for `pydotprint` to work.\n"
     ]
    }
   ],
   "source": [
    "# # create model\n",
    "def U_NetV2():\n",
    "    inputs = keras.layers.Input((image_size, image_size, 1))\n",
    "    conv1 = Conv2D(64, 3, activation='relu', padding='same', kernel_initializer='he_normal')(inputs)\n",
    "    conv1 = Conv2D(64, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv1)\n",
    "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "    conv2 = Conv2D(128, 3, activation='relu', padding='same', kernel_initializer='he_normal')(pool1)\n",
    "    conv2 = Conv2D(128, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv2)\n",
    "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "    conv3 = Conv2D(256, 3, activation='relu', padding='same', kernel_initializer='he_normal')(pool2)\n",
    "    conv3 = Conv2D(256, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv3)\n",
    "    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
    "    conv4 = Conv2D(512, 3, activation='relu', padding='same', kernel_initializer='he_normal')(pool3)\n",
    "    conv4 = Conv2D(512, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv4)\n",
    "    drop4 = Dropout(0.5)(conv4)\n",
    "    pool4 = MaxPooling2D(pool_size=(2, 2))(drop4)\n",
    "\n",
    "    conv5 = Conv2D(1024, 3, activation='relu', padding='same', kernel_initializer='he_normal')(pool4)\n",
    "    conv5 = Conv2D(1024, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv5)\n",
    "    drop5 = Dropout(0.5)(conv5)\n",
    "\n",
    "    up6 = Conv2D(512, 2, activation='relu', padding='same', kernel_initializer='he_normal')(\n",
    "        UpSampling2D(size=(2, 2))(drop5))\n",
    "#     merge6 = concatenate([drop4, up6], axis=3)\n",
    "    conv6 = Conv2D(512, 3, activation='relu', padding='same', kernel_initializer='he_normal')(up6)\n",
    "    conv6 = Conv2D(512, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv6)\n",
    "\n",
    "    up7 = Conv2D(256, 2, activation='relu', padding='same', kernel_initializer='he_normal')(\n",
    "        UpSampling2D(size=(2, 2))(conv6))\n",
    "#     merge7 = concatenate([conv3, up7], axis=3)\n",
    "    conv7 = Conv2D(256, 3, activation='relu', padding='same', kernel_initializer='he_normal')(up7)\n",
    "    conv7 = Conv2D(256, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv7)\n",
    "\n",
    "    up8 = Conv2D(128, 2, activation='relu', padding='same', kernel_initializer='he_normal')(\n",
    "        UpSampling2D(size=(2, 2))(conv7))\n",
    "#     merge8 = concatenate([conv2, up8], axis=3)\n",
    "    conv8 = Conv2D(128, 3, activation='relu', padding='same', kernel_initializer='he_normal')(up8)\n",
    "    conv8 = Conv2D(128, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv8)\n",
    "\n",
    "    up9 = Conv2D(64, 2, activation='relu', padding='same', kernel_initializer='he_normal')(\n",
    "        UpSampling2D(size=(2, 2))(conv8))\n",
    "#     merge9 = concatenate([conv1, up9], axis=3)\n",
    "    conv9 = Conv2D(64, 3, activation='relu', padding='same', kernel_initializer='he_normal')(up9)\n",
    "    conv9 = Conv2D(64, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv9)\n",
    "    conv9 = Conv2D(2, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv9)\n",
    "    outputs = Conv2D(1, 1, activation='linear')(conv9)\n",
    "    model = keras.models.Model(inputs, outputs)\n",
    "    return model\n",
    "model =U_NetV2()\n",
    "model.compile(loss='mse', optimizer='adam',\n",
    "              metrics=['mse'])\n",
    "model.summary()\n",
    "\n",
    "keras.utils.plot_model(model, show_shapes=True, dpi=200, expand_nested=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the image call back\n",
    "import os\n",
    "from datetime import datetime\n",
    "logdir =  os.path.join(\"logs\",\"image\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "# Define the basic TensorBoard callback.\n",
    "tensorboard_callback = keras.callbacks.TensorBoard(log_dir=logdir)\n",
    "file_writer_img = tf.summary.create_file_writer(logdir + '/img')\n",
    "\n",
    "def draw_input_output(epoch, logs):\n",
    "    output = model.predict(val_dataset)\n",
    "    print(output.shape)\n",
    "    with file_writer_img.as_default():\n",
    "        tf.summary.image(\"test_output\", tf.reshape(output,[-1, image_size , image_size , 1]), step=epoch)\n",
    "#         tf.summary.image(\"test_noisy_input\", tf.reshape(x_test_noisy,[-1, image_size , image_size , 1]), step=epoch)\n",
    "        tf.summary.image(\"test_input\", tf.reshape(val_dataset,[-1, image_size , image_size , 1]), step=epoch)\n",
    "log_mg = keras.callbacks.LambdaCallback(on_epoch_end=draw_input_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 11618 steps, validate for 1365 steps\n",
      "Epoch 1/30\n",
      "  397/11618 [>.............................] - ETA: 37:28 - loss: 1.0000 - mse: 1.0000"
     ]
    }
   ],
   "source": [
    "# Fit the model\n",
    "# !export LD_LIBRARY_PATH=/usr/local/cuda/extras/CUPTI/lib64:$LD_LIBRARY_PATH\n",
    "model.fit(train_dataset, epochs=30, steps_per_epoch=total_num_batches_per_epoch, validation_steps=total_num_batches_per_val,validation_data=val_dataset,\n",
    "          callbacks=[tensorboard_callback, log_mg],verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open tensorboard\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
