{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pydicom in e:\\projects\\intepreters\\anaconda\\envs\\tf21\\lib\\site-packages (1.4.2)\n",
      "Requirement already satisfied: xlrd in e:\\projects\\intepreters\\anaconda\\envs\\tf21\\lib\\site-packages (1.2.0)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "!pip install pydicom\n",
    "import pydicom\n",
    "from IPython import display\n",
    "!pip install xlrd\n",
    "import xlrd\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def _bytes_feature(value):\n",
    "  \"\"\"Returns a bytes_list from a string / byte.\"\"\"\n",
    "  if isinstance(value, type(tf.constant(0))):\n",
    "    value = value.numpy() # BytesList won't unpack a string from an EagerTensor.\n",
    "  return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "def _float_feature(value):\n",
    "  \"\"\"Returns a float_list from a float / double.\"\"\"\n",
    "  return tf.train.Feature(float_list=tf.train.FloatList(value=[value]))\n",
    "\n",
    "def _int64_feature(value):\n",
    "  \"\"\"Returns an int64_list from a bool / enum / int / uint.\"\"\"\n",
    "  return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n",
    "\n",
    "\n",
    "# Create a dictionary with features that may be relevant. # create tf.example message\n",
    "def image_example(each_dicom_path, seg_label, cls_label):\n",
    "    feature = {\n",
    "      \n",
    "        'dicom_path': _bytes_feature(each_dicom_path.encode()),\n",
    "        'seg_label': _bytes_feature(seg_label.tostring()), \n",
    "        'cls_label': _int64_feature(cls_label),\n",
    "        \n",
    "    }\n",
    "\n",
    "    return tf.train.Example(features=tf.train.Features(feature=feature))\n",
    "\n",
    "def make_samples_ubuntu(root_folder_path_for_cases, excel_loc, train_or_val, marked_datatype= np.ubyte):\n",
    "    \n",
    "    \n",
    "    # get case_list\n",
    "    case_list = glob(root_folder_path_for_cases + \"*\")\n",
    "    # print(\"case_list:\", case_list)\n",
    "    print(\"len of case_list:\", len(case_list))\n",
    "    \n",
    "    \n",
    "    # get work_book of excel\n",
    "    train_wb = xlrd.open_workbook(excel_loc)\n",
    "    # get train filp colomn\n",
    "    sheet =  train_wb.sheet_by_index(0)\n",
    "    print(\"sheet:\", sheet)\n",
    "\n",
    "    # extract_number_of rows\n",
    "    print(sheet.nrows)\n",
    "    \n",
    "    case_names =  []\n",
    "    case_flips = []\n",
    "    for i in range(1, sheet.nrows):\n",
    "        case_names.append(sheet.cell_value(i, 0))\n",
    "        case_flips.append(sheet.cell_value(i, 1))\n",
    "\n",
    "    # convert flip notes into integer \n",
    "    print(\"case_flips:\", case_flips)\n",
    "    case_flips =[int(s) for s in case_flips] \n",
    "    print(\"case_names:\", case_names)\n",
    "    print(\"case_flips:\", case_flips)\n",
    "    print(\"len of case data:\", len(case_names))\n",
    "    print(\"len of flip data:\", len(case_flips))\n",
    "\n",
    "    # check case list\n",
    "    print(\"case list len:\", len(case_list))\n",
    "    assert len(case_names) ==  len(case_flips), \" len of cases names does not macthed with flip len in train excel \"\n",
    "    assert len(case_list) ==  len(case_names), \" len of cases in the train folder not macthed with cases in train excel \"\n",
    "\n",
    "    my_positive_exmaple ={}\n",
    "    my_negative_exmaple={}\n",
    "    \n",
    "    for case_index, each_case in enumerate(case_list):\n",
    "        print(each_case)\n",
    "        # get case name from case list:\n",
    "        each_case_str = os.path.split(each_case) \n",
    "        print(\"each_case_str:\", each_case_str)\n",
    "        case_name =  each_case_str[-1]\n",
    "        print(\"case name:\", case_name)\n",
    "        print(\"case index:\", i, \"case name:\", case_names[case_index])\n",
    "        # check case name match or not\n",
    "        assert case_name ==  case_names[case_index]\n",
    "\n",
    "        # read_labels for specified case name\n",
    "        raw_files = glob(each_case + \"/*.raw\")\n",
    "        dicom_files = glob(each_case + \"/*.DCM\")\n",
    "        # get label path\n",
    "        for each_path in raw_files:\n",
    "            if \"marked\" in each_path:\n",
    "                label_path =  each_path\n",
    "        print(\"label path:\", label_path)\n",
    "        # read labels\n",
    "        labels = np.fromfile(label_path, dtype = marked_datatype)\n",
    "        reshaped_labels =  labels.reshape([-1, 512, 512])\n",
    "        print(\"reshape labels shape:\", reshaped_labels.shape)\n",
    "        print(\"len of dicoms in this case:\", len(dicom_files))\n",
    "        assert  reshaped_labels.shape[0] ==  len(dicom_files)  , 'len of dicoms is not equal to the raw label values'\n",
    "\n",
    "       # check whether needs to flip the label\n",
    "        if case_flips[case_index] ==1: # flip the enitre label\n",
    "            train_final_case_labels = np.flip(reshaped_labels, 0)\n",
    "        else: \n",
    "            train_final_case_labels =  reshaped_labels\n",
    "\n",
    "        None_zero_label_index = []\n",
    "        # read inputs\n",
    "        for dicom_index, each_dicom in enumerate(dicom_files):\n",
    "            path_split = os.path.split(each_dicom)\n",
    "            print(path_split)\n",
    "            file_name =  path_split[-1]\n",
    "            print(file_name)\n",
    "            \n",
    "           \n",
    "            each_slice_label =  train_final_case_labels[dicom_index]\n",
    "            # count non_zeros value from the each slice label\n",
    "            num_nonzero = np.count_nonzero(each_slice_label)\n",
    "            \n",
    "            if train_or_val == \"train\":\n",
    "                each_dicom =  \"/media/ytx/Japan_Deep_Data/dataset/LeiSang/myTry/BleedingDataDCM/train/\" + case_name +\"/\" +file_name\n",
    "            else:\n",
    "                each_dicom =  \"/media/ytx/Japan_Deep_Data/dataset/LeiSang/myTry/BleedingDataDCM/val/\" + case_name+ \"/\" +file_name\n",
    "            \n",
    "            if num_nonzero > 10:  # only consider the # of nonzeros values >10 to be the positive labels\n",
    "                each_slice_cls_label = 1\n",
    "                print(\"num_nonzero:\", num_nonzero)\n",
    "                None_zero_label_index.append(dicom_index+1)\n",
    "                print(\"each_slice_label shape:\", each_slice_label.shape)\n",
    "                print(\"each_slice label range before binarization: [{}, {}]\".format(each_slice_label.min(), each_slice_label.max()))\n",
    "\n",
    "                # binarize the mask label\n",
    "                each_slice_label[each_slice_label> 0] = 255\n",
    "                print(\"each_slice label range before after binarization: [{}, {}]\".format(each_slice_label.min(), each_slice_label.max()))\n",
    "\n",
    "                # genrate positive examples\n",
    "                my_positive_exmaple[each_dicom]=(each_slice_label, each_slice_cls_label)\n",
    "\n",
    "            else: # negative examples\n",
    "                each_slice_cls_label = 0\n",
    "                my_negative_exmaple[each_dicom]=(each_slice_label, each_slice_cls_label)\n",
    "\n",
    "            if dicom_index % 20 == 0:\n",
    "                    display.clear_output(wait=True)  \n",
    "\n",
    "            if dicom_index == len(dicom_files)-1:\n",
    "                print(\"total \" + str(len(dicom_files)) + \" checked ---------------------------------------------------------------------------->\")\n",
    "\n",
    "   \n",
    "        # check the length for each case\n",
    "        print(\"generate positive {} positive samples at currently case step:\".format(len(my_positive_exmaple)))\n",
    "        print(\"generate positive {} negative samples at currently case step:\".format(len(my_negative_exmaple)))   \n",
    "    return my_positive_exmaple, my_negative_exmaple\n",
    "\n",
    "\n",
    "\n",
    "def Make_TFRecords(feature_samples_dict, file_name):\n",
    "    # start to write \n",
    "    print(\"writing tfrecords....\")\n",
    "    count_display = 0\n",
    "    # strat to write postive train dicoms\n",
    "    with tf.io.TFRecordWriter(file_name) as writer:\n",
    "        for dicom_path, labels in feature_samples_dict.items():\n",
    "            print(\"dicom_path:\", dicom_path)\n",
    "    #         dicom_path_string = open(dicom_path, 'rb').read()\n",
    "            seg_label = labels[0]\n",
    "            print(seg_label.shape)\n",
    "            cls_label = labels[1]\n",
    "            tf_example = image_example(dicom_path, seg_label, cls_label)  # need image_example function to genearte mesage for writing tf records\n",
    "            writer.write(tf_example.SerializeToString())\n",
    "            count_display+=1\n",
    "            if count_display >20:\n",
    "                display.clear_output(wait=True)  \n",
    "    print(\"Generation of TFRecords is finished.\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Train TFRECORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# for train\n",
    "train_root_folder_path_for_cases= \"E:/dataset/Leisang/myTry/BleedingDataDCM/train/\"\n",
    "# ## Make into functions for genrerate tf records\n",
    "# train_positive_samples_file = 'train_positive_samples.tfrecords'\n",
    "# train_negative_samples_file = 'train_negative_samples.tfrecords'\n",
    "# give the location of the file\n",
    "train_excel_loc = \"E:\\\\dataset\\\\Leisang\\\\myTry\\\\BleedingDataDCM/TrainfilpOrNot.xlsx\"\n",
    "\n",
    "# # for ubuntu:\n",
    "train_positive_samples_file = 'train_positive_samples_unbuntu.tfrecords'\n",
    "train_negative_samples_file = 'train_negative_samples_unbuntu.tfrecords'\n",
    "\n",
    "# make positive and negative samples:\n",
    "\n",
    "# train_positive_samples, train_negative_samples = make_samples_ubuntu(root_folder_path_for_cases=train_root_folder_path_for_cases,\n",
    "#                                                               excel_loc = train_excel_loc,\n",
    "#                                                                     train_or_val=\"train\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation of TFRecords is finished.\n"
     ]
    }
   ],
   "source": [
    "# make train_positive\n",
    "Make_TFRecords(feature_samples_dict=train_positive_samples, file_name=train_positive_samples_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation of TFRecords is finished.\n"
     ]
    }
   ],
   "source": [
    "# make train_negative\n",
    "Make_TFRecords(feature_samples_dict=train_negative_samples, file_name=train_negative_samples_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# check generated positive samples\n",
    "import tensorflow_io as tfio\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "train_positive_dataset = tf.data.TFRecordDataset(train_positive_samples_file)\n",
    "train_negative_dataset = tf.data.TFRecordDataset(train_negative_samples_file)\n",
    "# Create a dictionary describing the features.\n",
    "image_feature_description = {\n",
    "        'dicom_path': tf.io.FixedLenFeature([], tf.string),\n",
    "        'seg_label': tf.io.FixedLenFeature([], tf.string), \n",
    "        'cls_label': tf.io.FixedLenFeature([], tf.int64),\n",
    "}\n",
    "\n",
    "\n",
    "def _parse_image_function(example_proto):\n",
    "    parsed_features = tf.io.parse_single_example(example_proto, image_feature_description)\n",
    "    # Parse the input tf.Example proto using the dictionary above.\n",
    "    \n",
    "    \n",
    "    # decode dicom\n",
    "    dicom_path = parsed_features[\"dicom_path\"]\n",
    "    image_bytes = tf.io.read_file(dicom_path)\n",
    "    input_image = tf.cast(tfio.image.decode_dicom_image(image_bytes, dtype=tf.uint16), tf.float32)\n",
    "    # decode mask\n",
    "    seg_label = tf.io.decode_raw(parsed_features['seg_label'], tf.uint8)\n",
    "    return dicom_path, input_image, seg_label,  parsed_features[\"cls_label\"]\n",
    "\n",
    "\n",
    "parsed_train_positive_dataset = train_positive_dataset.map(_parse_image_function)\n",
    "parsed_train_negative_dataset = train_negative_dataset.map(_parse_image_function)\n",
    "\n",
    "def check_dataset(dataset):\n",
    "    # window the input\n",
    "    def winwise(input,LB,HB):\n",
    "        # 20 ,380 for range (-32768, 32767)\n",
    "        # for tf input , (0, 65535)-? LB =  32788, 33148\n",
    "        input[input<LB] = LB # low boundary , if < LW , set to LW\n",
    "        input[input>HB] = HB # high boundary, if > Hw, Set to 255\n",
    "        return input\n",
    "\n",
    "    # print(len(parsed_train_positive_dataset))\n",
    "    BUFFER_SIZE =512\n",
    "    # random shuffle the train positive dagtaset\n",
    "    parsed_dataset = dataset.shuffle(BUFFER_SIZE)\n",
    "\n",
    "    for image_features in parsed_dataset.take(10):\n",
    "        dicom_path =  image_features[0]\n",
    "        input =  image_features[1]\n",
    "    #     dicom_path = image_features[0].numpy()\n",
    "        target = image_features[2]\n",
    "        cls_label = image_features[3]\n",
    "        print(\"dicom_path:\", dicom_path)\n",
    "        print(\"input_image.shape\", input.shape)\n",
    "    #     print(\"dicom_path\", dicom_path)\n",
    "    #     print(\"dicom_path:\", dicom_path)\n",
    "        print(\"seg_label\", target.shape)\n",
    "        print(\"cls_label\", cls_label)\n",
    "\n",
    "        # reshape label \n",
    "        target =  tf.reshape(target, input.shape)\n",
    "        fig, axes = plt.subplots(1,3, figsize=(20,20))\n",
    "\n",
    "        # \n",
    "        mask =  np.squeeze(target.numpy())\n",
    "        input_arr = np.squeeze(winwise(input.numpy(), 32788,33148 ))\n",
    "        masked_in = mask + input_arr\n",
    "        masked = np.ma.masked_where(mask == 0, mask)\n",
    "\n",
    "        axes[0].imshow(np.squeeze(winwise(input.numpy(), 32788,33148 )), cmap='gray')\n",
    "        axes[0].set_title('input range:[{}, {}]'.format((input.numpy().min()), np.max(input.numpy())))\n",
    "        axes[1].imshow(np.squeeze(target.numpy()), cmap='gray')\n",
    "        axes[1].set_title('target range:[{}, {}]'.format(np.min(target), np.max(target)))\n",
    "        axes[2].imshow(masked, 'jet', interpolation='none', alpha=0.7)\n",
    "        axes[2].set_title(\"maksed_input\")\n",
    "        print(dicom_path)\n",
    "\n",
    "        \n",
    "check_dataset(parsed_train_positive_dataset)\n",
    "# check_dataset(parsed_train_negative_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Val TFRECORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('E:/dataset/Leisang/myTry/BleedingDataDCM/val\\\\ZA-053_001', '00002102.DCM')\n",
      "00002102.DCM\n",
      "('E:/dataset/Leisang/myTry/BleedingDataDCM/val\\\\ZA-053_001', '00002103.DCM')\n",
      "00002103.DCM\n",
      "('E:/dataset/Leisang/myTry/BleedingDataDCM/val\\\\ZA-053_001', '00002104.DCM')\n",
      "00002104.DCM\n",
      "('E:/dataset/Leisang/myTry/BleedingDataDCM/val\\\\ZA-053_001', '00002105.DCM')\n",
      "00002105.DCM\n",
      "('E:/dataset/Leisang/myTry/BleedingDataDCM/val\\\\ZA-053_001', '00002106.DCM')\n",
      "00002106.DCM\n",
      "('E:/dataset/Leisang/myTry/BleedingDataDCM/val\\\\ZA-053_001', '00002107.DCM')\n",
      "00002107.DCM\n",
      "('E:/dataset/Leisang/myTry/BleedingDataDCM/val\\\\ZA-053_001', '00002108.DCM')\n",
      "00002108.DCM\n",
      "('E:/dataset/Leisang/myTry/BleedingDataDCM/val\\\\ZA-053_001', '00002109.DCM')\n",
      "00002109.DCM\n",
      "('E:/dataset/Leisang/myTry/BleedingDataDCM/val\\\\ZA-053_001', '00002110.DCM')\n",
      "00002110.DCM\n",
      "('E:/dataset/Leisang/myTry/BleedingDataDCM/val\\\\ZA-053_001', '00002111.DCM')\n",
      "00002111.DCM\n",
      "('E:/dataset/Leisang/myTry/BleedingDataDCM/val\\\\ZA-053_001', '00002112.DCM')\n",
      "00002112.DCM\n",
      "('E:/dataset/Leisang/myTry/BleedingDataDCM/val\\\\ZA-053_001', '00002113.DCM')\n",
      "00002113.DCM\n",
      "total 2113 checked ---------------------------------------------------------------------------->\n",
      "generate positive 238 positive samples at currently case step:\n",
      "generate positive 4220 negative samples at currently case step:\n"
     ]
    }
   ],
   "source": [
    "# for val\n",
    "val_root_folder_path_for_cases= \"E:/dataset/Leisang/myTry/BleedingDataDCM/val/\"\n",
    "\n",
    "## Make into functions for genrerate tf records\n",
    "# val_positive_samples_file = 'val_positive_samples_ubuntu.tfrecords'\n",
    "# val_negative_samples_file = 'val_negative_samples_ubuntu.tfrecords'\n",
    "\n",
    "# excel file\n",
    "val_excel_loc = \"E:\\\\dataset\\\\Leisang\\\\myTry\\\\BleedingDataDCM/ValfilpOrNot.xlsx\"\n",
    "print(\"val_excel_loc:\", val_excel_loc)\n",
    "\n",
    "# # for ubuntu:\n",
    "val_positive_samples_file = 'val_positive_samples_unbuntu.tfrecords'\n",
    "val_negative_samples_file = 'val_negative_samples_unbuntu.tfrecords'\n",
    "\n",
    "# make positive and negative samples:\n",
    "# val_positive_samples, val_negative_samples = make_samples(root_folder_path_for_cases=val_root_folder_path_for_cases,\n",
    "#                                                               excel_loc = val_excel_loc)\n",
    "val_positive_samples, val_negative_samples = make_samples_ubuntu(root_folder_path_for_cases=val_root_folder_path_for_cases,\n",
    "                                                              excel_loc = val_excel_loc,\n",
    "                                                               train_or_val=\"val\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation of TFRecords is finished.\n"
     ]
    }
   ],
   "source": [
    "# make val_positive\n",
    "Make_TFRecords(feature_samples_dict=val_positive_samples, file_name=val_positive_samples_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation of TFRecords is finished.\n"
     ]
    }
   ],
   "source": [
    "# make val_positive\n",
    "Make_TFRecords(feature_samples_dict=val_negative_samples, file_name=val_negative_samples_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# check validation dataset\n",
    "val_positive_dataset = tf.data.TFRecordDataset(val_positive_samples_file)\n",
    "val_negative_dataset = tf.data.TFRecordDataset(val_negative_samples_file)\n",
    "parsed_val_positive_dataset = val_positive_dataset.map(_parse_image_function)\n",
    "parsed_val_negative_dataset = val_negative_dataset.map(_parse_image_function)\n",
    "\n",
    "check_dataset(parsed_val_positive_dataset)\n",
    "check_dataset(parsed_val_negative_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "!pip install pydicom\n",
    "import pydicom\n",
    "from IPython import display\n",
    "!pip install xlrd\n",
    "import xlrd\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def _bytes_feature(value):\n",
    "  \"\"\"Returns a bytes_list from a string / byte.\"\"\"\n",
    "  if isinstance(value, type(tf.constant(0))):\n",
    "    value = value.numpy() # BytesList won't unpack a string from an EagerTensor.\n",
    "  return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "def _float_feature(value):\n",
    "  \"\"\"Returns a float_list from a float / double.\"\"\"\n",
    "  return tf.train.Feature(float_list=tf.train.FloatList(value=[value]))\n",
    "\n",
    "def _int64_feature(value):\n",
    "  \"\"\"Returns an int64_list from a bool / enum / int / uint.\"\"\"\n",
    "  return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n",
    "\n",
    "\n",
    "# Create a dictionary with features that may be relevant. # create tf.example message\n",
    "def image_example(each_dicom_path, seg_label, cls_label):\n",
    "    feature = {\n",
    "      \n",
    "        'dicom_path': _bytes_feature(each_dicom_path.encode()),\n",
    "        'seg_label': _bytes_feature(seg_label.tostring()), \n",
    "        'cls_label': _int64_feature(cls_label),\n",
    "        \n",
    "    }\n",
    "\n",
    "    return tf.train.Example(features=tf.train.Features(feature=feature))\n",
    "\n",
    "def make_samples(root_folder_path_for_cases, excel_loc, marked_datatype= np.ubyte):\n",
    "    \n",
    "    \n",
    "    # get case_list\n",
    "    case_list = glob(root_folder_path_for_cases + \"*\")\n",
    "    # print(\"case_list:\", case_list)\n",
    "    print(\"len of case_list:\", len(case_list))\n",
    "    \n",
    "    \n",
    "    # get work_book of excel\n",
    "    train_wb = xlrd.open_workbook(excel_loc)\n",
    "    # get train filp colomn\n",
    "    sheet =  train_wb.sheet_by_index(0)\n",
    "    print(\"sheet:\", sheet)\n",
    "\n",
    "    # extract_number_of rows\n",
    "    print(sheet.nrows)\n",
    "    \n",
    "    case_names =  []\n",
    "    case_flips = []\n",
    "    for i in range(1, sheet.nrows):\n",
    "        case_names.append(sheet.cell_value(i, 0))\n",
    "        case_flips.append(sheet.cell_value(i, 1))\n",
    "\n",
    "    # convert flip notes into integer \n",
    "    print(\"case_flips:\", case_flips)\n",
    "    case_flips =[int(s) for s in case_flips] \n",
    "    print(\"case_names:\", case_names)\n",
    "    print(\"case_flips:\", case_flips)\n",
    "    print(\"len of case data:\", len(case_names))\n",
    "    print(\"len of flip data:\", len(case_flips))\n",
    "\n",
    "    # check case list\n",
    "    print(\"case list len:\", len(case_list))\n",
    "    assert len(case_names) ==  len(case_flips), \" len of cases names does not macthed with flip len in train excel \"\n",
    "    assert len(case_list) ==  len(case_names), \" len of cases in the train folder not macthed with cases in train excel \"\n",
    "\n",
    "    my_positive_exmaple ={}\n",
    "    my_negative_exmaple={}\n",
    "    \n",
    "    for case_index, each_case in enumerate(case_list):\n",
    "        print(each_case)\n",
    "        # get case name from case list:\n",
    "        each_case_str = os.path.split(each_case) \n",
    "        print(\"each_case_str:\", each_case_str)\n",
    "        case_name =  each_case_str[-1]\n",
    "        print(\"case name:\", case_name)\n",
    "        print(\"case index:\", i, \"case name:\", case_names[case_index])\n",
    "        # check case name match or not\n",
    "        assert case_name ==  case_names[case_index]\n",
    "\n",
    "        # read_labels for specified case name\n",
    "        raw_files = glob(each_case + \"/*.raw\")\n",
    "        dicom_files = glob(each_case + \"/*.DCM\")\n",
    "        # get label path\n",
    "        for each_path in raw_files:\n",
    "            if \"marked\" in each_path:\n",
    "                label_path =  each_path\n",
    "        print(\"label path:\", label_path)\n",
    "        # read labels\n",
    "        labels = np.fromfile(label_path, dtype = marked_datatype)\n",
    "        reshaped_labels =  labels.reshape([-1, 512, 512])\n",
    "        print(\"reshape labels shape:\", reshaped_labels.shape)\n",
    "        print(\"len of dicoms in this case:\", len(dicom_files))\n",
    "        assert  reshaped_labels.shape[0] ==  len(dicom_files)  , 'len of dicoms is not equal to the raw label values'\n",
    "\n",
    "       # check whether needs to flip the label\n",
    "        if case_flips[case_index] ==1: # flip the enitre label\n",
    "            train_final_case_labels = np.flip(reshaped_labels, 0)\n",
    "        else: \n",
    "            train_final_case_labels =  reshaped_labels\n",
    "\n",
    "        None_zero_label_index = []\n",
    "        # read inputs\n",
    "        for dicom_index, each_dicom in enumerate(dicom_files):\n",
    "            \n",
    "            each_slice_label =  train_final_case_labels[dicom_index]\n",
    "            # count non_zeros value from the each slice label\n",
    "            num_nonzero = np.count_nonzero(each_slice_label)\n",
    "\n",
    "            if num_nonzero > 10:  # only consider the # of nonzeros values >10 to be the positive labels\n",
    "                each_slice_cls_label = 1\n",
    "                print(\"num_nonzero:\", num_nonzero)\n",
    "                None_zero_label_index.append(dicom_index+1)\n",
    "                print(\"each_slice_label shape:\", each_slice_label.shape)\n",
    "                print(\"each_slice label range before binarization: [{}, {}]\".format(each_slice_label.min(), each_slice_label.max()))\n",
    "\n",
    "                # binarize the mask label\n",
    "                each_slice_label[each_slice_label> 0] = 255\n",
    "                print(\"each_slice label range before after binarization: [{}, {}]\".format(each_slice_label.min(), each_slice_label.max()))\n",
    "\n",
    "                # genrate positive examples\n",
    "                my_positive_exmaple[each_dicom]=(each_slice_label, each_slice_cls_label)\n",
    "\n",
    "            else: # negative examples\n",
    "                each_slice_cls_label = 0\n",
    "                my_negative_exmaple[each_dicom]=(each_slice_label, each_slice_cls_label)\n",
    "\n",
    "            if dicom_index % 20 == 0:\n",
    "                    display.clear_output(wait=True)  \n",
    "\n",
    "            if dicom_index == len(dicom_files)-1:\n",
    "                print(\"total \" + str(len(dicom_files)) + \" checked ---------------------------------------------------------------------------->\")\n",
    "\n",
    "    #         print(\"my_example:\", my_exmaple)\n",
    "\n",
    "    #         # clear the output of the cell\n",
    "\n",
    "        # check the length for each case\n",
    "        print(\"generate positive {} positive samples at currently case step:\".format(len(my_positive_exmaple)))\n",
    "        print(\"generate positive {} negative samples at currently case step:\".format(len(my_negative_exmaple)))   \n",
    "    return my_positive_exmaple, my_negative_exmaple\n",
    "\n",
    "def Make_TFRecords(feature_samples_dict, file_name):\n",
    "    # start to write \n",
    "    print(\"writing tfrecords....\")\n",
    "    count_display = 0\n",
    "    # strat to write postive train dicoms\n",
    "    with tf.io.TFRecordWriter(file_name) as writer:\n",
    "        for dicom_path, labels in feature_samples_dict.items():\n",
    "            print(\"dicom_path:\", dicom_path)\n",
    "    #         dicom_path_string = open(dicom_path, 'rb').read()\n",
    "            seg_label = labels[0]\n",
    "            print(seg_label.shape)\n",
    "            cls_label = labels[1]\n",
    "            tf_example = image_example(dicom_path, seg_label, cls_label)  # need image_example function to genearte mesage for writing tf records\n",
    "            writer.write(tf_example.SerializeToString())\n",
    "            count_display+=1\n",
    "            if count_display >20:\n",
    "                display.clear_output(wait=True)  \n",
    "    print(\"Generation of TFRecords is finished.\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Train TFRECORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for train\n",
    "train_root_folder_path_for_cases= \"E:/dataset/Leisang/myTry/BleedingDataDCM/train/\"\n",
    "## Make into functions for genrerate tf records\n",
    "train_positive_samples_file = 'train_positive_samples.tfrecords'\n",
    "train_negative_samples_file = 'train_negative_samples.tfrecords'\n",
    "# give the location of the file\n",
    "train_excel_loc = \"E:\\\\dataset\\\\Leisang\\\\myTry\\\\BleedingDataDCM/TrainfilpOrNot.xlsx\"\n",
    "\n",
    "# # for ubuntu:\n",
    "# # for train\n",
    "# train_root_folder_path_for_cases= \"/media/ytx/Japan_Deep_Data/dataset/LeiSang/myTry/BleedingDataDCM/train/\"\n",
    "# ## Make into functions for genrerate tf records\n",
    "# train_positive_samples_file = '/media/ytx/Japan_Deep_Data/dataset/LeiSang/myTry/BleedingDataDCM/train_positive_samples.tfrecords'\n",
    "# train_negative_samples_file = '/media/ytx/Japan_Deep_Data/dataset/LeiSang/myTry/BleedingDataDCM/train_negative_samples.tfrecords'\n",
    "# # give the location of the file\n",
    "# train_excel_loc = \"/media/ytx/Japan_Deep_Data/dataset/LeiSang/myTry/BleedingDataDCM/TrainfilpOrNot.xlsx\"\n",
    "\n",
    "# # make positive and negative samples:\n",
    "# train_positive_samples, train_negative_samples = make_samples(root_folder_path_for_cases=train_root_folder_path_for_cases,\n",
    "#                                                               excel_loc = train_excel_loc)\n",
    "\n",
    "train_positive_samples, train_negative_samples = make_samples(root_folder_path_for_cases=train_root_folder_path_for_cases,\n",
    "                                                              excel_loc = train_excel_loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make train_positive\n",
    "Make_TFRecords(feature_samples_dict=train_positive_samples, file_name=train_positive_samples_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make train_negative\n",
    "Make_TFRecords(feature_samples_dict=train_negative_samples, file_name=train_negative_samples_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check generated positive samples\n",
    "import tensorflow_io as tfio\n",
    "import matplotlib.pyplot as plt\n",
    "train_positive_dataset = tf.data.TFRecordDataset(train_positive_samples_file)\n",
    "train_negative_dataset = tf.data.TFRecordDataset(train_negative_samples_file)\n",
    "# Create a dictionary describing the features.\n",
    "image_feature_description = {\n",
    "        'dicom_path': tf.io.FixedLenFeature([], tf.string),\n",
    "        'seg_label': tf.io.FixedLenFeature([], tf.string), \n",
    "        'cls_label': tf.io.FixedLenFeature([], tf.int64),\n",
    "}\n",
    "\n",
    "\n",
    "def _parse_image_function(example_proto):\n",
    "    parsed_features = tf.io.parse_single_example(example_proto, image_feature_description)\n",
    "    # Parse the input tf.Example proto using the dictionary above.\n",
    "    \n",
    "    \n",
    "    # decode dicom\n",
    "    dicom_path = parsed_features[\"dicom_path\"]\n",
    "    image_bytes = tf.io.read_file(dicom_path)\n",
    "    input_image = tf.cast(tfio.image.decode_dicom_image(image_bytes, dtype=tf.uint16), tf.float32)\n",
    "    # decode mask\n",
    "    seg_label = tf.io.decode_raw(parsed_features['seg_label'], tf.uint8)\n",
    "    return dicom_path, input_image, seg_label,  parsed_features[\"cls_label\"]\n",
    "\n",
    "\n",
    "parsed_train_positive_dataset = train_positive_dataset.map(_parse_image_function)\n",
    "parsed_train_negative_dataset = train_negative_dataset.map(_parse_image_function)\n",
    "\n",
    "def check_dataset(dataset):\n",
    "    # window the input\n",
    "    def winwise(input,LB,HB):\n",
    "        # 20 ,380 for range (-32768, 32767)\n",
    "        # for tf input , (0, 65535)-? LB =  32788, 33148\n",
    "        input[input<LB] = LB # low boundary , if < LW , set to LW\n",
    "        input[input>HB] = HB # high boundary, if > Hw, Set to 255\n",
    "        return input\n",
    "\n",
    "    # print(len(parsed_train_positive_dataset))\n",
    "    BUFFER_SIZE =512\n",
    "    # random shuffle the train positive dagtaset\n",
    "    parsed_dataset = dataset.shuffle(BUFFER_SIZE)\n",
    "\n",
    "    for image_features in parsed_dataset.take(10):\n",
    "        dicom_path =  image_features[0]\n",
    "        input =  image_features[1]\n",
    "    #     dicom_path = image_features[0].numpy()\n",
    "        target = image_features[2]\n",
    "        cls_label = image_features[3]\n",
    "        print(\"dicom_path:\", dicom_path)\n",
    "        print(\"input_image.shape\", input.shape)\n",
    "    #     print(\"dicom_path\", dicom_path)\n",
    "    #     print(\"dicom_path:\", dicom_path)\n",
    "        print(\"seg_label\", target.shape)\n",
    "        print(\"cls_label\", cls_label)\n",
    "\n",
    "        # reshape label \n",
    "        target =  tf.reshape(target, input.shape)\n",
    "        fig, axes = plt.subplots(1,3, figsize=(20,20))\n",
    "\n",
    "        # \n",
    "        mask =  np.squeeze(target.numpy())\n",
    "        input_arr = np.squeeze(winwise(input.numpy(), 32788,33148 ))\n",
    "        masked_in = mask + input_arr\n",
    "        masked = np.ma.masked_where(mask == 0, mask)\n",
    "\n",
    "        axes[0].imshow(np.squeeze(winwise(input.numpy(), 32788,33148 )), cmap='gray')\n",
    "        axes[0].set_title('input range:[{}, {}]'.format((input.numpy().min()), np.max(input.numpy())))\n",
    "        axes[1].imshow(np.squeeze(target.numpy()), cmap='gray')\n",
    "        axes[1].set_title('target range:[{}, {}]'.format(np.min(target), np.max(target)))\n",
    "        axes[2].imshow(masked, 'jet', interpolation='none', alpha=0.7)\n",
    "        axes[2].set_title(\"maksed_input\")\n",
    "        print(dicom_path)\n",
    "\n",
    "        \n",
    "check_dataset(parsed_train_positive_dataset)\n",
    "check_dataset(parsed_train_negative_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Val TFRECORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for val\n",
    "val_root_folder_path_for_cases= \"E:/dataset/Leisang/myTry/BleedingDataDCM/val/\"\n",
    "\n",
    "## Make into functions for genrerate tf records\n",
    "val_positive_samples_file = 'val_positive_samples.tfrecords'\n",
    "val_negative_samples_file = 'val_negative_samples.tfrecords'\n",
    "\n",
    "# excel file\n",
    "val_excel_loc = \"E:\\\\dataset\\\\Leisang\\\\myTry\\\\BleedingDataDCM/ValfilpOrNot.xlsx\"\n",
    "print(\"val_excel_loc:\", val_excel_loc)\n",
    "\n",
    "\n",
    "# make positive and negative samples:\n",
    "val_positive_samples, val_negative_samples = make_samples(root_folder_path_for_cases=val_root_folder_path_for_cases,\n",
    "                                                              excel_loc = val_excel_loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make val_positive\n",
    "Make_TFRecords(feature_samples_dict=val_positive_samples, file_name=val_positive_samples_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make val_positive\n",
    "Make_TFRecords(feature_samples_dict=val_negative_samples, file_name=val_negative_samples_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check validation dataset\n",
    "val_positive_dataset = tf.data.TFRecordDataset('val_positive_samples.tfrecords')\n",
    "val_negative_dataset = tf.data.TFRecordDataset('val_negative_samples.tfrecords')\n",
    "parsed_val_positive_dataset = val_positive_dataset.map(_parse_image_function)\n",
    "parsed_val_negative_dataset = val_negative_dataset.map(_parse_image_function)\n",
    "\n",
    "check_dataset(parsed_val_positive_dataset)\n",
    "check_dataset(parsed_val_negative_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
