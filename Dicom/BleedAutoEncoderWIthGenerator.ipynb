{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This script is used for training auto encoder with custom generator in order to load large dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pydicom in /home/ytx/anaconda3/envs/TF21Wio/lib/python3.7/site-packages (1.4.1)\n",
      "\n",
      "--->current working direcotry:\n",
      " /home/ytx/myProjects/TensorflowV2/Dicom\n",
      "--->tensorflow version: 2.1.0\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import math\n",
    "!pip install pydicom\n",
    "import pydicom\n",
    "import os\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D, Reshape, Dropout, UpSampling2D\n",
    "from tensorflow.keras.callbacks import LambdaCallback\n",
    "# print current working directory and tensorflow version\n",
    "print()\n",
    "print(\"--->current working direcotry:\\n\", os.getcwd())\n",
    "print(\"--->tensorflow version:\", tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    " #hyperparameters\n",
    "batch_size = 2\n",
    "batch_size =  2\n",
    "image_size = 512\n",
    "EPOCHS = 30\n",
    "img_width = img_height = image_size "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 512, 512, 1)]     0         \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 512, 512, 64)      640       \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 512, 512, 64)      36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 256, 256, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 256, 256, 128)     73856     \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 256, 256, 128)     147584    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 128, 128, 128)     0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 128, 128, 256)     295168    \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 128, 128, 256)     590080    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 64, 64, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 64, 64, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 64, 64, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 64, 64, 512)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 32, 32, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 32, 32, 1024)      4719616   \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 32, 32, 1024)      9438208   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 32, 32, 1024)      0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d (UpSampling2D) (None, 64, 64, 1024)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 64, 64, 512)       2097664   \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 64, 64, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 64, 64, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "up_sampling2d_1 (UpSampling2 (None, 128, 128, 512)     0         \n",
      "_________________________________________________________________\n",
      "conv2d_13 (Conv2D)           (None, 128, 128, 256)     524544    \n",
      "_________________________________________________________________\n",
      "conv2d_14 (Conv2D)           (None, 128, 128, 256)     590080    \n",
      "_________________________________________________________________\n",
      "conv2d_15 (Conv2D)           (None, 128, 128, 256)     590080    \n",
      "_________________________________________________________________\n",
      "up_sampling2d_2 (UpSampling2 (None, 256, 256, 256)     0         \n",
      "_________________________________________________________________\n",
      "conv2d_16 (Conv2D)           (None, 256, 256, 128)     131200    \n",
      "_________________________________________________________________\n",
      "conv2d_17 (Conv2D)           (None, 256, 256, 128)     147584    \n",
      "_________________________________________________________________\n",
      "conv2d_18 (Conv2D)           (None, 256, 256, 128)     147584    \n",
      "_________________________________________________________________\n",
      "up_sampling2d_3 (UpSampling2 (None, 512, 512, 128)     0         \n",
      "_________________________________________________________________\n",
      "conv2d_19 (Conv2D)           (None, 512, 512, 64)      32832     \n",
      "_________________________________________________________________\n",
      "conv2d_20 (Conv2D)           (None, 512, 512, 64)      36928     \n",
      "_________________________________________________________________\n",
      "conv2d_21 (Conv2D)           (None, 512, 512, 64)      36928     \n",
      "_________________________________________________________________\n",
      "conv2d_22 (Conv2D)           (None, 512, 512, 2)       1154      \n",
      "_________________________________________________________________\n",
      "conv2d_23 (Conv2D)           (None, 512, 512, 1)       3         \n",
      "=================================================================\n",
      "Total params: 27,898,245\n",
      "Trainable params: 27,898,245\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Failed to import pydot. You must install pydot and graphviz for `pydotprint` to work.\n"
     ]
    }
   ],
   "source": [
    "# create model\n",
    "def U_NetV2():\n",
    "    inputs = keras.layers.Input((image_size, image_size, 1))\n",
    "    conv1 = Conv2D(64, 3, activation='relu', padding='same', kernel_initializer='he_normal')(inputs)\n",
    "    conv1 = Conv2D(64, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv1)\n",
    "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "    conv2 = Conv2D(128, 3, activation='relu', padding='same', kernel_initializer='he_normal')(pool1)\n",
    "    conv2 = Conv2D(128, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv2)\n",
    "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "    conv3 = Conv2D(256, 3, activation='relu', padding='same', kernel_initializer='he_normal')(pool2)\n",
    "    conv3 = Conv2D(256, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv3)\n",
    "    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
    "    conv4 = Conv2D(512, 3, activation='relu', padding='same', kernel_initializer='he_normal')(pool3)\n",
    "    conv4 = Conv2D(512, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv4)\n",
    "    drop4 = Dropout(0.5)(conv4)\n",
    "    pool4 = MaxPooling2D(pool_size=(2, 2))(drop4)\n",
    "\n",
    "    conv5 = Conv2D(1024, 3, activation='relu', padding='same', kernel_initializer='he_normal')(pool4)\n",
    "    conv5 = Conv2D(1024, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv5)\n",
    "    drop5 = Dropout(0.5)(conv5)\n",
    "\n",
    "    up6 = Conv2D(512, 2, activation='relu', padding='same', kernel_initializer='he_normal')(\n",
    "        UpSampling2D(size=(2, 2))(drop5))\n",
    "#     merge6 = concatenate([drop4, up6], axis=3)\n",
    "    conv6 = Conv2D(512, 3, activation='relu', padding='same', kernel_initializer='he_normal')(up6)\n",
    "    conv6 = Conv2D(512, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv6)\n",
    "\n",
    "    up7 = Conv2D(256, 2, activation='relu', padding='same', kernel_initializer='he_normal')(\n",
    "        UpSampling2D(size=(2, 2))(conv6))\n",
    "#     merge7 = concatenate([conv3, up7], axis=3)\n",
    "    conv7 = Conv2D(256, 3, activation='relu', padding='same', kernel_initializer='he_normal')(up7)\n",
    "    conv7 = Conv2D(256, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv7)\n",
    "\n",
    "    up8 = Conv2D(128, 2, activation='relu', padding='same', kernel_initializer='he_normal')(\n",
    "        UpSampling2D(size=(2, 2))(conv7))\n",
    "#     merge8 = concatenate([conv2, up8], axis=3)\n",
    "    conv8 = Conv2D(128, 3, activation='relu', padding='same', kernel_initializer='he_normal')(up8)\n",
    "    conv8 = Conv2D(128, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv8)\n",
    "\n",
    "    up9 = Conv2D(64, 2, activation='relu', padding='same', kernel_initializer='he_normal')(\n",
    "        UpSampling2D(size=(2, 2))(conv8))\n",
    "#     merge9 = concatenate([conv1, up9], axis=3)\n",
    "    conv9 = Conv2D(64, 3, activation='relu', padding='same', kernel_initializer='he_normal')(up9)\n",
    "    conv9 = Conv2D(64, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv9)\n",
    "    conv9 = Conv2D(2, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv9)\n",
    "    outputs = Conv2D(1, 1, activation='linear')(conv9)\n",
    "    model = keras.models.Model(inputs, outputs)\n",
    "    return model\n",
    "model =U_NetV2()\n",
    "model.compile(loss='mse', optimizer='adam',\n",
    "              metrics=['mse'])\n",
    "model.summary()\n",
    "\n",
    "keras.utils.plot_model(model, show_shapes=True, dpi=200, expand_nested=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import cusstom generator class\n",
    "from MyGenerators import DicomGenegeratorAutoTFio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root: /media/ytx/Japan_Deep_Data/dataset/LeiSang/myTry/BleedingDataDCM/train/ZA-006_000\n",
      "one_root_paths: 1251\n",
      "\n",
      "root: /media/ytx/Japan_Deep_Data/dataset/LeiSang/myTry/BleedingDataDCM/train/ZA-008_000\n",
      "one_root_paths: 1182\n",
      "\n",
      "root: /media/ytx/Japan_Deep_Data/dataset/LeiSang/myTry/BleedingDataDCM/train/ZA-012_000\n",
      "one_root_paths: 665\n",
      "\n",
      "root: /media/ytx/Japan_Deep_Data/dataset/LeiSang/myTry/BleedingDataDCM/train/ZA-015_001\n",
      "one_root_paths: 1132\n",
      "\n",
      "root: /media/ytx/Japan_Deep_Data/dataset/LeiSang/myTry/BleedingDataDCM/train/ZA-017_001\n",
      "one_root_paths: 1226\n",
      "\n",
      "root: /media/ytx/Japan_Deep_Data/dataset/LeiSang/myTry/BleedingDataDCM/train/ZA-019_001\n",
      "one_root_paths: 1176\n",
      "\n",
      "root: /media/ytx/Japan_Deep_Data/dataset/LeiSang/myTry/BleedingDataDCM/train/ZA-020_002\n",
      "one_root_paths: 1251\n",
      "\n",
      "root: /media/ytx/Japan_Deep_Data/dataset/LeiSang/myTry/BleedingDataDCM/train/ZA-021_001\n",
      "one_root_paths: 1219\n",
      "\n",
      "root: /media/ytx/Japan_Deep_Data/dataset/LeiSang/myTry/BleedingDataDCM/train/ZA-022_001\n",
      "one_root_paths: 1251\n",
      "\n",
      "root: /media/ytx/Japan_Deep_Data/dataset/LeiSang/myTry/BleedingDataDCM/train/ZA-023_001\n",
      "one_root_paths: 869\n",
      "\n",
      "root: /media/ytx/Japan_Deep_Data/dataset/LeiSang/myTry/BleedingDataDCM/train/ZA-024_001\n",
      "one_root_paths: 1113\n",
      "\n",
      "root: /media/ytx/Japan_Deep_Data/dataset/LeiSang/myTry/BleedingDataDCM/train/ZA-027_001\n",
      "one_root_paths: 1232\n",
      "\n",
      "root: /media/ytx/Japan_Deep_Data/dataset/LeiSang/myTry/BleedingDataDCM/train/ZA-029_001\n",
      "one_root_paths: 1251\n",
      "\n",
      "root: /media/ytx/Japan_Deep_Data/dataset/LeiSang/myTry/BleedingDataDCM/train/ZA-031_001\n",
      "one_root_paths: 1169\n",
      "\n",
      "root: /media/ytx/Japan_Deep_Data/dataset/LeiSang/myTry/BleedingDataDCM/train/ZA-032_001\n",
      "one_root_paths: 1013\n",
      "\n",
      "root: /media/ytx/Japan_Deep_Data/dataset/LeiSang/myTry/BleedingDataDCM/train/ZA-033_001\n",
      "one_root_paths: 1076\n",
      "\n",
      "root: /media/ytx/Japan_Deep_Data/dataset/LeiSang/myTry/BleedingDataDCM/train/ZA-035_001\n",
      "one_root_paths: 1182\n",
      "\n",
      "root: /media/ytx/Japan_Deep_Data/dataset/LeiSang/myTry/BleedingDataDCM/train/ZA-036_001\n",
      "one_root_paths: 1751\n",
      "\n",
      "root: /media/ytx/Japan_Deep_Data/dataset/LeiSang/myTry/BleedingDataDCM/train/ZA-037_001\n",
      "one_root_paths: 1213\n",
      "\n",
      "root: /media/ytx/Japan_Deep_Data/dataset/LeiSang/myTry/BleedingDataDCM/train/ZA-038_001\n",
      "one_root_paths: 1013\n",
      "\n",
      "root: /media/ytx/Japan_Deep_Data/dataset/LeiSang/myTry/BleedingDataDCM/train/ZA-039_001\n",
      "one_root_paths: 1344\n",
      "\n",
      "root: /media/ytx/Japan_Deep_Data/dataset/LeiSang/myTry/BleedingDataDCM/train/ZA-040_001\n",
      "one_root_paths: 1369\n",
      "\n",
      "root: /media/ytx/Japan_Deep_Data/dataset/LeiSang/myTry/BleedingDataDCM/train/ZA-041_001\n",
      "one_root_paths: 1326\n",
      "\n",
      "root: /media/ytx/Japan_Deep_Data/dataset/LeiSang/myTry/BleedingDataDCM/train/ZA-042_001\n",
      "one_root_paths: 1182\n",
      "\n",
      "root: /media/ytx/Japan_Deep_Data/dataset/LeiSang/myTry/BleedingDataDCM/train/ZA-043_001\n",
      "one_root_paths: 1663\n",
      "\n",
      "root: /media/ytx/Japan_Deep_Data/dataset/LeiSang/myTry/BleedingDataDCM/train/ZA-044_001\n",
      "one_root_paths: 1669\n",
      "\n",
      "root: /media/ytx/Japan_Deep_Data/dataset/LeiSang/myTry/BleedingDataDCM/train/ZA-045_001\n",
      "one_root_paths: 1251\n",
      "\n",
      "root: /media/ytx/Japan_Deep_Data/dataset/LeiSang/myTry/BleedingDataDCM/train/ZA-046_001\n",
      "one_root_paths: 1251\n",
      "\n",
      "root: /media/ytx/Japan_Deep_Data/dataset/LeiSang/myTry/BleedingDataDCM/train/ZA-047_001\n",
      "one_root_paths: 1251\n",
      "\n",
      "root: /media/ytx/Japan_Deep_Data/dataset/LeiSang/myTry/BleedingDataDCM/train/ZA-048_001\n",
      "one_root_paths: 1082\n",
      "\n",
      "root: /media/ytx/Japan_Deep_Data/dataset/LeiSang/myTry/BleedingDataDCM/val/ZA-049_001\n",
      "one_root_paths: 1126\n",
      "\n",
      "root: /media/ytx/Japan_Deep_Data/dataset/LeiSang/myTry/BleedingDataDCM/val/ZA-051_001\n",
      "one_root_paths: 1219\n",
      "\n",
      "root: /media/ytx/Japan_Deep_Data/dataset/LeiSang/myTry/BleedingDataDCM/val/ZA-053_001\n",
      "one_root_paths: 2113\n",
      "\n",
      "root: /media/ytx/Japan_Deep_Data/dataset/LeiSang/myTry/BleedingDataDCM/val/ZA-049_001\n",
      "one_root_paths: 1126\n",
      "\n",
      "root: /media/ytx/Japan_Deep_Data/dataset/LeiSang/myTry/BleedingDataDCM/val/ZA-051_001\n",
      "one_root_paths: 1219\n",
      "\n",
      "root: /media/ytx/Japan_Deep_Data/dataset/LeiSang/myTry/BleedingDataDCM/val/ZA-053_001\n",
      "one_root_paths: 2113\n",
      "\n",
      "Found 36623 train images \n",
      "Found 4458 val images \n",
      "Found 18312 training batches\n",
      "Found 2229 validation batches\n",
      "Found 2229 validation batches for image prediction log\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "     # root dirs for dcms\n",
    "    train_images_root = sorted(glob('/media/ytx/Japan_Deep_Data/dataset/LeiSang/myTry/BleedingDataDCM/train/*'))\n",
    "    val_images_root = sorted(glob('/media/ytx/Japan_Deep_Data/dataset/LeiSang/myTry/BleedingDataDCM/val/*'))\n",
    "    \n",
    "   \n",
    "    \n",
    "    # create generator instances\n",
    "    train_dcm_gen =  DicomGenegeratorAutoTFio(batch_size=batch_size, \n",
    "                                          train_or_test=\"train\", \n",
    "                                          dims =(image_size, image_size),\n",
    "                                          shuffle=False,\n",
    "                                          train_images_root=train_images_root)\n",
    "    val_dcm_gen =  DicomGenegeratorAutoTFio(batch_size=batch_size, \n",
    "                                          train_or_test=\"val\", \n",
    "                                          dims =(image_size, image_size),\n",
    "                                          shuffle=False,\n",
    "                                          val_images_root=val_images_root)\n",
    "    \n",
    "    val_dcm_gen2_img =  DicomGenegeratorAutoTFio(batch_size=batch_size, \n",
    "                                          train_or_test=\"val\", \n",
    "                                          dims =(image_size, image_size),\n",
    "                                          shuffle=False,\n",
    "                                          val_images_root=val_images_root)\n",
    "    \n",
    "    print(f'Found {len(train_dcm_gen.dcm_paths)} {train_dcm_gen.train_or_test} images ')\n",
    "    print(f'Found {len(val_dcm_gen.dcm_paths)} {val_dcm_gen.train_or_test} images ')\n",
    "    print(f'Found {train_dcm_gen.__len__()} training batches')\n",
    "    print(f'Found {val_dcm_gen.__len__()} validation batches')\n",
    "    \n",
    "  \n",
    "    print(f'Found {val_dcm_gen2_img.__len__()} validation batches for image prediction log')\n",
    "    \n",
    "    # define custome callbacks\n",
    "    # some callbacks\n",
    "# add the image call back\n",
    "    import os\n",
    "    from datetime import datetime\n",
    "    logdir =  os.path.join(\"logs\",\"image\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "    # Define the basic TensorBoard callback.\n",
    "    tensorboard_callback = keras.callbacks.TensorBoard(log_dir=logdir)\n",
    "    file_writer_img = tf.summary.create_file_writer(logdir + '/img')\n",
    "\n",
    "#     @tf.function()\n",
    "    def draw_input_output(epoch, logs):\n",
    "        for idx, data in enumerate(val_dcm_gen2_img):\n",
    "            print(idx)\n",
    "            print(\"val_sample shape:\", data[0].shape) # remember data generator now return (X, X)\n",
    "            print(\"val_target shape:\", data[1].shape)\n",
    "#         print(\"min: {} max:{}\".format(np.min(data), np.max(data)))\n",
    "            output = model.predict(data[0]) # remember the data contains the both input and target (X, X)\n",
    "            print(\"prediction shape:\", output.shape)\n",
    "            if idx >9:  # only take 10 samples for image logs\n",
    "                    break\n",
    "        with file_writer_img.as_default():\n",
    "            tf.summary.image(\"test_output\", tf.reshape(output,[-1, image_size , image_size , 1]), step=epoch)\n",
    "    #         tf.summary.image(\"test_noisy_input\", tf.reshape(x_test_noisy,[-1, image_size , image_size , 1]), step=epoch)\n",
    "            tf.summary.image(\"test_input\", tf.reshape(data[0],[-1, image_size , image_size , 1]), step=epoch)\n",
    "    log_mg = keras.callbacks.LambdaCallback(on_epoch_end=draw_input_output)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # fit model with built generator\n",
    "#     H = model.fit(train_dcm_gen,\n",
    "#                             validation_data=val_dcm_gen, \n",
    "#                             steps_per_epoch=6, \n",
    "#                             validation_steps=4,\n",
    "#                             epochs=EPOCHS, \n",
    "#                             callbacks=[tensorboard_callback, log_mg],\n",
    "#                             verbose=1)\n",
    "#     H = model.fit_generator(train_dcm_gen,\n",
    "#                             validation_data=val_dcm_gen, \n",
    "#                             steps_per_epoch=train_dcm_gen.__len__(), \n",
    "#                             validation_steps=val_dcm_gen.__len__(),\n",
    "#                             epochs=EPOCHS, \n",
    "#                             callbacks=[tensorboard_callback, log_mg],\n",
    "#                             verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
