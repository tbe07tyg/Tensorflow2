{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--AA\n",
      "AA--\n",
      "Requirement already satisfied: pydicom in e:\\projects\\intepreters\\anaconda\\envs\\tf21\\lib\\site-packages (1.4.2)\n",
      "Requirement already satisfied: xlrd in e:\\projects\\intepreters\\anaconda\\envs\\tf21\\lib\\site-packages (1.2.0)\n",
      "None\n",
      "--AA\n",
      "AA--\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "!pip install pydicom\n",
    "import pydicom\n",
    "from IPython import display\n",
    "!pip install xlrd\n",
    "import xlrd\n",
    "# check generated positive samples\n",
    "import tensorflow_io as tfio\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from copy import copy\n",
    "import matplotlib.colors as colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def _bytes_feature(value):\n",
    "  \"\"\"Returns a bytes_list from a string / byte.\"\"\"\n",
    "  if isinstance(value, type(tf.constant(0))):\n",
    "    value = value.numpy() # BytesList won't unpack a string from an EagerTensor.\n",
    "  return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "def _float_feature(value):\n",
    "  \"\"\"Returns a float_list from a float / double.\"\"\"\n",
    "  return tf.train.Feature(float_list=tf.train.FloatList(value=[value]))\n",
    "\n",
    "def _int64_feature(value):\n",
    "  \"\"\"Returns an int64_list from a bool / enum / int / uint.\"\"\"\n",
    "  return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n",
    "\n",
    "\n",
    "# Create a dictionary with features that may be relevant. # create tf.example message\n",
    "def image_example(each_dicom_path, seg_label, cls_label):\n",
    "    feature = {\n",
    "      \n",
    "        'dicom_path': _bytes_feature(each_dicom_path.encode()),\n",
    "        'seg_label': _bytes_feature(seg_label.tostring()), \n",
    "        'cls_label': _int64_feature(cls_label),\n",
    "        \n",
    "    }\n",
    "\n",
    "    return tf.train.Example(features=tf.train.Features(feature=feature))\n",
    "\n",
    "\n",
    "def image_pair_example(each_dicom_path1, each_dicom_path2, each_dicom_path3, seg_label1, seg_label2, seg_label3, cls_label1, cls_label2,  cls_label3):\n",
    "    feature = {\n",
    "      \n",
    "        'dicom_path1': _bytes_feature(each_dicom_path1.encode()),\n",
    "        'seg_label1': _bytes_feature(seg_label1.tostring()), \n",
    "        'cls_label1': _int64_feature(cls_label1),\n",
    "        \n",
    "        'dicom_path2': _bytes_feature(each_dicom_path2.encode()),\n",
    "        'seg_label2': _bytes_feature(seg_label2.tostring()), \n",
    "        'cls_label2': _int64_feature(cls_label2),\n",
    "        \n",
    "        'dicom_path3': _bytes_feature(each_dicom_path3.encode()),\n",
    "        'seg_label3': _bytes_feature(seg_label3.tostring()), \n",
    "        'cls_label3': _int64_feature(cls_label3),\n",
    "        \n",
    "    }\n",
    "\n",
    "    return tf.train.Example(features=tf.train.Features(feature=feature))\n",
    "\n",
    "def make_sample_pairs(root_folder_path_for_cases, excel_loc, train_or_val, marked_datatype= np.ubyte):\n",
    "    # get case_list\n",
    "    case_list = glob(root_folder_path_for_cases + \"*\")\n",
    "    # print(\"case_list:\", case_list)\n",
    "    print(\"len of case_list:\", len(case_list))\n",
    "    \n",
    "    \n",
    "    # get work_book of excel\n",
    "    train_wb = xlrd.open_workbook(excel_loc)\n",
    "    # get train filp colomn\n",
    "    sheet =  train_wb.sheet_by_index(0)\n",
    "    print(\"sheet:\", sheet)\n",
    "\n",
    "    # extract_number_of rows\n",
    "    print(sheet.nrows)\n",
    "    \n",
    "    case_names =  []\n",
    "    case_flips = []\n",
    "    for i in range(1, sheet.nrows):\n",
    "        case_names.append(sheet.cell_value(i, 0))\n",
    "        case_flips.append(sheet.cell_value(i, 1))\n",
    "\n",
    "    \n",
    "    # convert flip notes into integer \n",
    "    print(\"case_flips:\", case_flips)\n",
    "    case_flips =[int(s) for s in case_flips] \n",
    "    print(\"case_names:\", case_names)\n",
    "    print(\"case_flips:\", case_flips)\n",
    "    print(\"len of case data:\", len(case_names))\n",
    "    print(\"len of flip data:\", len(case_flips))\n",
    "\n",
    "    # check case list\n",
    "    print(\"case list len:\", len(case_list))\n",
    "    assert len(case_names) ==  len(case_flips), \" len of cases names does not macthed with flip len in train excel \"\n",
    "    assert len(case_list) ==  len(case_names), \" len of cases in the train folder not macthed with cases in train excel \"\n",
    "    \n",
    "    # one feature contains : input 1 , seg1, cls1, input 2, seg2, cls2, input3, seg3, cls3 with shape [1, 9]\n",
    "    \n",
    "    positive_sample_features =  []\n",
    "    negative_sample_features =  []\n",
    "    \n",
    "    \n",
    "    # loop each case folder to select samples\n",
    "    for case_index, each_case in enumerate(case_list):\n",
    "        display.clear_output(wait=True)\n",
    "        print(\"case index:{}, case:{}\".format(case_index, each_case))\n",
    "        each_case_str = os.path.split(each_case) \n",
    "        print(\"each_case_str:\", each_case_str)\n",
    "        case_name =  each_case_str[-1]\n",
    "        print(\"case name:\", case_name)\n",
    "        print(\"case index:\", case_index, \"case name:\", case_names[case_index])\n",
    "        # check case name match or not\n",
    "        assert case_name ==  case_names[case_index]\n",
    "        \n",
    "        # get dicom files and label file\n",
    "        # read_labels for specified case name\n",
    "        # read rawfile to get seg labels for the entire case: ----------------------->  for seg label \n",
    "        raw_files = glob(each_case + \"/*.raw\")\n",
    "        # get label path\n",
    "        for each_path in raw_files:\n",
    "            if \"marked\" in each_path:\n",
    "                label_path =  each_path\n",
    "        print(\"label path:\", label_path)\n",
    "        # read labels\n",
    "        labels = np.fromfile(label_path, dtype = marked_datatype)\n",
    "        reshaped_labels =  labels.reshape([-1, 512, 512])\n",
    "        print(\"reshape labels shape:\", reshaped_labels.shape)\n",
    "#\n",
    "\n",
    "       # check whether needs to flip the label\n",
    "        if case_flips[case_index] ==1: # flip the enitre label\n",
    "            final_case_labels = np.flip(reshaped_labels, 0)\n",
    "        else: \n",
    "            final_case_labels =  reshaped_labels\n",
    "        \n",
    "        \n",
    "        step=3 # number of elements for each sample\n",
    "        \n",
    "        # rearrange the label array ever 3 elemtns step 1\n",
    "        rearrange_3_seg_labels_list = []\n",
    "        \n",
    "       \n",
    "        for i in range(0, final_case_labels.shape[0]-step+1):\n",
    "            rearrange_3_seg_labels_list.append(final_case_labels[i:i+step])\n",
    "        print(\"rearrange_3labels_list len---->:\", len(rearrange_3_seg_labels_list))\n",
    "        final_seg_label_list = rearrange_3_seg_labels_list\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        # get dicom list...-------------------------------->   for dicom input path\n",
    "        dicom_files = glob(each_case + \"/*.DCM\")\n",
    "        \n",
    "        # in order for ubuntu system, needs to rewrite the path of dicom\n",
    "        dicom_path_list =[]\n",
    "        cls_list=[]\n",
    "        for dicom_index, each_dicom in enumerate(dicom_files):\n",
    "            path_split = os.path.split(each_dicom)\n",
    "#             print(path_split)\n",
    "            file_name =  path_split[-1]\n",
    "#             print(file_name)\n",
    "\n",
    "\n",
    "            each_slice_label =  final_case_labels[dicom_index]\n",
    "            # count non_zeros value from the each slice label\n",
    "            num_nonzero = np.count_nonzero(each_slice_label)\n",
    "            \n",
    "            \n",
    "#             # for lab ubuntu\n",
    "#             if train_or_val == \"train\": # change replace paths considering for ubuntu path root changed\n",
    "#                 each_dicom =  \"/media/ytx/Japan_Deep_Data/dataset/LeiSang/myTry/BleedingDataDCM/train/\" + case_name +\"/\" +file_name\n",
    "#             else:\n",
    "#                 each_dicom =  \"/media/ytx/Japan_Deep_Data/dataset/LeiSang/myTry/BleedingDataDCM/val/\" + case_name+ \"/\" +file_name\n",
    "            # for lab windows\n",
    "            if train_or_val == \"train\": # change replace paths considering for ubuntu path root changed\n",
    "                each_dicom =  \"F:\\\\dataset\\\\Leisang\\\\myTry\\\\BleedingDataDCM\\\\train/\" + case_name +\"/\" +file_name\n",
    "            else:\n",
    "                each_dicom =  \"F:\\\\dataset\\\\Leisang\\\\myTry\\\\BleedingDataDCM\\\\val/\" + case_name+ \"/\" +file_name\n",
    "            \n",
    "            \n",
    "            if num_nonzero > 10:  # only consider the # of nonzeros values >10 to be the positive labels\n",
    "                each_slice_cls_label = 1\n",
    "                print(\"num_nonzero:\", num_nonzero)\n",
    "#                 None_zero_label_index.append(dicom_index+1)\n",
    "                print(\"each_slice_label shape:\", each_slice_label.shape)\n",
    "                print(\"each_slice label range before binarization: [{}, {}]\".format(each_slice_label.min(), each_slice_label.max()))\n",
    "\n",
    "                # binarize the mask label\n",
    "                each_slice_label[each_slice_label> 0] = 255\n",
    "                print(\"each_slice label range before after binarization: [{}, {}]\".format(each_slice_label.min(), each_slice_label.max()))\n",
    "            else: # negative examples\n",
    "                each_slice_cls_label = 0\n",
    "            \n",
    "            if dicom_index % 20 == 0:\n",
    "                    display.clear_output(wait=True)\n",
    "            \n",
    "            dicom_path_list.append(each_dicom)\n",
    "            cls_list.append(each_slice_cls_label)\n",
    "        \n",
    "        # rearrange dicom path list and cls_list into 3 elements per group\n",
    "        final_dicom_path_list = [dicom_path_list[i:i+step] for i in range(0, len(dicom_path_list)-step+1)]\n",
    "        final_cls_list = [cls_list[i:i+step] for i in range(0, len(cls_list)-step+1)]\n",
    "        \n",
    "        print(\"final_dicom_path_list:\", len(final_dicom_path_list))\n",
    "#         print(final_dicom_path_list)\n",
    "        print(\"final_cls_list:\", len(final_cls_list))\n",
    "#         print(final_cls_list)\n",
    "        \n",
    "#         # reform the dicom list into 3 elements per step\n",
    "      \n",
    "#         dicom_3_files_list = [dicom_files[i:i+step] for i in range(0, len(dicom_files)-step+1)]\n",
    "#         print(\"dicom_3_files_list len---->:\", len(dicom_3_files_list))\n",
    "        print(\"dicom_files[0]:\", dicom_files[0])\n",
    "        print(\"final_dicom_path_list[0][0]:\", final_dicom_path_list[0][0])\n",
    "        print(\"dicom_files[-1]:\", dicom_files[-1])\n",
    "        print(\"final_dicom_path_list[-1][-1]:\", final_dicom_path_list[-1][-1])\n",
    "        print(\"final_dicom_path_list[0]:\", final_dicom_path_list[0])\n",
    "        print(\"final_dicom_path_list[1]:\", final_dicom_path_list[1])\n",
    "        \n",
    "        print()\n",
    "        print(\"final case label shape:\", final_case_labels.shape)\n",
    "#         assert os.path.normpath(dicom_files[0]) == os.path.normpath(final_dicom_path_list[0][0])  # Use os.path.normpath to convert c:/fold1/fold2 to c:\\fold1\\fold2\n",
    "#         assert os.path.normpath (dicom_files[-1]) == os.path.normpath(final_dicom_path_list[-1][-1])\n",
    "        print(\"len of dicoms in this case:\", len(dicom_files))\n",
    "        assert  reshaped_labels.shape[0] ==  len(dicom_files), 'len of dicoms is not equal to the raw label values'\n",
    "        print()    \n",
    "        \n",
    "        # pack the data with as [(inputs1, seg_tar1, cls_tar1) , （inputs2, seg_tar2, cls_tar2）， (inputs3, seg_tar3, cls_tar3)] \n",
    "        final_samples =  zip(final_dicom_path_list, final_seg_label_list, final_cls_list)\n",
    "#         print(\"len of case samples:\", list(final_samples))\n",
    "        final_case_samples_list =  list(final_samples)\n",
    "#         print(\"final_case_samples_list[0]:\",  final_case_samples_list[0]) \n",
    "        # Consider add_positive samples and negative samples\n",
    "        for i in range(len(final_cls_list)):\n",
    "            if i  % 20 == 0:\n",
    "                display.clear_output(wait=True)\n",
    "            print(\"i:\",i)\n",
    "            print(\"final_cls_list[i]:\", final_cls_list[i])\n",
    "#             print(\"final_clas_list[i]:\", final_cls_list[i])\n",
    "            num_non_zero_clses = np.count_nonzero(final_cls_list[i])\n",
    "            print(\"num_non_zero_clses\", num_non_zero_clses)\n",
    "            if num_non_zero_clses >1:\n",
    "#                 print(\"final_case_samples_list[i]:\", final_case_samples_list[i])\n",
    "                positive_sample_features.append(final_case_samples_list[i])\n",
    "            else: \n",
    "#                 print(\"final_case_samples_list[i]:\", final_case_samples_list[i])\n",
    "                negative_sample_features.append(final_case_samples_list[i])\n",
    "            \n",
    "           \n",
    "                \n",
    "    print(\"total positive samples:\",len(positive_sample_features))\n",
    "    print(\"total negative samples:\",len(negative_sample_features))            \n",
    "    return positive_sample_features, negative_sample_features\n",
    "        \n",
    "# def make_samples_win(root_folder_path_for_cases, excel_loc, train_or_val, marked_datatype= np.ubyte):\n",
    "    \n",
    "    \n",
    "#     # get case_list\n",
    "#     case_list = glob(root_folder_path_for_cases + \"*\")\n",
    "#     # print(\"case_list:\", case_list)\n",
    "#     print(\"len of case_list:\", len(case_list))\n",
    "    \n",
    "    \n",
    "#     # get work_book of excel\n",
    "#     train_wb = xlrd.open_workbook(excel_loc)\n",
    "#     # get train filp colomn\n",
    "#     sheet =  train_wb.sheet_by_index(0)\n",
    "#     print(\"sheet:\", sheet)\n",
    "\n",
    "#     # extract_number_of rows\n",
    "#     print(sheet.nrows)\n",
    "    \n",
    "#     case_names =  []\n",
    "#     case_flips = []\n",
    "#     for i in range(1, sheet.nrows):\n",
    "#         case_names.append(sheet.cell_value(i, 0))\n",
    "#         case_flips.append(sheet.cell_value(i, 1))\n",
    "\n",
    "#     # convert flip notes into integer \n",
    "#     print(\"case_flips:\", case_flips)\n",
    "#     case_flips =[int(s) for s in case_flips] \n",
    "#     print(\"case_names:\", case_names)\n",
    "#     print(\"case_flips:\", case_flips)\n",
    "#     print(\"len of case data:\", len(case_names))\n",
    "#     print(\"len of flip data:\", len(case_flips))\n",
    "\n",
    "#     # check case list\n",
    "#     print(\"case list len:\", len(case_list))\n",
    "#     assert len(case_names) ==  len(case_flips), \" len of cases names does not macthed with flip len in train excel \"\n",
    "#     assert len(case_list) ==  len(case_names), \" len of cases in the train folder not macthed with cases in train excel \"\n",
    "\n",
    "#     my_positive_exmaple ={}\n",
    "#     my_negative_exmaple={}\n",
    "    \n",
    "#     for case_index, each_case in enumerate(case_list):\n",
    "#         print(each_case)\n",
    "#         # get case name from case list:\n",
    "#         each_case_str = os.path.split(each_case) \n",
    "#         print(\"each_case_str:\", each_case_str)\n",
    "#         case_name =  each_case_str[-1]\n",
    "#         print(\"case name:\", case_name)\n",
    "#         print(\"case index:\", case_index, \"case name:\", case_names[case_index])\n",
    "#         # check case name match or not\n",
    "#         assert case_name ==  case_names[case_index]\n",
    "\n",
    "#         # read_labels for specified case name\n",
    "#         raw_files = glob(each_case + \"/*.raw\")\n",
    "#         dicom_files = glob(each_case + \"/*.DCM\")\n",
    "        \n",
    "        \n",
    "#         # get label path\n",
    "#         for each_path in raw_files:\n",
    "#             if \"marked\" in each_path:\n",
    "#                 label_path =  each_path\n",
    "#         print(\"label path:\", label_path)\n",
    "#         # read labels\n",
    "#         labels = np.fromfile(label_path, dtype = marked_datatype)\n",
    "#         reshaped_labels =  labels.reshape([-1, 512, 512])\n",
    "#         print(\"reshape labels shape:\", reshaped_labels.shape)\n",
    "#         print(\"len of dicoms in this case:\", len(dicom_files))\n",
    "#         assert  reshaped_labels.shape[0] ==  len(dicom_files)  , 'len of dicoms is not equal to the raw label values'\n",
    "\n",
    "#        # check whether needs to flip the label\n",
    "#         if case_flips[case_index] ==1: # flip the enitre label\n",
    "#             train_final_case_labels = np.flip(reshaped_labels, 0)\n",
    "#         else: \n",
    "#             train_final_case_labels =  reshaped_labels\n",
    "\n",
    "#         None_zero_label_index = []\n",
    "#         # read inputs\n",
    "#         for dicom_index, each_dicom in enumerate(dicom_files):\n",
    "#             path_split = os.path.split(each_dicom)\n",
    "#             print(path_split)\n",
    "#             file_name =  path_split[-1]\n",
    "#             print(file_name)\n",
    "            \n",
    "           \n",
    "#             each_slice_label =  train_final_case_labels[dicom_index]\n",
    "#             # count non_zeros value from the each slice label\n",
    "#             num_nonzero = np.count_nonzero(each_slice_label)\n",
    "            \n",
    "#             if train_or_val == \"train\": # change replace paths considering for ubuntu path root changed\n",
    "#                 each_dicom =  \"E:\\\\dataset\\\\Leisang\\\\myTry\\\\BleedingDataDCM\\\\train/\" + case_name +\"/\" +file_name\n",
    "#             else:\n",
    "#                 each_dicom =  \"E:\\\\dataset\\\\Leisang\\\\myTry\\\\BleedingDataDCM\\\\val/\" + case_name+ \"/\" +file_name\n",
    "            \n",
    "#             if num_nonzero > 10:  # only consider the # of nonzeros values >10 to be the positive labels\n",
    "#                 each_slice_cls_label = 1\n",
    "#                 print(\"num_nonzero:\", num_nonzero)\n",
    "#                 None_zero_label_index.append(dicom_index+1)\n",
    "#                 print(\"each_slice_label shape:\", each_slice_label.shape)\n",
    "#                 print(\"each_slice label range before binarization: [{}, {}]\".format(each_slice_label.min(), each_slice_label.max()))\n",
    "\n",
    "#                 # binarize the mask label\n",
    "#                 each_slice_label[each_slice_label> 0] = 255\n",
    "#                 print(\"each_slice label range before after binarization: [{}, {}]\".format(each_slice_label.min(), each_slice_label.max()))\n",
    "\n",
    "#                 # genrate positive examples dict = {\"path\"： label}\n",
    "#                 my_positive_exmaple[each_dicom]=(each_slice_label, each_slice_cls_label)\n",
    "\n",
    "#             else: # negative examples\n",
    "#                 each_slice_cls_label = 0\n",
    "#                 my_negative_exmaple[each_dicom]=(each_slice_label, each_slice_cls_label)\n",
    "\n",
    "#             if dicom_index % 20 == 0:\n",
    "#                     display.clear_output(wait=True)  \n",
    "\n",
    "#             if dicom_index == len(dicom_files)-1:\n",
    "#                 print(\"total \" + str(len(dicom_files)) + \" checked ---------------------------------------------------------------------------->\")\n",
    "\n",
    "   \n",
    "#         # check the length for each case\n",
    "#         print(\"generate positive {} positive samples at currently case step:\".format(len(my_positive_exmaple)))\n",
    "#         print(\"generate positive {} negative samples at currently case step:\".format(len(my_negative_exmaple)))   \n",
    "#     return my_positive_exmaple, my_negative_exmaple\n",
    "\n",
    "\n",
    "def Make_InputPairsTFrecords(sample_list, file_nameToSave):\n",
    "    from pathlib import Path\n",
    "    # start to write \n",
    "    print(\"writing tfrecords....\")\n",
    "   \n",
    "    # strat to write postive train dicoms\n",
    "    with tf.io.TFRecordWriter(file_nameToSave) as writer:\n",
    "        print(\"paths sample_list[0][0]:\", sample_list[0][0])\n",
    "        print(\"seg_tars sample_list[0][1]:\", sample_list[0][1].shape)\n",
    "        print(\"cls_tars sample_list[0][2]:\", sample_list[0][2])\n",
    "        for i in range(len(sample_list)):\n",
    "            if i % 10 ==0:\n",
    "                 display.clear_output(wait=True)\n",
    "                    \n",
    "            print(\"sample index:\", i)\n",
    "            one_sample =  sample_list[i]\n",
    "#             print(\"one temp sample:\", one_sample)\n",
    "            # extract feture elements:\n",
    "            # for paths-->:\n",
    "            each_dicom_paths = sample_list[i][0]\n",
    "            \n",
    "            each_dicom_path0 = each_dicom_paths[0]\n",
    "            each_dicom_path1 = each_dicom_paths[1]\n",
    "            each_dicom_path2 = each_dicom_paths[2]\n",
    "            # for seg_tars-->:\n",
    "            each_seg_tars = sample_list[i][1]\n",
    "            \n",
    "            each_seg_tar0 = each_seg_tars[0]\n",
    "            each_seg_tar1 = each_seg_tars[1]\n",
    "            each_seg_tar2 = each_seg_tars[2]\n",
    "            \n",
    "            # for cls_tars-->:\n",
    "            each_cls_tars = sample_list[i][2]\n",
    "            each_cls_tar0 = each_cls_tars[0]\n",
    "            each_cls_tar1 = each_cls_tars[1]\n",
    "            each_cls_tar2 = each_cls_tars[2]\n",
    "            \n",
    "            \n",
    "            # write sample elements into pre-defined proto\n",
    "            tf_example = image_pair_example(each_dicom_path0, each_dicom_path1, each_dicom_path2, \n",
    "                                            each_seg_tar0, each_seg_tar1, each_seg_tar2, \n",
    "                                            each_cls_tar0, each_cls_tar1, each_cls_tar2)  # need image_example function to genearte mesage for writing tf records\n",
    "            writer.write(tf_example.SerializeToString())\n",
    "#             count_display+=1\n",
    "#             if count_display >20:\n",
    "#                 display.clear_output(wait=True)  \n",
    "    print(\"Generation of TFRecords is finished.\")\n",
    "        \n",
    "    \n",
    "#     for dicom_path, labels in feature_samples_dict.items():\n",
    "#         if previous_sample is None:\n",
    "#             previous_sample_sample\n",
    "#         p =  Path(dicom_path)\n",
    "# #         path_split = os.path.split(dicom_path)\n",
    "# #         sub_path_split = os.path.split(path_split[0])\n",
    "# #         case_name =  sub_path_split[-1]\n",
    "# #         file_name = path_split[-1] \n",
    "# #         file_index =  p.stem()\n",
    "# #         print(\"dicom_path:\", dicom_path)\n",
    "# #         print(\"dicom_path_split:\", path_split)\n",
    "# #         print(\"sub_path_split:\", sub_path_split)\n",
    "# #         print(\"case name:\", case_name)\n",
    "# #         print(\"filename:\", file_name)\n",
    "#         print(p.with_suffix(''))\n",
    "#         print(p.stem)\n",
    "#         file_index = int(p.stem)\n",
    "        \n",
    "#         print(\"file index:\", file_index)\n",
    "#         print(\"file index:\", file_index)\n",
    "        \n",
    "#     with tf.io.TFRecordWriter(file_nameToSave) as writer:\n",
    "#         for dicom_path, labels in feature_samples_dict.items():\n",
    "#             print(\"dicom_path:\", dicom_path)\n",
    "#     #         dicom_path_string = open(dicom_path, 'rb').read()\n",
    "#             seg_label = labels[0]\n",
    "#             print(seg_label.shape)\n",
    "#             cls_label = labels[1]\n",
    "# #             tf_example = image_example(dicom_path, seg_label, cls_label)  # need image_example function to genearte mesage for writing tf records\n",
    "# #             writer.write(tf_example.SerializeToString())\n",
    "# #             count_display+=1\n",
    "# #             if count_display >20:\n",
    "# #                 display.clear_output(wait=True)  \n",
    "#     print(\"Generation of TFRecords is finished.\")\n",
    "\n",
    "\n",
    "# def Make_TFRecords(feature_samples_dict, file_nameToSave):\n",
    "#     # start to write \n",
    "#     print(\"writing tfrecords....\")\n",
    "#     count_display = 0\n",
    "#     # strat to write postive train dicoms\n",
    "    \n",
    "#     with tf.io.TFRecordWriter(file_name) as writer:\n",
    "#         for dicom_path, labels in feature_samples_dict.items():\n",
    "#             print(\"dicom_path:\", dicom_path)\n",
    "#     #         dicom_path_string = open(dicom_path, 'rb').read()\n",
    "#             seg_label = labels[0]\n",
    "#             print(seg_label.shape)\n",
    "#             cls_label = labels[1]\n",
    "#             tf_example = image_example(dicom_path, seg_label, cls_label)  # need image_example function to genearte mesage for writing tf records\n",
    "#             writer.write(tf_example.SerializeToString())\n",
    "#             count_display+=1\n",
    "#             if count_display >20:\n",
    "#                 display.clear_output(wait=True)  \n",
    "#     print(\"Generation of TFRecords is finished.\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Train TFRECORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i: 1060\n",
      "final_cls_list[i]: [0, 0, 0]\n",
      "num_non_zero_clses 0\n",
      "i: 1061\n",
      "final_cls_list[i]: [0, 0, 0]\n",
      "num_non_zero_clses 0\n",
      "i: 1062\n",
      "final_cls_list[i]: [0, 0, 0]\n",
      "num_non_zero_clses 0\n",
      "i: 1063\n",
      "final_cls_list[i]: [0, 0, 0]\n",
      "num_non_zero_clses 0\n",
      "i: 1064\n",
      "final_cls_list[i]: [0, 0, 0]\n",
      "num_non_zero_clses 0\n",
      "i: 1065\n",
      "final_cls_list[i]: [0, 0, 0]\n",
      "num_non_zero_clses 0\n",
      "i: 1066\n",
      "final_cls_list[i]: [0, 0, 0]\n",
      "num_non_zero_clses 0\n",
      "i: 1067\n",
      "final_cls_list[i]: [0, 0, 0]\n",
      "num_non_zero_clses 0\n",
      "i: 1068\n",
      "final_cls_list[i]: [0, 0, 0]\n",
      "num_non_zero_clses 0\n",
      "i: 1069\n",
      "final_cls_list[i]: [0, 0, 0]\n",
      "num_non_zero_clses 0\n",
      "i: 1070\n",
      "final_cls_list[i]: [0, 0, 0]\n",
      "num_non_zero_clses 0\n",
      "i: 1071\n",
      "final_cls_list[i]: [0, 0, 0]\n",
      "num_non_zero_clses 0\n",
      "i: 1072\n",
      "final_cls_list[i]: [0, 0, 0]\n",
      "num_non_zero_clses 0\n",
      "i: 1073\n",
      "final_cls_list[i]: [0, 0, 0]\n",
      "num_non_zero_clses 0\n",
      "i: 1074\n",
      "final_cls_list[i]: [0, 0, 0]\n",
      "num_non_zero_clses 0\n",
      "i: 1075\n",
      "final_cls_list[i]: [0, 0, 0]\n",
      "num_non_zero_clses 0\n",
      "i: 1076\n",
      "final_cls_list[i]: [0, 0, 0]\n",
      "num_non_zero_clses 0\n",
      "i: 1077\n",
      "final_cls_list[i]: [0, 0, 0]\n",
      "num_non_zero_clses 0\n",
      "i: 1078\n",
      "final_cls_list[i]: [0, 0, 0]\n",
      "num_non_zero_clses 0\n",
      "i: 1079\n",
      "final_cls_list[i]: [0, 0, 0]\n",
      "num_non_zero_clses 0\n",
      "total positive samples: 3048\n",
      "total negative samples: 33515\n"
     ]
    }
   ],
   "source": [
    "# for train\n",
    "train_root_folder_path_for_cases= \"E:/dataset/Leisang/myTry/BleedingDataDCM/train/\"\n",
    "\n",
    "# give the location of the file\n",
    "train_excel_loc = \"E:\\\\dataset\\\\Leisang\\\\myTry\\\\BleedingDataDCM/TrainfilpOrNot.xlsx\"\n",
    "\n",
    "\n",
    "# make positive and negative samples:\n",
    "\n",
    "# train_positive_samples, train_negative_samples = make_samples_win(root_folder_path_for_cases=train_root_folder_path_for_cases,\n",
    "#                                                               excel_loc = train_excel_loc,\n",
    "#                                                                     train_or_val=\"train\")\n",
    "\n",
    "# make sample pairs: [(inputs1, seg_tar1, cls_tar1) , （inputs2, seg_tar2, cls_tar2）， (inputs3, seg_tar3, cls_tar3)] \n",
    "\n",
    "positive_sample_features_list, negative_sample_features_list =make_sample_pairs(root_folder_path_for_cases=train_root_folder_path_for_cases,\n",
    "                                                              excel_loc = train_excel_loc,\n",
    "                                                                    train_or_val=\"train\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample index: 33510\n",
      "sample index: 33511\n",
      "sample index: 33512\n",
      "sample index: 33513\n",
      "sample index: 33514\n",
      "Generation of TFRecords is finished.\n"
     ]
    }
   ],
   "source": [
    "# ## Make into functions for genrerate tf records\n",
    "# train_positive_file_name =  \"E:\\\\dataset\\\\Leisang\\\\myTry\\\\BleedingDataDCM/train_positive_sample_pairs_ubuntu.tfrecords\"\n",
    "# train_negative_file_name =  'E:\\\\dataset\\\\Leisang\\\\myTry\\\\BleedingDataDCM/train_negative_sample_pairs_ubuntu.tfrecords'\n",
    "# for lab windows\n",
    "train_positive_file_name =  \"E:\\\\dataset\\\\Leisang\\\\myTry\\\\BleedingDataDCM/train_positive_sample_pairs_WinLab.tfrecords\"\n",
    "train_negative_file_name =  'E:\\\\dataset\\\\Leisang\\\\myTry\\\\BleedingDataDCM/train_negative_sample_pairs_WinLab.tfrecords'\n",
    "# make paris dataset for positive\n",
    "Make_InputPairsTFrecords(positive_sample_features_list,file_nameToSave=train_positive_file_name )\n",
    "\n",
    "# make paris dataset for negative\n",
    "Make_InputPairsTFrecords(negative_sample_features_list,file_nameToSave=train_negative_file_name )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## checek the generated TRAIN TFRECORDS dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "train_positive_dataset = tf.data.TFRecordDataset(train_positive_file_name)\n",
    "train_negative_dataset = tf.data.TFRecordDataset(train_negative_file_name)\n",
    "# Create a dictionary describing the features.\n",
    "image_feature_description = {\n",
    "    \n",
    "        'dicom_path1': tf.io.FixedLenFeature([], tf.string),\n",
    "        'seg_label1': tf.io.FixedLenFeature([], tf.string), \n",
    "        'cls_label1': tf.io.FixedLenFeature([], tf.int64),\n",
    "        \n",
    "        'dicom_path2': tf.io.FixedLenFeature([], tf.string),\n",
    "        'seg_label2': tf.io.FixedLenFeature([], tf.string), \n",
    "        'cls_label2': tf.io.FixedLenFeature([], tf.int64),\n",
    "        \n",
    "        'dicom_path3': tf.io.FixedLenFeature([], tf.string),\n",
    "        'seg_label3': tf.io.FixedLenFeature([], tf.string), \n",
    "        'cls_label3': tf.io.FixedLenFeature([], tf.int64),\n",
    "}\n",
    "\n",
    "\n",
    "def _parse_image_function(example_proto):\n",
    "    parsed_features = tf.io.parse_single_example(example_proto, image_feature_description)\n",
    "    # Parse the input tf.Example proto using the dictionary above.\n",
    "    \n",
    "    # for input 1 --->\n",
    "    # decode dicom\n",
    "    dicom_path1 = parsed_features[\"dicom_path1\"]\n",
    "    image_bytes1 = tf.io.read_file(dicom_path1)\n",
    "    input_image1 = tf.cast(tfio.image.decode_dicom_image(image_bytes1, dtype=tf.uint16), tf.float32)\n",
    "    # decode mask\n",
    "    seg_label1 = tf.cast(tf.io.decode_raw(parsed_features['seg_label1'], tf.uint8), tf.float32)\n",
    "    seg_label1 = tf.reshape(seg_label1, [-1, 512,512,1])\n",
    "    \n",
    "    # for input 2 --->\n",
    "    # decode dicom\n",
    "    dicom_path2 = parsed_features[\"dicom_path2\"]\n",
    "    image_bytes2 = tf.io.read_file(dicom_path2)\n",
    "    input_image2 = tf.cast(tfio.image.decode_dicom_image(image_bytes2, dtype=tf.uint16), tf.float32)\n",
    "    # decode mask\n",
    "    seg_label2 = tf.cast(tf.io.decode_raw(parsed_features['seg_label2'], tf.uint8), tf.float32)\n",
    "    seg_label2 = tf.reshape(seg_label2, [-1, 512,512,1])\n",
    "    \n",
    "    # for input 3 --->\n",
    "    # decode dicom\n",
    "    dicom_path3 = parsed_features[\"dicom_path3\"]\n",
    "    image_bytes3 = tf.io.read_file(dicom_path3)\n",
    "    input_image3 = tf.cast(tfio.image.decode_dicom_image(image_bytes3, dtype=tf.uint16), tf.float32)\n",
    "    # decode mask\n",
    "    seg_label3 = tf.cast(tf.io.decode_raw(parsed_features['seg_label3'], tf.uint8), tf.float32)\n",
    "    seg_label3 = tf.reshape(seg_label3, [-1, 512,512,1])\n",
    "    \n",
    "    return (dicom_path1, dicom_path2, dicom_path3), (input_image1, input_image2, input_image3), (seg_label1, seg_label2, seg_label3),  (parsed_features[\"cls_label1\"], parsed_features[\"cls_label2\"], parsed_features[\"cls_label3\"])\n",
    "\n",
    "\n",
    "parsed_train_positive_dataset = train_positive_dataset.map(_parse_image_function)\n",
    "parsed_train_negative_dataset = train_negative_dataset.map(_parse_image_function)\n",
    "\n",
    "def truncate(x, min, max):\n",
    "#     print(x.shape)\n",
    "    cliped =  tf.clip_by_value(x, min, max)\n",
    "    return cliped\n",
    " \n",
    "def norm(x, min, max):\n",
    "    # normalize_value = (value − min_value) / (max_value − min_value)\n",
    "    tensor = tf.math.divide(tf.subtract(x, min),\n",
    "                    tf.subtract(max, min))\n",
    "    return tensor\n",
    "\n",
    "def linear_normalization(input, min=30720.0, max=34816.0):\n",
    "    truncated_input = truncate(input, min, max)\n",
    "    norm_input = norm(truncated_input, min, max )\n",
    "    return  norm_input\n",
    "\n",
    "def winwise(input,LB,HB):\n",
    "    # 20 ,380 for range (-32768, 32767)\n",
    "    # for tf input , (0, 65535)-? LB =  32788, 33148\n",
    "    input[input<LB] = LB # low boundary , if < LW , set to LW\n",
    "    input[input>HB] = HB # high boundary, if > Hw, Set to 255\n",
    "    return input\n",
    "\n",
    "def plot_one_input(dicom_path, input, seg_label, cls_label):\n",
    "    palette = copy(plt.cm.gray)\n",
    "    palette.set_over('r', 1.0)\n",
    "    print(\"dicom_path:\", dicom_path)\n",
    "    print(\"input_image.shape\", input.shape)\n",
    "    print(\"seg_label\", seg_label.shape)\n",
    "    print(\"cls_label\", cls_label)\n",
    "    num_nonzeros=np.count_nonzero(seg_label)\n",
    "    # reshape label \n",
    "    target =  tf.reshape(seg_label, input.shape)\n",
    "    fig, axes = plt.subplots(1,2, figsize=(20,20))\n",
    "    \n",
    "    mask =  np.squeeze(target.numpy())\n",
    "    norm_mask =mask/255.0\n",
    "# #     input_arr = np.squeeze(winwise(input.numpy(), 32788,33148 ))\n",
    "    norm_input = linear_normalization(input, 32788.0,33148.0)\n",
    "    norm_input = np.squeeze(norm_input.numpy())\n",
    "#     masked_in = norm_mask + norm_input\n",
    "#     print('masked_in range:[{}, {}]'.format((masked_in.numpy().min()), masked_in.numpy().max()))\n",
    "\n",
    "#     masked = np.ma.masked_where(norm_mask==0, masked_in)  # this is only generated mask\n",
    "#     print('masked range:[{}, {}]'.format(masked.min(), masked.max())) \n",
    "#     print()\n",
    "    axes[0].imshow(norm_input, cmap='gray')\n",
    "    axes[0].set_title('input range:[{}, {}]'.format((norm_input.min()), np.max(norm_input)))\n",
    "    axes[1].imshow(norm_mask, cmap='gray')\n",
    "    axes[1].set_title('seg target range:[{}, {}] \\n cls_tar: {} # of nonzeros: {}'.format(np.min(norm_mask), np.max(norm_mask), cls_label, num_nonzeros))\n",
    "    \n",
    "    print(\"norm_mask\", norm_mask.shape)\n",
    "    print(\"norm_input\", norm_input.shape)\n",
    "    masked_in = norm_mask + norm_input\n",
    "    print('masked_in range:[{}, {}]'.format((masked_in.min()), masked_in.max()))\n",
    "\n",
    "    masked = np.ma.masked_where(norm_mask==0, masked_in)  # this is only generated mask\n",
    "    print('masked range:[{}, {}]'.format(masked.min(), masked.max())) \n",
    "#     axes[2].imshow(np.squeeze(norm_input), cmap='gray')\n",
    "#     axes[2].imshow(np.squeeze(masked), palette, colors.Normalize(vmin=0, vmax=1), interpolation='none', alpha=0.4)\n",
    "#     axes[2].set_title('target range:[{}, {}]'.format(np.min(masked), np.max(masked)) + dicom_path)\n",
    "    \n",
    "    fig2, axes2 = plt.subplots(1,1, figsize=(20,20))\n",
    "    axes2.imshow(norm_input, cmap='gray')\n",
    "    axes2.imshow(masked, palette, colors.Normalize(vmin=0, vmax=1), interpolation='none', alpha=0.4)\n",
    "    axes2.set_title('target range:[{}, {}]'.format(np.min(masked), np.max(masked)) + dicom_path)\n",
    "    \n",
    "    \n",
    "    \n",
    "def check_dataset(dataset):\n",
    "    # window the input\n",
    "  \n",
    "\n",
    "    # print(len(parsed_train_positive_dataset))\n",
    "    BUFFER_SIZE =512\n",
    "    # random shuffle the train positive dagtaset\n",
    "    parsed_dataset = dataset.shuffle(BUFFER_SIZE)\n",
    "    \n",
    " \n",
    "#     palette.set_under('g', 1.0)\n",
    "#     palette.set_bad('b', 1.0)\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    for dicom_paths, inputs, seg_labels, cls_labels in parsed_dataset.take(1):\n",
    "        for i in range(len(dicom_paths)):\n",
    "            plot_one_input(dicom_paths[i], inputs[i], seg_labels[i], cls_labels[i])\n",
    "            \n",
    "\n",
    "       \n",
    "\n",
    "        \n",
    "check_dataset(parsed_train_positive_dataset)\n",
    "check_dataset(parsed_train_negative_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Val TFRECORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i: 2100\n",
      "final_cls_list[i]: [0, 0, 0]\n",
      "num_non_zero_clses 0\n",
      "i: 2101\n",
      "final_cls_list[i]: [0, 0, 0]\n",
      "num_non_zero_clses 0\n",
      "i: 2102\n",
      "final_cls_list[i]: [0, 0, 0]\n",
      "num_non_zero_clses 0\n",
      "i: 2103\n",
      "final_cls_list[i]: [0, 0, 0]\n",
      "num_non_zero_clses 0\n",
      "i: 2104\n",
      "final_cls_list[i]: [0, 0, 0]\n",
      "num_non_zero_clses 0\n",
      "i: 2105\n",
      "final_cls_list[i]: [0, 0, 0]\n",
      "num_non_zero_clses 0\n",
      "i: 2106\n",
      "final_cls_list[i]: [0, 0, 0]\n",
      "num_non_zero_clses 0\n",
      "i: 2107\n",
      "final_cls_list[i]: [0, 0, 0]\n",
      "num_non_zero_clses 0\n",
      "i: 2108\n",
      "final_cls_list[i]: [0, 0, 0]\n",
      "num_non_zero_clses 0\n",
      "i: 2109\n",
      "final_cls_list[i]: [0, 0, 0]\n",
      "num_non_zero_clses 0\n",
      "i: 2110\n",
      "final_cls_list[i]: [0, 0, 0]\n",
      "num_non_zero_clses 0\n",
      "total positive samples: 238\n",
      "total negative samples: 4214\n"
     ]
    }
   ],
   "source": [
    "# for val\n",
    "val_root_folder_path_for_cases= \"E:/dataset/Leisang/myTry/BleedingDataDCM/val/\"\n",
    "\n",
    "# give the location of the file\n",
    "val_excel_loc = \"E:\\\\dataset\\\\Leisang\\\\myTry\\\\BleedingDataDCM/valfilpOrNot.xlsx\"\n",
    "\n",
    "\n",
    "# make positive and negative samples:\n",
    "\n",
    "val_positive_sample_features_list, val_negative_sample_features_list =make_sample_pairs(root_folder_path_for_cases=val_root_folder_path_for_cases,\n",
    "                                                              excel_loc = val_excel_loc,\n",
    "                                                                    train_or_val=\"val\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample index: 4210\n",
      "sample index: 4211\n",
      "sample index: 4212\n",
      "sample index: 4213\n",
      "Generation of TFRecords is finished.\n"
     ]
    }
   ],
   "source": [
    "# ## Make into functions for genrerate tf records\n",
    "# train_positive_file_name =  \"E:\\\\dataset\\\\Leisang\\\\myTry\\\\BleedingDataDCM/train_positive_sample_pairs_win.tfrecords\"\n",
    "# train_negative_file_name =  'E:\\\\dataset\\\\Leisang\\\\myTry\\\\BleedingDataDCM/train_negative_sample_pairs_win.tfrecords'\n",
    "\n",
    "# val_positive_file_name =  \"E:\\\\dataset\\\\Leisang\\\\myTry\\\\BleedingDataDCM/val_positive_sample_pairs_ubuntu.tfrecords\"\n",
    "# val_negative_file_name =  'E:\\\\dataset\\\\Leisang\\\\myTry\\\\BleedingDataDCM/val_negative_sample_pairs_ubuntu.tfrecords'\n",
    "# for lab win\n",
    "val_positive_file_name =  \"E:\\\\dataset\\\\Leisang\\\\myTry\\\\BleedingDataDCM/val_positive_sample_pairs_WinLab.tfrecords\"\n",
    "val_negative_file_name =  'E:\\\\dataset\\\\Leisang\\\\myTry\\\\BleedingDataDCM/val_negative_sample_pairs_WinLab.tfrecords'\n",
    "# make paris dataset for positive\n",
    "Make_InputPairsTFrecords(val_positive_sample_features_list,file_nameToSave=val_positive_file_name )\n",
    "\n",
    "# make paris dataset for negative\n",
    "Make_InputPairsTFrecords(val_negative_sample_features_list,file_nameToSave=val_negative_file_name )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# make val_positive\n",
    "# Make_TFRecords(feature_samples_dict=val_negative_samples, file_name=val_negative_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "val_positive_dataset = tf.data.TFRecordDataset(val_positive_file_name)\n",
    "val_negative_dataset = tf.data.TFRecordDataset(val_negative_file_name)\n",
    "parsed_val_positive_dataset = val_positive_dataset.map(_parse_image_function)\n",
    "parsed_val_negative_dataset = val_negative_dataset.map(_parse_image_function)\n",
    "\n",
    "check_dataset(parsed_val_positive_dataset)\n",
    "check_dataset(parsed_val_negative_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
