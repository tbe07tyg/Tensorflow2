{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "from glob import glob\n",
    "!pip install -q tensorflow-io\n",
    "import tensorflow_io as tfio\n",
    "\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from IPython import display\n",
    "import imageio\n",
    "\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HYPERPARAMETERS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read dataset from train and validation tfrecords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paths for the tfrecords\n",
    "# for train\n",
    "train_positive =  \"E:\\\\dataset\\\\Leisang\\\\myTry\\\\BleedingDataDCM/train_positive_samples_win.tfrecords\"\n",
    "train_negative =  'E:\\\\dataset\\\\Leisang\\\\myTry\\\\BleedingDataDCM/train_negative_samples_win.tfrecords'\n",
    "\n",
    "# for val\n",
    "val_positive =  \"E:\\\\dataset\\\\Leisang\\\\myTry\\\\BleedingDataDCM/val_positive_samples_win.tfrecords\"\n",
    "val_negative =  'E:\\\\dataset\\\\Leisang\\\\myTry\\\\BleedingDataDCM/val_negative_samples_win.tfrecords'\n",
    "\n",
    "# for ubuntu\n",
    "\n",
    "# # for train\n",
    "# train_positive =  \"/media/ytx/Japan_Deep_Data/dataset/LeiSang/myTry/BleedingDataDCM/train_positive_samples_unbuntu.tfrecords\"\n",
    "# train_negative =  '/media/ytx/Japan_Deep_Data/dataset/LeiSang/myTry/BleedingDataDCM/train_negative_samples_unbuntu.tfrecords'\n",
    "\n",
    "# # for val\n",
    "# val_positive =  \"/media/ytx/Japan_Deep_Data/dataset/LeiSang/myTry/BleedingDataDCM/val_positive_samples_unbuntu.tfrecords\"\n",
    "# val_negative =  '/media/ytx/Japan_Deep_Data/dataset/LeiSang/myTry/BleedingDataDCM/val_negative_samples_unbuntu.tfrecords'\n",
    "\n",
    "train_positive_dataset = tf.data.TFRecordDataset(train_positive)\n",
    "train_negative_dataset = tf.data.TFRecordDataset(train_negative)\n",
    "\n",
    "val_positive_dataset = tf.data.TFRecordDataset(val_positive)\n",
    "val_negative_dataset = tf.data.TFRecordDataset(val_negative)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PREPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_HEIGHT = 256\n",
    "IMG_WIDTH = 256\n",
    "\n",
    "def resize(input, target):\n",
    "    print(input.shape)\n",
    "    print(target.shape)\n",
    "    resized_input = tf.image.resize(input, [IMG_HEIGHT, IMG_WIDTH],\n",
    "                                method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "    resized_target = tf.image.resize(target, [IMG_HEIGHT, IMG_WIDTH],\n",
    "                                method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "    resized_input =  tf.reshape(resized_input, [IMG_HEIGHT, IMG_WIDTH, 1])\n",
    "    resized_target =  tf.reshape(resized_target, [IMG_HEIGHT, IMG_WIDTH, 1])\n",
    "    return  resized_input, resized_target\n",
    "\n",
    "# @tf.function()\n",
    "def truncate(x, min, max):\n",
    "#     print(x.shape)\n",
    "    cliped =  tf.clip_by_value(x, min, max)\n",
    "    return cliped\n",
    " \n",
    "def norm(x, min, max):\n",
    "    # normalize_value = (value − min_value) / (max_value − min_value)\n",
    "    tensor = tf.math.divide(tf.subtract(x, min),\n",
    "                    tf.subtract(max, min))\n",
    "    return tensor\n",
    "\n",
    "def linear_normalization(input, min=30720.0, max=34816.0):\n",
    "    truncated_input = truncate(input, min, max)\n",
    "    norm_input = norm(truncated_input, min, max )\n",
    "    return  norm_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, None, None, None)\n",
      "(None, 512, 512, 1)\n",
      "(256, 256, 1)\n",
      "(256, 256, 1)\n"
     ]
    }
   ],
   "source": [
    "# parser the dataset to decode the features\n",
    "# Create a dictionary describing the features.\n",
    "image_feature_description = {\n",
    "        'dicom_path': tf.io.FixedLenFeature([], tf.string),\n",
    "        'seg_label': tf.io.FixedLenFeature([], tf.string), \n",
    "        'cls_label': tf.io.FixedLenFeature([], tf.int64),\n",
    "}\n",
    "\n",
    "@tf.function()\n",
    "def _parse_image_function(example_proto):\n",
    "    # extract features # Parse the input tf.Example proto using the dictionary above.\n",
    "    parsed_features = tf.io.parse_single_example(example_proto, image_feature_description)\n",
    "       \n",
    "    # decode dicom\n",
    "    dicom_path = parsed_features[\"dicom_path\"]\n",
    "    image_bytes = tf.io.read_file(dicom_path)\n",
    "    input_image = tf.cast(tfio.image.decode_dicom_image(image_bytes, dtype=tf.uint16), tf.float32)\n",
    "    # decode mask\n",
    "    seg_label = tf.cast(tf.io.decode_raw(parsed_features['seg_label'], tf.uint8), tf.float32)\n",
    "    # reshape\n",
    "    seg_label = tf.reshape(seg_label, [-1, 512,512,1])\n",
    "    \n",
    "    \n",
    "    # preprocessing--->\n",
    "    # resize\n",
    "    input_image, seg_label = resize(input_image, seg_label)\n",
    "    print(input_image.shape)\n",
    "    print(seg_label.shape)\n",
    "    # normalize\n",
    "    input_image = linear_normalization(input_image)\n",
    "    seg_label =  seg_label/255.0\n",
    "    \n",
    "#     print(\"range of cliped_input: [{}, {}]\".format(np.min(input_image), np.max(input_image)))\n",
    "#     print(\"range of cliped_target: [{}, {}]\".format(np.min(seg_label), np.max(seg_label)))\n",
    "    \n",
    "    \n",
    "    return dicom_path, input_image, seg_label,  parsed_features[\"cls_label\"]\n",
    "\n",
    "# train: positive\n",
    "train_nb_pos = 3035\n",
    "train_nb_neg = 33588\n",
    "parsed_train_positive_dataset = train_positive_dataset.map(_parse_image_function)\n",
    "parsed_train_negative_dataset = train_negative_dataset.map(_parse_image_function)\n",
    "\n",
    "val_nb_pos = 238\n",
    "val_nb_neg = 4420\n",
    "parsed_val_positive_dataset = val_positive_dataset.map(_parse_image_function)\n",
    "parsed_val_negative_dataset = val_negative_dataset.map(_parse_image_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # check the dataset\n",
    "# def winwise(input,LB,HB):\n",
    "#         # 20 ,380 for range (-32768, 32767)\n",
    "#         # for tf input , (0, 65535)-? LB =  32788, 33148\n",
    "#         input[input<LB] = LB # low boundary , if < LW , set to LW\n",
    "#         input[input>HB] = HB # high boundary, if > Hw, Set to 255\n",
    "#         return input\n",
    "\n",
    "for image_features in parsed_train_positive_dataset.take(2):\n",
    "    dicom_path =  image_features[0]\n",
    "    input =  image_features[1]\n",
    "    #     dicom_path = image_features[0].numpy()\n",
    "    target = image_features[2]\n",
    "    cls_label = image_features[3]\n",
    "    print(\"dicom_path:\", dicom_path)\n",
    "    print(\"input_image.shape\", input.shape)\n",
    "    print(\"seg_label\", target.shape)\n",
    "    print(\"cls_label\", cls_label)\n",
    "    # reshape label \n",
    "    target =  tf.reshape(target, input.shape)\n",
    "    # # for lab ubuntu system\n",
    "    # input, target, image_path= load('/media/ytx/Japan_Deep_Data/dataset/LeiSang/myTry/BleedingDataDCM/train/ZA-006_000/00000001.DCM')\n",
    "    fig, axes = plt.subplots(1,2, figsize=(10,10))\n",
    "    axes[0].imshow(np.squeeze(input.numpy()), cmap='gray')\n",
    "    axes[0].set_title('input range:[{}, {}]'.format((input.numpy().min()), np.max(input.numpy())))\n",
    "    axes[1].imshow(np.squeeze(target.numpy()), cmap='gray')\n",
    "    axes[1].set_title('target range:[{}, {}]'.format(np.min(target), np.max(target)))\n",
    "    print(dicom_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DEFINE LOAD FUNCTION IN INPUT PIPELINE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## INPUT PIPELINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# balance the dataset\n",
    "train_dataset = tf.data.experimental.sample_from_datasets([parsed_train_positive_dataset, parsed_train_negative_dataset], weights=[0.5, 0.5])\n",
    "\n",
    "train_dataset = train_dataset.shuffle(BUFFER_SIZE)\n",
    "train_dataset = train_dataset.batch(BATCH_SIZE).prefetch(2)\n",
    "\n",
    "test_dataset = parsed_val_positive_dataset.concatenate(parsed_val_negative_dataset)\n",
    "test_dataset = test_dataset.shuffle(BUFFER_SIZE)  # for the random check if not comment it\n",
    "test_dataset = test_dataset.batch(BATCH_SIZE).prefetch(2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To use this dataset, you'll need the number of steps per epoch.\n",
    "\n",
    "The definition of \"epoch\" in this case is less clear. Say it's the number of batches required to see each positive example once:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resampled_steps_per_epoch = np.ceil(2.0*train_nb_pos/BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  check the dataset\n",
    "for Dicom_path, one_batch_input, one_batch_segLabel, one_batch_classLabel  in test_dataset.take(1):\n",
    "    print(\"dicom_path:\", Dicom_path[0])\n",
    "    print(\"input_image.shape\", one_batch_input.shape)\n",
    "    print(\"seg_label\", one_batch_segLabel.shape)\n",
    "    print(\"cls_label\", one_batch_classLabel)\n",
    "    fig4, axes4 = plt.subplots(1,2, figsize=(10,10))\n",
    "    axes4[0].imshow(np.squeeze(one_batch_input[0].numpy()), cmap='gray')\n",
    "    axes4[0].set_title(\"cliped_norm_input\")\n",
    "    axes4[1].imshow(np.squeeze(one_batch_segLabel[0].numpy()), cmap='gray')\n",
    "    axes4[1].set_title(\"cliped_norm_target\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DESIGN MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_CHANNELS = 1\n",
    "latent_dim =50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model input conponents\n",
    "en_inputs = tf.keras.layers.Input(shape=[IMG_HEIGHT,IMG_HEIGHT,1])\n",
    "de_inputs = tf.keras.layers.Input(shape=[latent_dim])\n",
    "# full model design and construct encoder, decoder , AE object\n",
    "# entire model\n",
    "\n",
    "# Define encoder part ---->\n",
    "x = en_inputs\n",
    "x = tf.keras.layers.Conv2D(\n",
    "                filters=16, kernel_size=3, strides=(1, 1), activation='relu', padding=\"same\")(x)\n",
    "x = tf.keras.layers.Conv2D(\n",
    "                filters=32, kernel_size=3, strides=(2, 2), activation='relu', padding=\"same\")(x)\n",
    "x = tf.keras.layers.Conv2D(\n",
    "                filters=64, kernel_size=3, strides=(2, 2), activation='relu', padding=\"SAME\")(x)\n",
    "x = tf.keras.layers.Conv2D(\n",
    "                filters=128, kernel_size=3, strides=(2, 2), activation='relu',padding=\"SAME\")(x)\n",
    "x = tf.keras.layers.Flatten()(x)\n",
    "# No activation\n",
    "latent_v = tf.keras.layers.Dense(latent_dim)(x)\n",
    "encoder = tf.keras.Model(inputs=en_inputs, outputs=latent_v, name='encoder')\n",
    "\n",
    "\n",
    "# Define decoder part ---->\n",
    "x = tf.keras.layers.Dense(units=131072, activation=tf.nn.relu)(de_inputs)\n",
    "x = tf.keras.layers.Reshape(target_shape=(32, 32, 128))(x)\n",
    "x = tf.keras.layers.Conv2DTranspose(\n",
    "      filters=64,\n",
    "      kernel_size=3,\n",
    "      strides=(2, 2),\n",
    "      padding=\"SAME\",\n",
    "      activation='relu')(x)\n",
    "x = tf.keras.layers.Conv2DTranspose(\n",
    "      filters=32,\n",
    "      kernel_size=3,\n",
    "      strides=(2, 2),\n",
    "      padding=\"SAME\",\n",
    "      activation='relu')(x)\n",
    "x = tf.keras.layers.Conv2DTranspose(\n",
    "      filters=16,\n",
    "      kernel_size=3,\n",
    "      strides=(2, 2),\n",
    "      padding=\"SAME\",\n",
    "      activation='relu')(x)\n",
    "\n",
    "decoded = tf.keras.layers.Conv2DTranspose(\n",
    "      filters=1, kernel_size=3, strides=(1, 1), padding=\"SAME\", use_bias=True)(x)\n",
    "# output with sigmoid\n",
    "decoded =  tf.sigmoid(decoded)\n",
    "decoder = tf.keras.Model(inputs=de_inputs, outputs=decoded, name='decoder')\n",
    "\n",
    "\n",
    "# Define AE model---->\n",
    "outputs = decoder(latent_v)\n",
    "autoencoder =  tf.keras.Model(inputs=en_inputs, outputs=outputs, name='AE')\n",
    "\n",
    "# a sperately decoder is struggle leave for the moment\n",
    "\n",
    "# # # create a placeholder for an encoded (32-dimensional) input\n",
    "# encoded_input = tf.keras.layers.Input(shape=(latent_dim))\n",
    "# # # retrieve the last layer of the autoencoder model\n",
    "# decoder_layer = autoencoder.layers[-1]\n",
    "# # # create the decoder model\n",
    "# decoder = tf.keras.Model(encoded_input, decoder_layer)\n",
    "             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.utils.plot_model(encoder, to_file=encoder_path, show_shapes=True, dpi=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.utils.plot_model(decoder, to_file=decoder_path, show_shapes=True, dpi=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.utils.plot_model(autoencoder, to_file=autoencoder_path, show_shapes=True, dpi=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OPTIMIZER AND OBJECTIVE LOSSES\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "BCE =  tf.keras.losses.BinaryCrossentropy() \n",
    "Huber =  tf.keras.losses.Huber(delta=0.1)\n",
    "@tf.function()\n",
    "# define losses\n",
    "def compute_loss(decoded_x, x):\n",
    "    \n",
    "    # cross_entropy,  use reduce mean not sum, otherwise loss will be very big\n",
    "    BCE_loss = BCE(y_true=x, y_pred=decoded_x) \n",
    "    \n",
    "    \n",
    "    # L1 loss \n",
    "#     L1_loss = tf.reduce_mean(tf.abs(x - decoded_x))\n",
    "    \n",
    "    # Huber_loss\n",
    "    Huber_loss = Huber(y_true=x, y_pred=decoded_x)\n",
    "    total_loss = Huber_loss + BCE_loss\n",
    "    return total_loss, BCE_loss, Huber_loss\n",
    "    \n",
    "# appliy graidients  this is acutaully trianing step\n",
    "# @tf.function()\n",
    "# def compute_apply_gradients(model, x, optimizer, epoch):\n",
    "#     decoded_x =  model(x)\n",
    "#     with tf.GradientTape() as tape:\n",
    "#         total_loss, CE_loss, L1_loss = compute_loss(decoded_x, x)\n",
    "#     gradients = tape.gradient(total_loss, model.trainable_variables)\n",
    "#     optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    \n",
    "#     with summary_writer.as_default():\n",
    "#         # write scalars to the tensorboard after each train step\n",
    "#         tf.summary.scalar('total_loss', total_loss, step=epoch)\n",
    "#         tf.summary.scalar('CE_loss', CE_loss, step=epoch)\n",
    "#         tf.summary.scalar('L1_loss', L1_loss, step=epoch) \n",
    "@tf.function()\n",
    "def train_step(model, input, target, epoch):\n",
    "    print(\"in trianing step\")\n",
    "    with tf.GradientTape() as tape:  # very interesting\n",
    "        decoded_img = model(input, training=True)\n",
    "        total_loss, BCE_loss, Huber_loss = compute_loss(decoded_img, target)\n",
    "    gradients = tape.gradient(total_loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    return total_loss, BCE_loss, Huber_loss\n",
    "\n",
    "@tf.function()\n",
    "def test_step(model, input, target, epoch):\n",
    "    with tf.GradientTape() as tape:  # very interesting\n",
    "        decoded_img = model(input, training=True)\n",
    "        total_loss, BCE_loss, Huber_loss = compute_loss(decoded_img, target)\n",
    "    return total_loss, BCE_loss, Huber_loss\n",
    "#     train_avg_loss(train_loss)\n",
    "#     train_avg_metric(metric)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # check the AE OUTPUT\n",
    "# print(\"inp.shape:\", cliped_norm_input.shape)\n",
    "# print(\"inp[tf.newaxis,...]:\", cliped_norm_input[tf.newaxis,...].shape)\n",
    "# AE_output = autoencoder(resized_input[tf.newaxis,...], training=False)  # inp is the image sample from cell code 6 ; \n",
    "# print(AE_output.shape)\n",
    "# plt.imshow(np.squeeze(AE_output[0,...]), cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training PREPARING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# define the epoch image check\n",
    "def generate_images(model, test_input, tar, path, total_loss, batch_idx, epoch, save=True):\n",
    "    prediction = model(test_input, training=True)\n",
    "\n",
    "   \n",
    "\n",
    "    display_list = [np.squeeze(test_input[0]), np.squeeze(tar[0]), np.squeeze(prediction[0])]\n",
    "    title = ['Input range:[{},{}]'.format(test_input[0].numpy().min(), test_input[0].numpy().max()), \n",
    "           'GT range:[{},{}]'.format(tar[0].numpy().min(), tar[0].numpy().max()), 'Pred. [B:{}/E:{}]: loss->'.format(batch_idx,epoch)+ str(total_loss)]\n",
    "    \n",
    "    fig5 = plt.figure(figsize=(15,5))\n",
    "    fig5.suptitle(str(path[0].numpy(), 'utf-8')) # decode convert byte b'' to normal ''\n",
    "  \n",
    "    for i in range(3):\n",
    "        ax = fig5.add_subplot(1,3, i+1,title=title[i])\n",
    "        # getting the pixel values between [0, 1] to plot it.\n",
    "        ax.imshow(display_list[i], cmap=\"gray\") # prediction in range[-1, 1]*0.5 = [-0.5, 0.5],+ 0.5=[0, 1]\n",
    "        ax.axis('off')\n",
    "     \n",
    "    if save==True:\n",
    "        fig5.savefig(save_figure_path + \"/image_at_epoch_{:04d}_batch_{}.png\".format(epoch,batch_idx))\n",
    "    \n",
    "    plt.figure(figsize=(8,8))\n",
    "    plt.title('Pred. [B:{}/E:{}]: loss->'.format(batch_idx,epoch)+ str(total_loss))\n",
    "    plt.imshow(np.squeeze(prediction[0]), cmap=\"gray\") # prediction in range[-1, 1]*0.5 = [-0.5, 0.5],+ 0.5=[0, 1]\n",
    "    plt.axis('off')\n",
    "    if save ==True:\n",
    "        plt.savefig(save_figure_path + \"/Predictions/pred_only_at_epoch_{:04d}_batch_{}.png\".format(epoch,batch_idx))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# define the  encoder latent space check\n",
    "def en_generate_images(model, test_input, tar, path, batch_idx, epoch, save=True):\n",
    "    prediction = model(test_input, training=True)\n",
    "\n",
    "   \n",
    "\n",
    "    display_list = [np.squeeze(test_input[0]), np.squeeze(tar[0]), np.squeeze(prediction[0])]\n",
    "    title = ['Input range:[{},{}]'.format(test_input[0].numpy().min(), test_input[0].numpy().max()), \n",
    "           'GT range:[{},{}]'.format(tar[0].numpy().min(), tar[0].numpy().max()), 'Pred. Latent Space {}'.format(prediction[0].shape)]\n",
    "    \n",
    "    fig5 = plt.figure(figsize=(15,5))\n",
    "    fig5.suptitle(str(path[0].numpy(), 'utf-8')) # decode convert byte b'' to normal ''\n",
    "  \n",
    "    for i in range(3):\n",
    "        ax = fig5.add_subplot(1,3, i+1,title=title[i])\n",
    "        # getting the pixel values between [0, 1] to plot it.\n",
    "        ax.imshow(display_list[i], cmap=\"gray\") # prediction in range[-1, 1]*0.5 = [-0.5, 0.5],+ 0.5=[0, 1]\n",
    "        ax.axis('off')\n",
    "     \n",
    "    if save==True:\n",
    "        fig5.savefig(save_figure_path + \"/image_at_epoch_{:04d}_batch_{}.png\".format(epoch,batch_idx))\n",
    "    \n",
    "    plt.figure(figsize=(8,8))\n",
    "    plt.title('Pred. Latent Space {}'.format(prediction[0].shape))\n",
    "#     plt.imshow(np.squeeze(prediction[0]), cmap=\"gray\") # prediction in range[-1, 1]*0.5 = [-0.5, 0.5],+ 0.5=[0, 1]\n",
    "    plt.axis('off')\n",
    "    if save ==True:\n",
    "        plt.savefig(save_figure_path + \"/Predictions/pred_only_at_epoch_{:04d}_batch_{}.png\".format(epoch,batch_idx))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def en_de_generate_images(encoder,decoder, test_input, tar, path, batch_idx, epoch, save=True):\n",
    "#     prediction = model(test_input, training=True)\n",
    "    latent_v = encoder(test_input, training=False)\n",
    "    prediction = decoder(latent_v)\n",
    "\n",
    "    display_list = [np.squeeze(test_input[0]), np.squeeze(tar[0]), np.squeeze(prediction[0])]\n",
    "    title = ['Input range:[{},{}]'.format(test_input[0].numpy().min(), test_input[0].numpy().max()), \n",
    "           'GT range:[{},{}]'.format(tar[0].numpy().min(), tar[0].numpy().max()), 'en-to-de Pred. out {}'.format(prediction.shape)]\n",
    "    \n",
    "    fig5 = plt.figure(figsize=(15,5))\n",
    "    fig5.suptitle(str(path[0].numpy(), 'utf-8')) # decode convert byte b'' to normal ''\n",
    "  \n",
    "    for i in range(3):\n",
    "        ax = fig5.add_subplot(1,3, i+1,title=title[i])\n",
    "        # getting the pixel values between [0, 1] to plot it.\n",
    "        ax.imshow(display_list[i], cmap=\"gray\") # prediction in range[-1, 1]*0.5 = [-0.5, 0.5],+ 0.5=[0, 1]\n",
    "        ax.axis('off')\n",
    "     \n",
    "    if save==True:\n",
    "        fig5.savefig(save_figure_path + \"/image_at_epoch_{:04d}_batch_{}.png\".format(epoch,batch_idx))\n",
    "    \n",
    "    plt.figure(figsize=(8,8))\n",
    "    plt.title('en-to-de Pred. out {}'.format(prediction.shape))\n",
    "    plt.imshow(np.squeeze(prediction[0]), cmap=\"gray\") # prediction in range[-1, 1]*0.5 = [-0.5, 0.5],+ 0.5=[0, 1]\n",
    "    plt.axis('off')\n",
    "    if save ==True:\n",
    "        plt.savefig(save_figure_path + \"/Predictions/pred_only_at_epoch_{:04d}_batch_{}.png\".format(epoch,batch_idx))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "epochs = 100\n",
    "# define opitmizer \n",
    "optimizer =  tf.keras.optimizers.Adam(1e-4)\n",
    "## get current working directory\n",
    "cwd = os.getcwd()\n",
    "print(\"current working directory:\", cwd)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "full_AE_saves =  os.path.join(cwd, save_figure_path)\n",
    "print(full_AE_saves)\n",
    "if not os.path.exists(full_AE_saves):\n",
    "    os.makedirs(full_AE_saves)\n",
    "predictions_save_path =os.path.join(full_AE_saves, \"Predictions\")\n",
    "\n",
    "if not os.path.exists(predictions_save_path):\n",
    "    os.makedirs(predictions_save_path)\n",
    "\n",
    "# define check points\n",
    "\n",
    "\n",
    "# pre_saved_ckpt_path = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "# change to pre-saved model path: \n",
    "# 8 \n",
    "pre_saved_ckpt_path = \"E:\\\\Projects\\\\logs\\dicoms\\\\fixedArranged\\\\fixed\\\\8Bleed_AE_Huber_BCE_Sigmoid_NormNarrow_ShuffleTr_RandWinTrTe_SaveBest\\\\training_checkpoints\\\\ckpt\"\n",
    "# for ubuntu\n",
    "# pre_saved_ckpt_path = \"/media/ytx/Japan_Deep_Data/DicomProject2020/logs/NewFramework/fixed/8Bleed_AE_Huber_BCE_Sigmoid_NormNarrow_ShuffleTr_RandWinTrTe_SaveBest/training_checkpoints/ckpt\"\n",
    "# the path for the new training\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "\n",
    "if not os.path.exists(pre_saved_ckpt_path):\n",
    "    print(\"please check the pre-trained saved model path\")\n",
    "#  contents of states to be saved as attributes on the checkpoint object\n",
    "checkpoint_ob = tf.train.Checkpoint(step= tf.Variable(1),\n",
    "                                    epoch=  tf.Variable(1),\n",
    "                                    optimizer=optimizer,\n",
    "                                 encoder=encoder,\n",
    "                                 decoder =  decoder,\n",
    "                                 autoencoder = autoencoder\n",
    "                                 )\n",
    "# define restore checkpoint manager\n",
    "restore_manager =  tf.train.CheckpointManager(checkpoint_ob, pre_saved_ckpt_path, max_to_keep=1)\n",
    "\n",
    "# # define checkpoint manager for new training\n",
    "# manager =  tf.train.CheckpointManager(checkpoint_ob, checkpoint_prefix, max_to_keep=1)\n",
    "\n",
    "datetime_rec =  datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "train_summary_writer = tf.summary.create_file_writer(\n",
    "  log_dir + \"train\")\n",
    "val_summary_writer = tf.summary.create_file_writer(\n",
    "  log_dir + \"val\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start tensorboard\n",
    "# !kill 5032\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir {log_dir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pre-trained model\n",
    "checkpoint_ob.restore(restore_manager.latest_checkpoint)\n",
    "\n",
    "step =  checkpoint_ob.step\n",
    "epoch =  checkpoint_ob.epoch\n",
    "print(int(step))\n",
    "print(int(epoch))\n",
    "\n",
    "AE=  checkpoint_ob.autoencoder\n",
    "en = checkpoint_ob.encoder\n",
    "de =  checkpoint_ob.decoder\n",
    "print(AE)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## use the pre-saved encoder as the base model and design the new model\n",
    "base_model  = en\n",
    "print(base_model.trainable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model =  en # use encoder as the base model\n",
    "\n",
    "# check the loaded ae output\n",
    "\n",
    "for TEST_dicom_path, TEST_input, TEST_target, TEST_cls_label in test_dataset.take(1):  # take one data from the test set\n",
    "    print(\"dicom_path:\", TEST_dicom_path[0])\n",
    "    print(\"input_image.shape\", TEST_input.shape)\n",
    "    print(\"seg_label\", TEST_target.shape)\n",
    "    print(\"cls_label\", TEST_cls_label)\n",
    "    test_total_loss, test_BCE_loss, test_Huber_loss = test_step(AE, TEST_input, TEST_target, epoch) \n",
    "    generate_images(AE, TEST_input, TEST_target,TEST_dicom_path,test_total_loss.numpy(), int(step), int(epoch), save=False)\n",
    "    \n",
    "    # check the loaded encoder and decoder\n",
    "    en_de_generate_images(base_model, de,TEST_input, TEST_target,TEST_dicom_path, int(step), int(epoch), save=False)\n",
    "\n",
    "# check the base base model traininable\n",
    "print(base_model.trainable)\n",
    "\n",
    "# \n",
    "trainable_or_not =  True\n",
    "\n",
    "if trainable_or_not == True:\n",
    "    base_model.trainable =  True # set encoder untranable\n",
    "print(\"after reset basemodel trainable property:\", base_model.trainable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the trainable model architecture\n",
    "base_model.summary()\n",
    "tf.keras.utils.plot_model(base_model, to_file=base_model_path, show_shapes=True, dpi=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add classification head for bleed exist or not\n",
    "print(\"# of layers in the base_model:\", len(base_model.layers))\n",
    "class_head_nodes = tf.keras.layers.Dense(2)(base_model.layers[-1].output)\n",
    "class_softmax_output =  tf.nn.softmax(class_head_nodes)\n",
    "print(base_model.output)\n",
    "print(base_model.layers[-1].output)\n",
    "Classifier =  tf.keras.Model(inputs=en_inputs, outputs=class_softmax_output, name='classifier_head')\n",
    "# check the architecture\n",
    "Classifier.summary()\n",
    "tf.keras.utils.plot_model(Classifier, to_file=classifier_path, show_shapes=True, dpi=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set THE SETMENTATION PART\n",
    "# 1st take out the feature from the base_model\n",
    "base_feature =  base_model.layers[-3].output\n",
    "print(base_feature)\n",
    "\n",
    "\n",
    "# define down_sample block as the pix-2-pix from tensorflow official set\n",
    "def downsample(filters, size, apply_batchnorm=True):\n",
    "    initializer = tf.random_normal_initializer(0., 0.02)\n",
    "\n",
    "    result = tf.keras.Sequential()\n",
    "    result.add(\n",
    "      tf.keras.layers.Conv2D(filters, size, strides=2, padding='same',\n",
    "                             kernel_initializer=initializer, use_bias=False))\n",
    "\n",
    "    if apply_batchnorm:\n",
    "        result.add(tf.keras.layers.BatchNormalization())\n",
    "\n",
    "    result.add(tf.keras.layers.LeakyReLU())\n",
    "\n",
    "    return result\n",
    "\n",
    "def upsample(filters, size, apply_dropout=False):\n",
    "    initializer = tf.random_normal_initializer(0., 0.02)\n",
    "\n",
    "    result = tf.keras.Sequential()\n",
    "    result.add(\n",
    "    tf.keras.layers.Conv2DTranspose(filters, size, strides=2,\n",
    "                                    padding='same',\n",
    "                                    kernel_initializer=initializer,\n",
    "                                    use_bias=False))\n",
    "\n",
    "    result.add(tf.keras.layers.BatchNormalization())\n",
    "\n",
    "    if apply_dropout:\n",
    "        result.add(tf.keras.layers.Dropout(0.5))\n",
    "\n",
    "    result.add(tf.keras.layers.ReLU())\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "# def skips needed to be connected\n",
    "skips = []\n",
    "\n",
    "def en_for_seg():\n",
    "# new another path for the segmentation of the new model\n",
    "    x = downsample(64, 4, apply_batchnorm=False)(en_inputs) # output  (None, 128, 128, 64)\n",
    "    skips.append(x)\n",
    "    x = downsample(128, 4)(x)  # output (None, 64, 64, 128)\n",
    "    skips.append(x)\n",
    "    x = downsample(256, 4)(x)  # output (None, 32, 32, 256)\n",
    "    skips.append(x)\n",
    "    # if want seperate train the generator, return the Model object e.g. return tf.keras.Model(inputs=inputs, outputs=x)\n",
    "    return tf.keras.Model(inputs=en_inputs, outputs=x)\n",
    "\n",
    "en_coder_for_seg_out =  en_for_seg()\n",
    "print(en_coder_for_seg_out.output)\n",
    "tf.keras.utils.plot_model(en_coder_for_seg_out, to_file=\"encodre_for_seg.png\", show_shapes=True, dpi=128)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# concatenate the feature from base and feature from the new enocoder\n",
    "concate_features =  tf.keras.layers.Concatenate()([base_feature, en_coder_for_seg_out.output])\n",
    "print(concate_features)\n",
    "\n",
    "# start to one more step to encoder\n",
    "further_en = downsample(512, 4)(concate_features)  # output (None, 16, 16, 512)\n",
    "print(\"one more step encoding:\",further_en)\n",
    "\n",
    "# global pooling for that one more step encoding fetures\n",
    "GA  = tf.keras.layers.GlobalAveragePooling2D()(further_en)  # squeeze\n",
    "print(\"GA:\", GA)\n",
    "# RESHAPE GA  TO THE IMAGE FORMMAT\n",
    "RESHAPE_GA =  tf.reshape(GA, [-1,1,1, GA.shape[-1]])\n",
    "print(\"RESHAPE_GA:\", RESHAPE_GA)\n",
    "#resize to the feature maps size\n",
    "resize_GA =  tf.image.resize(RESHAPE_GA, (further_en.shape[1], further_en.shape[2]))\n",
    "print(\"resized GA:\", resize_GA)\n",
    "# concate future_en and reisze_GA\n",
    "bottom_concate =  tf.keras.layers.Concatenate()([resize_GA, further_en])\n",
    "print(\"bottom_concate:\", bottom_concate)\n",
    "\n",
    "print()\n",
    "print(\"start to upsample--------->\")\n",
    "# start to upsample and decoder from resize_GA and further_en\n",
    "up_feature1 =  upsample(512, 4) (bottom_concate) #  upsample/relarge ti [None, 32, 32, 1024] without dropout default false for drop\n",
    "print(\"up_feature1:\", up_feature1)\n",
    "up_concate1 =  tf.keras.layers.Concatenate()([up_feature1, skips[-1]])\n",
    "print(\"up_concate1:\", up_concate1)\n",
    "up_feature2 =  upsample(256, 4) (up_concate1) #  upsample/relarge ti [None, 64, 64, 256] without dropout default false for drop\n",
    "print(\"up_feature2:\", up_feature2)\n",
    "up_concate2 =  tf.keras.layers.Concatenate()([up_feature2, skips[-2]])\n",
    "print(\"up_concate2:\", up_concate2)\n",
    "up_feature3 =  upsample(128, 4) (up_concate2) #  upsample/relarge ti [None, 128, 128, 128] without dropout default false for drop\n",
    "print(\"up_feature3:\", up_feature2)\n",
    "up_concate3 =  tf.keras.layers.Concatenate()([up_feature3, skips[-3]])\n",
    "print(\"up_concate3:\", up_concate3)\n",
    "\n",
    "# last layer output\n",
    "initializer = tf.random_normal_initializer(0., 0.02)\n",
    "last = tf.keras.layers.Conv2DTranspose(OUTPUT_CHANNELS, 4,\n",
    "                                         strides=2, # stride 2\n",
    "                                         padding='same',\n",
    "                                         kernel_initializer=initializer,\n",
    "                                         activation='sigmoid') # (bs, 256, 256, 3)\n",
    "seg_out  =  last(up_concate3)\n",
    "\n",
    "Final_seg_model = tf.keras.Model(inputs=en_inputs, outputs=[seg_out, class_softmax_output])\n",
    "tf.keras.utils.plot_model(Final_seg_model, to_file=\"new_model.png\", show_shapes=True, dpi=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check new model\n",
    "# check the AE OUTPUT\n",
    "# print(\"inp.shape:\", cliped_norm_input.shape)\n",
    "# print(\"inp[tf.newaxis,...]:\", cliped_norm_input[tf.newaxis,...].shape)\n",
    "# AE_output = autoencoder(resized_input[tf.newaxis,...], training=False)  # inp is the image sample from cell code 6 ; \n",
    "# print(AE_output.shape)\n",
    "# plt.imshow(np.squeeze(AE_output[0,...]), cmap=\"gray\")\n",
    "\n",
    "for TEST_dicom_path, TEST_input, TEST_target, TEST_cls_label in test_dataset.take(1):  # take one data from the test set\n",
    "    print(\"inp[tf.newaxis,...]:\", TEST_input.shape)\n",
    "    print(\"dicom_path:\", TEST_dicom_path[0])\n",
    "    print(\"input_image.shape\", TEST_input.shape)\n",
    "    print(\"seg_label\", TEST_target.shape)\n",
    "    print(\"cls_label\", TEST_cls_label)\n",
    "    \n",
    "    new_model_output =  Final_seg_model(TEST_input)\n",
    "    # outputs=[seg_out, class_sigmoid_output]\n",
    "    output1=   new_model_output[0]\n",
    "    output2=   new_model_output[1]\n",
    "    print(\"output1 shape:\", output1.shape)  \n",
    "    print(\"output2 shape:\", output2.shape)  \n",
    "#     test_total_loss, test_BCE_loss, test_Huber_loss = test_step(AE, TEST_input, TEST_target, epoch) \n",
    "#     generate_images(AE, TEST_input, TEST_target,TEST_dicom_path,test_total_loss.numpy(), int(step), int(epoch), save=False)\n",
    "    plt.imshow(np.squeeze(output1[0,...]), cmap=\"gray\")\n",
    "#     # check the loaded encoder and decoder\n",
    "#     en_de_generate_images(en, de,TEST_input, TEST_target,TEST_dicom_path, int(step), int(epoch), save=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# design new loss for the new model as new_train_step, new_test_step\n",
    "new_optimizer =  tf.keras.optimizers.Adam(1e-4)\n",
    "# from tf.keras.utils import to_categorical\n",
    "BCE =  tf.keras.losses.BinaryCrossentropy() \n",
    "Huber =  tf.keras.losses.Huber(delta=0.1)\n",
    "@tf.function()\n",
    "# define losses\n",
    "def new_compute_loss(pred_seg, gt_seg, pred_cls, gt_cls):\n",
    "    \n",
    "    #seg  cross_entropy,  use reduce mean not sum, otherwise loss will be very big\n",
    "    seg_BCE_loss = BCE(y_true=gt_seg, y_pred=pred_seg) \n",
    "    \n",
    "    \n",
    "    # L1 loss \n",
    "#     L1_loss = tf.reduce_mean(tf.abs(x - decoded_x))\n",
    "    \n",
    "    #seg Huber_loss\n",
    "    seg_Huber_loss = Huber(y_true=gt_seg, y_pred=pred_seg)\n",
    "    \n",
    "    seg_total_loss = seg_Huber_loss + seg_BCE_loss\n",
    "    \n",
    "    \n",
    "    # classification BCE_loss\n",
    "    cls_BCE_loss=  BCE(y_true=gt_cls, y_pred=pred_cls)\n",
    "    \n",
    "    # total_loss = cls_BCE_loss + seg_loss\n",
    "    total_loss = cls_BCE_loss + seg_total_loss\n",
    "    return total_loss, cls_BCE_loss, seg_total_loss\n",
    "    \n",
    "# appliy graidients  this is acutaully trianing step\n",
    "# @tf.function()\n",
    "# def compute_apply_gradients(model, x, optimizer, epoch):\n",
    "#     decoded_x =  model(x)\n",
    "#     with tf.GradientTape() as tape:\n",
    "#         total_loss, CE_loss, L1_loss = compute_loss(decoded_x, x)\n",
    "#     gradients = tape.gradient(total_loss, model.trainable_variables)\n",
    "#     optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    \n",
    "#     with summary_writer.as_default():\n",
    "#         # write scalars to the tensorboard after each train step\n",
    "#         tf.summary.scalar('total_loss', total_loss, step=epoch)\n",
    "#         tf.summary.scalar('CE_loss', CE_loss, step=epoch)\n",
    "#         tf.summary.scalar('L1_loss', L1_loss, step=epoch) \n",
    "def mask_to_categorical(cls_tar, num_cls=2):\n",
    "    tar = tf.one_hot(tf.cast(cls_tar, tf.int32), num_cls)\n",
    "    tar = tf.cast(tar, tf.float32)\n",
    "    return tar\n",
    "\n",
    "@tf.function()\n",
    "def new_train_step(model, input, seg_target, cls_target,epoch):\n",
    "    # change cls labe 0 or 1 into  [1, 0] [0, 1]\n",
    "    print(cls_target)\n",
    "    cls_target = mask_to_categorical(cls_target)\n",
    "#     print(\"in trianing step\")\n",
    "    with tf.GradientTape() as tape:  # very interesting\n",
    "        # outputs=[seg_out, class_sigmoid_output] \n",
    "        seg_out, class_softmax_output= model(input, training=True)\n",
    "        total_loss, cls_BCE_loss, seg_total_loss = new_compute_loss(pred_seg=seg_out,\n",
    "                                                            gt_seg=seg_target, \n",
    "                                                            pred_cls=class_softmax_output, \n",
    "                                                            gt_cls=cls_target)\n",
    "    gradients = tape.gradient(total_loss, model.trainable_variables)\n",
    "    new_optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    return total_loss, cls_BCE_loss, seg_total_loss\n",
    "\n",
    "@tf.function()\n",
    "def new_test_step(model, input, seg_target, cls_target, epoch):\n",
    "    with tf.GradientTape() as tape:  # very interesting\n",
    "        seg_out, class_softmax_output= model(input, training=False)\n",
    "        total_loss, cls_BCE_loss, seg_total_loss = new_compute_loss(pred_seg=seg_out,\n",
    "                                                            gt_seg=seg_target, \n",
    "                                                            pred_cls=class_softmax_output, \n",
    "                                                            gt_cls=cls_target)\n",
    "    return total_loss, cls_BCE_loss, seg_total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the epoch image check\n",
    "def new_generate_images(model, test_input, tar_seg, tar_cls, path, total_loss, batch_idx, epoch, save=True):\n",
    "    seg_out, class_sigmoid_output  = model(test_input, training=True)\n",
    "    \n",
    "    one_hot_cls_tar =  tf.keras.utils.to_categorical(tar_cls)\n",
    "\n",
    "    display_list = [np.squeeze(test_input[0]), np.squeeze(tar_seg[0]), np.squeeze(seg_out[0])]\n",
    "    title = ['Input range:[{},{}]'.format(test_input[0].numpy().min(), test_input[0].numpy().max()), \n",
    "           'GT range:[{},{}]'.format(tar_seg[0].numpy().min(), tar_seg[0].numpy().max()), '[B:{}/E:{}]: tar_cls:{}({})->pred_cls:{}'.format(batch_idx,epoch, \n",
    "                                                                                                                                    tar_cls[0],\n",
    "                                                                                                                                    one_hot_cls_tar[0], \n",
    "                                                                                                                                    class_sigmoid_output[0])]\n",
    "    \n",
    "    fig5 = plt.figure(figsize=(15,5))\n",
    "    fig5.suptitle(str(path[0].numpy(), 'utf-8')) # decode convert byte b'' to normal ''\n",
    "  \n",
    "    for i in range(3):\n",
    "        ax = fig5.add_subplot(1,3, i+1,title=title[i])\n",
    "        # getting the pixel values between [0, 1] to plot it.\n",
    "        ax.imshow(display_list[i], cmap=\"gray\") # prediction in range[-1, 1]*0.5 = [-0.5, 0.5],+ 0.5=[0, 1]\n",
    "        ax.axis('off')\n",
    "     \n",
    "    if save==True:\n",
    "        fig5.savefig(save_figure_path + \"/image_at_epoch_{:04d}_batch_{}.png\".format(epoch,batch_idx))\n",
    "    \n",
    "    plt.figure(figsize=(8,8))\n",
    "    plt.title('Pred. [B:{}/E:{}]: loss->'.format(batch_idx,epoch)+ str(total_loss))\n",
    "    plt.imshow(np.squeeze(seg_out[0]), cmap=\"gray\") # prediction in range[-1, 1]*0.5 = [-0.5, 0.5],+ 0.5=[0, 1]\n",
    "    plt.axis('off')\n",
    "    if save ==True:\n",
    "        plt.savefig(save_figure_path + \"/Predictions/pred_only_at_epoch_{:04d}_batch_{}.png\".format(epoch,batch_idx))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "\n",
    "# build_new check point manager\n",
    "new_ckpt_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "if not os.path.exists(checkpoint_prefix):\n",
    "    os.makedirs(checkpoint_prefix)\n",
    "#  contents of states to be saved as attributes on the checkpoint object\n",
    "new_ckpt_ob = tf.train.Checkpoint(step= tf.Variable(1),\n",
    "                                    epoch=  tf.Variable(1),\n",
    "                                    optimizer=new_optimizer,\n",
    "                                     new_model =  Final_seg_model\n",
    "                                 )\n",
    "# define checkpoint manager\n",
    "new_manager =  tf.train.CheckpointManager(new_ckpt_ob, checkpoint_prefix, max_to_keep=1)\n",
    "\n",
    "# check the whether there is a checkpoint in the checkpoint folder, if it is restore from it\n",
    "if new_manager.latest_checkpoint:\n",
    "    print(\"Restored from {}\".format(new_manager.latest_checkpoint))\n",
    "    new_ckpt_ob.restore(new_manager.latest_checkpoint)\n",
    "else:\n",
    "    print(\"Initializing from scratch.\")\n",
    "\n",
    "# reset checkcpoint.step for each epoch\n",
    "step =  new_ckpt_ob.step\n",
    "ckpt_epoch =  new_ckpt_ob.epoch  \n",
    "\n",
    "# test average_loss = for saving the test results\n",
    "test_avg_tmp_loss = 100\n",
    "\n",
    "for epoch in range(int(ckpt_epoch), epochs+1):\n",
    "    train_total_loss_mean = tf.keras.metrics.Mean()\n",
    "    train_seg_total_loss_mean = tf.keras.metrics.Mean()\n",
    "    train_cls_BCE_loss_mean = tf.keras.metrics.Mean()\n",
    "   \n",
    "    \n",
    "    # initial saving of checkpoints\n",
    "#     if epoch == 1:\n",
    "#         save_path =  manager.save()\n",
    "#         print(\"Initial saving checkpoints at epoch {} batch {} at path {}\".format(int(ckpt_epoch),  int(step) ,save_path))\n",
    "       \n",
    "        \n",
    "    \n",
    "    start_time =  time.time()\n",
    "    for train_dicom_path, train_input, train_seg_target, train_cls_label  in train_dataset:\n",
    "#         print(train_input.shape)\n",
    "#         print(target.shape)\n",
    "#         print(autoencoder)\n",
    "        # one training step\n",
    "        total_loss, cls_BCE_loss, seg_total_loss = new_train_step(Final_seg_model, train_input, train_seg_target, train_cls_label, epoch)\n",
    "        end_time =  time.time()\n",
    "        \n",
    "        # calculate mean value for trainning losses\n",
    "        train_total_loss_mean(total_loss)\n",
    "        train_seg_total_loss_mean(seg_total_loss)\n",
    "        train_cls_BCE_loss_mean(cls_BCE_loss)\n",
    "        \n",
    "#         #\n",
    "        if int(step) == 1:\n",
    "            for test_dicom_path, test_input, test_seg_target, test_cls_label in test_dataset.take(1):\n",
    "                new_generate_images(Final_seg_model, test_input, test_seg_target, test_cls_label, test_dicom_path,total_loss.numpy(), int(step), epoch)\n",
    "        \n",
    "        # every 100 batch step show the generate test image to check.\n",
    "        if int(step) % 50 == 0:\n",
    "            display.clear_output(wait=True)   \n",
    "            for test_dicom_path, test_input, test_seg_target, test_cls_label in test_dataset.take(1):\n",
    "                new_generate_images(Final_seg_model, test_input, test_seg_target, test_cls_label, test_dicom_path,total_loss.numpy(), int(step), epoch)\n",
    "                # every 400 steps save the genrate images\n",
    "          \n",
    "            print('Epoch{}: {}/{}: total_loss: {}; cls_BCE_loss: {}; seg_total_loss: {} '\n",
    "          'time elapse {}'.format(int(ckpt_epoch), int(step), resampled_steps_per_epoch, total_loss, cls_BCE_loss, seg_total_loss, end_time-start_time))\n",
    "                       \n",
    "             \n",
    "        \n",
    "          \n",
    "        if step > resampled_steps_per_epoch:\n",
    "            break\n",
    "        else:\n",
    "            step.assign_add(1)\n",
    "        \n",
    "    # print validation step\n",
    "    if ckpt_epoch % 1 == 0:\n",
    "        test_total_loss_mean = tf.keras.metrics.Mean()\n",
    "        test_seg_total_loss_mean = tf.keras.metrics.Mean()\n",
    "        test_cls_BCE_loss_mean = tf.keras.metrics.Mean()\n",
    "        for  test_dicom_path, test_input, test_seg_target, test_cls_label in test_dataset:\n",
    "            test_total_loss, test_cls_BCE_loss, test_seg_total_loss = new_test_step(Final_seg_model, test_input, test_seg_target, test_cls_label, epoch)\n",
    "            test_total_loss_mean(test_total_loss)\n",
    "            test_seg_total_loss_mean(test_seg_total_loss)\n",
    "            test_cls_BCE_loss_mean(test_cls_BCE_loss)\n",
    "                                                             \n",
    "      \n",
    "        \n",
    "        print('Epoch: {}, Test total_loss set loss: {},  time elapse for current epoch {}'.format(int(ckpt_epoch),\n",
    "                                                    test_total_loss_mean.result(),\n",
    "                                                    end_time - start_time))\n",
    "        \n",
    "        with train_summary_writer.as_default():\n",
    "            print(\"writing train logs to tensorboard...\")                                                       \n",
    "            # write scalars to the tensorboard after each train step\n",
    "            tf.summary.scalar('total_loss', train_total_loss_mean.result(), step=epoch)\n",
    "            tf.summary.scalar('seg_total_loss', train_seg_total_loss_mean.result(), step=epoch)\n",
    "            tf.summary.scalar('cls_BCE_los', train_cls_BCE_loss_mean.result(), step=epoch)\n",
    "            \n",
    "        with val_summary_writer.as_default():\n",
    "            print(\"writing val logs to tensorboard...\")                                                        \n",
    "            # write scalars to the tensorboard after each train step\n",
    "            tf.summary.scalar('total_loss', test_total_loss_mean.result(), step=epoch)\n",
    "            tf.summary.scalar('seg_total_loss', test_seg_total_loss_mean.result(), step=epoch)\n",
    "            tf.summary.scalar('cls_BCE_los', test_cls_BCE_loss_mean.result(), step=epoch)\n",
    "            \n",
    "        if test_total_loss_mean.result() < test_avg_tmp_loss:\n",
    "            save_path =  new_manager.save() # save the checkpoint and return the save path\n",
    "            print(\"Saved checkpoint for epoch {}-  step {}: {}\".format(int(ckpt_epoch), int(step), save_path))\n",
    "            test_avg_tmp_loss =  test_total_loss_mean.result()\n",
    "            \n",
    "        \n",
    "    ckpt_epoch.assign_add(1)     \n",
    "    step.assign(1)\n",
    "\n",
    "#     # saving (checkpoint) the model every 5 epochs\n",
    "#     if epoch % 5 == 0:\n",
    "#         print(\"saving checkpoints at epoch {}\".format(epoch))\n",
    "#         checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "\n",
    "print(\"training finished\")   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
