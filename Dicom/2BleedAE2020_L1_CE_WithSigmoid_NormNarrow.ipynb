{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "from glob import glob\n",
    "!pip install -q tensorflow-io\n",
    "import tensorflow_io as tfio\n",
    "\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from IPython import display\n",
    "import imageio\n",
    "\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HYPERPARAMETERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dicom_root E:/dataset/Leisang/myTry/BleedingDataDCM\n",
      "train_path E:/dataset/Leisang/myTry/BleedingDataDCM\\train\n",
      "val_path E:/dataset/Leisang/myTry/BleedingDataDCM\\val\n",
      "train_roots ['E:/dataset/Leisang/myTry/BleedingDataDCM\\\\train\\\\ZA-006_000', 'E:/dataset/Leisang/myTry/BleedingDataDCM\\\\train\\\\ZA-008_000', 'E:/dataset/Leisang/myTry/BleedingDataDCM\\\\train\\\\ZA-012_000', 'E:/dataset/Leisang/myTry/BleedingDataDCM\\\\train\\\\ZA-015_001', 'E:/dataset/Leisang/myTry/BleedingDataDCM\\\\train\\\\ZA-017_001', 'E:/dataset/Leisang/myTry/BleedingDataDCM\\\\train\\\\ZA-019_001', 'E:/dataset/Leisang/myTry/BleedingDataDCM\\\\train\\\\ZA-020_002', 'E:/dataset/Leisang/myTry/BleedingDataDCM\\\\train\\\\ZA-021_001', 'E:/dataset/Leisang/myTry/BleedingDataDCM\\\\train\\\\ZA-022_001', 'E:/dataset/Leisang/myTry/BleedingDataDCM\\\\train\\\\ZA-023_001', 'E:/dataset/Leisang/myTry/BleedingDataDCM\\\\train\\\\ZA-024_001', 'E:/dataset/Leisang/myTry/BleedingDataDCM\\\\train\\\\ZA-027_001', 'E:/dataset/Leisang/myTry/BleedingDataDCM\\\\train\\\\ZA-029_001', 'E:/dataset/Leisang/myTry/BleedingDataDCM\\\\train\\\\ZA-031_001', 'E:/dataset/Leisang/myTry/BleedingDataDCM\\\\train\\\\ZA-032_001', 'E:/dataset/Leisang/myTry/BleedingDataDCM\\\\train\\\\ZA-033_001', 'E:/dataset/Leisang/myTry/BleedingDataDCM\\\\train\\\\ZA-035_001', 'E:/dataset/Leisang/myTry/BleedingDataDCM\\\\train\\\\ZA-036_001', 'E:/dataset/Leisang/myTry/BleedingDataDCM\\\\train\\\\ZA-037_001', 'E:/dataset/Leisang/myTry/BleedingDataDCM\\\\train\\\\ZA-038_001', 'E:/dataset/Leisang/myTry/BleedingDataDCM\\\\train\\\\ZA-039_001', 'E:/dataset/Leisang/myTry/BleedingDataDCM\\\\train\\\\ZA-040_001', 'E:/dataset/Leisang/myTry/BleedingDataDCM\\\\train\\\\ZA-041_001', 'E:/dataset/Leisang/myTry/BleedingDataDCM\\\\train\\\\ZA-042_001', 'E:/dataset/Leisang/myTry/BleedingDataDCM\\\\train\\\\ZA-043_001', 'E:/dataset/Leisang/myTry/BleedingDataDCM\\\\train\\\\ZA-044_001', 'E:/dataset/Leisang/myTry/BleedingDataDCM\\\\train\\\\ZA-045_001', 'E:/dataset/Leisang/myTry/BleedingDataDCM\\\\train\\\\ZA-046_001', 'E:/dataset/Leisang/myTry/BleedingDataDCM\\\\train\\\\ZA-047_001', 'E:/dataset/Leisang/myTry/BleedingDataDCM\\\\train\\\\ZA-048_001']\n",
      "val_roots ['E:/dataset/Leisang/myTry/BleedingDataDCM\\\\val\\\\ZA-049_001', 'E:/dataset/Leisang/myTry/BleedingDataDCM\\\\val\\\\ZA-051_001', 'E:/dataset/Leisang/myTry/BleedingDataDCM\\\\val\\\\ZA-053_001']\n",
      "# of train roots: 30\n",
      "# of val roots: 3\n",
      "one roort len of dicoms: 1251\n",
      "one roort len of dicoms: 1182\n",
      "one roort len of dicoms: 665\n",
      "one roort len of dicoms: 1132\n",
      "one roort len of dicoms: 1226\n",
      "one roort len of dicoms: 1176\n",
      "one roort len of dicoms: 1251\n",
      "one roort len of dicoms: 1219\n",
      "one roort len of dicoms: 1251\n",
      "one roort len of dicoms: 869\n",
      "one roort len of dicoms: 1113\n",
      "one roort len of dicoms: 1232\n",
      "one roort len of dicoms: 1251\n",
      "one roort len of dicoms: 1169\n",
      "one roort len of dicoms: 1013\n",
      "one roort len of dicoms: 1076\n",
      "one roort len of dicoms: 1182\n",
      "one roort len of dicoms: 1751\n",
      "one roort len of dicoms: 1213\n",
      "one roort len of dicoms: 1013\n",
      "one roort len of dicoms: 1344\n",
      "one roort len of dicoms: 1369\n",
      "one roort len of dicoms: 1326\n",
      "one roort len of dicoms: 1182\n",
      "one roort len of dicoms: 1663\n",
      "one roort len of dicoms: 1669\n",
      "one roort len of dicoms: 1251\n",
      "one roort len of dicoms: 1251\n",
      "one roort len of dicoms: 1251\n",
      "one roort len of dicoms: 1082\n",
      "one roort len of dicoms: 1126\n",
      "one roort len of dicoms: 1219\n",
      "one roort len of dicoms: 2113\n",
      "# of train dicoms: 36623\n",
      "\n",
      "train_dicom_paths[0] E:/dataset/Leisang/myTry/BleedingDataDCM\\train\\ZA-006_000\\00000001.DCM\n",
      "val_dicom_paths[0] E:/dataset/Leisang/myTry/BleedingDataDCM\\val\\ZA-049_001\\00000001.DCM\n",
      "\n",
      "# of val dicoms: 4458\n",
      "batch size: 2\n",
      "total_num_batches per epoch: 18312\n",
      "input shape: [256, 256]\n"
     ]
    }
   ],
   "source": [
    "BUFFER_SIZE = 400\n",
    "BATCH_SIZE = 2\n",
    "IMG_WIDTH = 256\n",
    "IMG_HEIGHT = 256\n",
    "\n",
    "# project_name\n",
    "project_name = \"Bleed_AE_L1_CE_Sigmoid_NormNarrow_NoShuffle/\"\n",
    "if not os.path.exists(project_name):\n",
    "    os.makedirs(project_name)\n",
    "# tb_log_name \n",
    "log_dir=project_name + \"AE_logs/\"\n",
    "\n",
    "# image_save name\n",
    "save_figure_path =project_name + \"AE_saves\"\n",
    "\n",
    "# training_checkpoint name\n",
    "checkpoint_dir = project_name + \"training_checkpoints\"\n",
    "\n",
    "# model architecture name\n",
    "encoder_path = project_name + \"Encoder.png\"\n",
    "decoder_path = project_name +\"Decoder.png\"\n",
    "autoencoder_path = project_name + \"Autoencoder.png\"\n",
    "\n",
    "# gif save name\n",
    "anim_file = project_name + 'AE_saves.gif'\n",
    "\n",
    "# dicom root \n",
    "# dicom_root = 'E:/dataset/Leisang/myTry/BleedingDataDCM'\n",
    "# for lab ubuntu:\n",
    "dicom_root = '/media/ytx/Japan_Deep_Data/dataset/LeiSang/myTry/BleedingDataDCM'\n",
    "# # for home ubuntu:\n",
    "# dicom_root = '/media/ytx/DL/dataset/Leisang/myTry/BleedingDataDCM'\n",
    "train_path  = os.path.join(dicom_root, \"train\")\n",
    "val_path =  os.path.join(dicom_root, \"val\")\n",
    "print(\"dicom_root\",dicom_root)\n",
    "print(\"train_path\",train_path)\n",
    "print(\"val_path\",val_path)\n",
    "\n",
    "train_roots = sorted(glob(train_path + \"/*\"))\n",
    "val_roots =  sorted(glob(val_path + \"/*\"))\n",
    "print(\"train_roots\", train_roots)\n",
    "print(\"val_roots\", val_roots)\n",
    "\n",
    "print(\"# of train roots:\", len(train_roots))\n",
    "print(\"# of val roots:\", len(val_roots))\n",
    "\n",
    "train_dicom_paths = []\n",
    "val_dicom_paths = []\n",
    "for each_train in train_roots:\n",
    "    temp_dicoms =  sorted(glob(each_train +'/*.DCM'))\n",
    "    print(\"one roort len of dicoms:\", len(temp_dicoms))\n",
    "    train_dicom_paths = train_dicom_paths+ temp_dicoms\n",
    "#     train_dicom_paths.append(sorted(glob(each + '/*.DCM')))\n",
    "\n",
    "for each_val in val_roots:\n",
    "    temp_dicoms =  sorted(glob(each_val +'/*.DCM'))\n",
    "    print(\"one roort len of dicoms:\", len(temp_dicoms))\n",
    "    val_dicom_paths = val_dicom_paths+ temp_dicoms\n",
    "#     train_dicom_paths.append(sorted(glob(each + '/*.DCM')))\n",
    "print(\"# of train dicoms:\", len(train_dicom_paths))\n",
    "print()\n",
    "print(\"train_dicom_paths[0]\", train_dicom_paths[0])\n",
    "print(\"val_dicom_paths[0]\", val_dicom_paths[0])\n",
    "print()\n",
    "print(\"# of val dicoms:\", len(val_dicom_paths))    \n",
    "\n",
    "total_num_batches_per_epoch = math.ceil(len(train_dicom_paths) / BATCH_SIZE)\n",
    "\n",
    "total_num_batches_per_val = math.ceil(len(val_dicom_paths) / BATCH_SIZE)\n",
    "print(\"batch size:\", BATCH_SIZE)\n",
    "print(\"total_num_batches per epoch:\", total_num_batches_per_epoch)\n",
    "print(\"input shape:\", [IMG_HEIGHT, IMG_WIDTH])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FUNCTIONS FOR LOADING INPUTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function()\n",
    "def load(image_file):\n",
    "    image_bytes = tf.io.read_file(image_file)\n",
    "    image = tfio.image.decode_dicom_image(image_bytes, dtype=tf.uint16)  \n",
    "    # tf.uint16: 16-bit unsigned integer  [0, 65535]\n",
    "    print(image_bytes)\n",
    "    print(\"load image:\", image.shape)\n",
    "#     w = tf.shape(image)[1]\n",
    "\n",
    "#     input_image = tf.cast(image, tf.int16)\n",
    "\n",
    "    input_image = tf.cast(image, tf.float32)\n",
    "    target_image = input_image\n",
    "\n",
    "\n",
    "    return input_image, target_image, image_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # check the load fucntion\n",
    "# single_dicom_path =  train_dicom_paths[1]\n",
    "# print(\"single_dicom_path:\", single_dicom_path)\n",
    "# # input, target = load(single_dicom_path)\n",
    "# input, target, image_path = load('E:\\\\dataset\\\\Leisang\\\\myTry\\\\BleedingDataDCM\\\\train\\\\ZA-006_000\\\\00001083.DCM')\n",
    "# # for home ubuntu system\n",
    "# input, target, image_path = load('/media/ytx/DL/dataset/Leisang/myTry/BleedingDataDCM/train/ZA-006_000/00000001.DCM')\n",
    "\n",
    "# # for lab ubuntu system\n",
    "input, target, image_path= load('/media/ytx/Japan_Deep_Data/dataset/LeiSang/myTry/BleedingDataDCM/train/ZA-006_000/00000001.DCM')\n",
    "fig, axes = plt.subplots(1,2, figsize=(10,10))\n",
    "axes[0].imshow(np.squeeze(input.numpy()), cmap='gray')\n",
    "axes[0].set_title('input range:[{}, {}]'.format((input.numpy().min()), np.max(input.numpy())))\n",
    "axes[1].imshow(np.squeeze(target.numpy()), cmap='gray')\n",
    "axes[1].set_title('target range:[{}, {}]'.format(np.min(target), np.max(target)))\n",
    "print(image_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PREPROCESSING\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize(input, target):\n",
    "    resized_input = tf.image.resize(input, [IMG_HEIGHT, IMG_WIDTH],\n",
    "                                method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "    resized_target = tf.image.resize(target, [IMG_HEIGHT, IMG_WIDTH],\n",
    "                                method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "    resized_input =  tf.reshape(resized_input, [IMG_HEIGHT, IMG_WIDTH, 1])\n",
    "    resized_target =  tf.reshape(resized_target, [IMG_HEIGHT, IMG_WIDTH, 1])\n",
    "    return  resized_input, resized_target\n",
    "\n",
    "# @tf.function()\n",
    "def truncate(x, min, max):\n",
    "#     print(x.shape)\n",
    "    cliped =  tf.clip_by_value(x, min, max)\n",
    "    return cliped\n",
    " \n",
    "def norm(x, min, max):\n",
    "    # normalize_value = (value − min_value) / (max_value − min_value)\n",
    "    tensor = tf.math.divide(tf.subtract(x, min),\n",
    "                    tf.subtract(max, min))\n",
    "    return tensor\n",
    "\n",
    "def linear_normalization(input, target, min=30720.0, max=34816.0):\n",
    "    truncated_input = truncate(input, min, max)\n",
    "    truncated_target = truncate(target, min, max)\n",
    "    norm_input = norm(truncated_input, min, max )\n",
    "    norm_target = norm(truncated_target, min, max)\n",
    "    \n",
    "    return  norm_input, norm_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check preprocessing\n",
    "print(\"range of input: [{}, {}]\".format(np.min(input), np.max(input)))\n",
    "print(\"range of target: [{}, {}]\".format(np.min(target), np.max(target)))\n",
    "fig2, axes2 = plt.subplots(1,2, figsize=(10,10))\n",
    "resized_input, resized_target = resize(input, target)\n",
    "print(\"range of resized_input: [{}, {}]\".format(np.min(resized_input), np.max(resized_input)))\n",
    "print(\"range of resized_target: [{}, {}]\".format(np.min(resized_target), np.max(resized_target)))\n",
    "\n",
    "# # clipped\n",
    "cliped_input = truncate(input, min=30720.0, max=34816.0)\n",
    "cliped_target = truncate(target, min=30720.0, max=34816.0)\n",
    "print(\"range of cliped_input: [{}, {}]\".format(np.min(cliped_input), np.max(cliped_input)))\n",
    "print(\"range of cliped_target: [{}, {}]\".format(np.min(cliped_target), np.max(cliped_target)))\n",
    "\n",
    "\n",
    "cliped_norm_input, cliped_norm_target =  linear_normalization(resized_input, resized_target)\n",
    "axes2[0].imshow(np.squeeze(resized_input.numpy()), cmap='gray')\n",
    "axes2[0].set_title(\"resized input before cliped range\")\n",
    "axes2[1].imshow(np.squeeze(resized_target.numpy()), cmap='gray')\n",
    "axes2[1].set_title(\"resized target before cliped range\")\n",
    "print(\"resized input shape:\", resized_input.shape)\n",
    "print(\"range of cliped_norm_input: [{}, {}]\".format(np.min(cliped_norm_input), np.max(cliped_norm_input)))\n",
    "print(\"range of cliped_norm_target: [{}, {}]\".format(np.min(cliped_norm_target), np.max(cliped_norm_target)))\n",
    "# print(truncated_input.shape)\n",
    "fig3, axes3 = plt.subplots(1,2, figsize=(10,10))\n",
    "axes3[0].imshow(np.squeeze(cliped_norm_input.numpy()), cmap='gray')\n",
    "axes3[0].set_title(\"cliped_norm_input\")\n",
    "axes3[1].imshow(np.squeeze(cliped_norm_target.numpy()), cmap='gray')\n",
    "axes3[1].set_title(\"cliped_norm_target\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DEFINE LOAD FUNCTION IN INPUT PIPELINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image_train(image_file):\n",
    "    input, target, image_path= load(image_file)\n",
    "    input, target = resize(input, target)\n",
    "    input, target = linear_normalization(input, target)\n",
    "\n",
    "    return input, target, image_path\n",
    "\n",
    "def load_image_val(image_file):\n",
    "    input, target, image_path = load(image_file)\n",
    "    input, target = resize(input, target)\n",
    "    input, target = linear_normalization(input, target)\n",
    "\n",
    "    return input, target, image_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## INPUT PIPELINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices(train_dicom_paths)  # this can diretly get a list of kind of files\n",
    "train_dataset = train_dataset.map(load_image_train,\n",
    "                                  num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "# train_dataset = train_dataset.shuffle(BUFFER_SIZE)\n",
    "train_dataset = train_dataset.batch(BATCH_SIZE)\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices(val_dicom_paths)\n",
    "test_dataset = test_dataset.map(load_image_val)\n",
    "test_dataset = test_dataset.batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  check the dataset\n",
    "for one_batch_input, one_batch_target, batch_paths  in test_dataset.take(1):\n",
    "    print(one_batch_input.shape)\n",
    "    print(\"batch_path[0]:\", batch_paths[0])\n",
    "    fig4, axes4 = plt.subplots(1,2, figsize=(10,10))\n",
    "    axes4[0].imshow(np.squeeze(one_batch_input[0].numpy()), cmap='gray')\n",
    "    axes4[0].set_title(\"cliped_norm_input\")\n",
    "    axes4[1].imshow(np.squeeze(one_batch_target[0].numpy()), cmap='gray')\n",
    "    axes4[1].set_title(\"cliped_norm_target\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DESIGN MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_CHANNELS = 1\n",
    "latent_dim =50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model input conponents\n",
    "en_inputs = tf.keras.layers.Input(shape=[IMG_HEIGHT,IMG_HEIGHT,1])\n",
    "de_inputs = tf.keras.layers.Input(shape=[latent_dim])\n",
    "# full model design and construct encoder, decoder , AE object\n",
    "# entire model\n",
    "\n",
    "# Define encoder part ---->\n",
    "x = en_inputs\n",
    "x = tf.keras.layers.Conv2D(\n",
    "                filters=16, kernel_size=3, strides=(1, 1), activation='relu', padding=\"same\")(x)\n",
    "x = tf.keras.layers.Conv2D(\n",
    "                filters=32, kernel_size=3, strides=(2, 2), activation='relu', padding=\"same\")(x)\n",
    "x = tf.keras.layers.Conv2D(\n",
    "                filters=64, kernel_size=3, strides=(2, 2), activation='relu', padding=\"SAME\")(x)\n",
    "x = tf.keras.layers.Conv2D(\n",
    "                filters=128, kernel_size=3, strides=(2, 2), activation='relu',padding=\"SAME\")(x)\n",
    "x = tf.keras.layers.Flatten()(x)\n",
    "# No activation\n",
    "latent_v = tf.keras.layers.Dense(latent_dim)(x)\n",
    "encoder = tf.keras.Model(inputs=en_inputs, outputs=latent_v, name='encoder')\n",
    "\n",
    "\n",
    "# Define decoder part ---->\n",
    "x = tf.keras.layers.Dense(units=131072, activation=tf.nn.relu)(de_inputs)\n",
    "x = tf.keras.layers.Reshape(target_shape=(32, 32, 128))(x)\n",
    "x = tf.keras.layers.Conv2DTranspose(\n",
    "      filters=64,\n",
    "      kernel_size=3,\n",
    "      strides=(2, 2),\n",
    "      padding=\"SAME\",\n",
    "      activation='relu')(x)\n",
    "x = tf.keras.layers.Conv2DTranspose(\n",
    "      filters=32,\n",
    "      kernel_size=3,\n",
    "      strides=(2, 2),\n",
    "      padding=\"SAME\",\n",
    "      activation='relu')(x)\n",
    "x = tf.keras.layers.Conv2DTranspose(\n",
    "      filters=16,\n",
    "      kernel_size=3,\n",
    "      strides=(2, 2),\n",
    "      padding=\"SAME\",\n",
    "      activation='relu')(x)\n",
    "\n",
    "decoded = tf.keras.layers.Conv2DTranspose(\n",
    "      filters=1, kernel_size=3, strides=(1, 1), padding=\"SAME\", use_bias=True)(x)\n",
    "# output with sigmoid\n",
    "decoded =  tf.sigmoid(decoded)\n",
    "decoder = tf.keras.Model(inputs=de_inputs, outputs=decoded, name='decoder')\n",
    "\n",
    "\n",
    "# Define AE model---->\n",
    "outputs = decoder(latent_v)\n",
    "autoencoder =  tf.keras.Model(inputs=en_inputs, outputs=outputs, name='AE')\n",
    "\n",
    "# a sperately decoder is struggle leave for the moment\n",
    "\n",
    "# # # create a placeholder for an encoded (32-dimensional) input\n",
    "# encoded_input = tf.keras.layers.Input(shape=(latent_dim))\n",
    "# # # retrieve the last layer of the autoencoder model\n",
    "# decoder_layer = autoencoder.layers[-1]\n",
    "# # # create the decoder model\n",
    "# decoder = tf.keras.Model(encoded_input, decoder_layer)\n",
    "             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.utils.plot_model(encoder, to_file=encoder_path, show_shapes=True, dpi=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.utils.plot_model(decoder, to_file=decoder_path, show_shapes=True, dpi=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.utils.plot_model(autoencoder, to_file=autoencoder_path, show_shapes=True, dpi=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OPTIMIZER AND OBJECTIVE LOSSES\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define opitmizer \n",
    "optimizer =  tf.keras.optimizers.Adam(1e-4)\n",
    "@tf.function()\n",
    "# define losses\n",
    "def compute_loss(decoded_x, x):\n",
    "    \n",
    "    # cross_entropy,  use reduce mean not sum, otherwise loss will be very big\n",
    "    CE_loss =  tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=decoded_x, labels=x))\n",
    "    \n",
    "    \n",
    "    # L1 loss \n",
    "    L1_loss = tf.reduce_mean(tf.abs(x - decoded_x))\n",
    "    \n",
    "    total_loss = CE_loss + L1_loss\n",
    "    return total_loss, CE_loss, L1_loss \n",
    "    \n",
    "# appliy graidients  this is acutaully trianing step\n",
    "# @tf.function()\n",
    "# def compute_apply_gradients(model, x, optimizer, epoch):\n",
    "#     decoded_x =  model(x)\n",
    "#     with tf.GradientTape() as tape:\n",
    "#         total_loss, CE_loss, L1_loss = compute_loss(decoded_x, x)\n",
    "#     gradients = tape.gradient(total_loss, model.trainable_variables)\n",
    "#     optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    \n",
    "#     with summary_writer.as_default():\n",
    "#         # write scalars to the tensorboard after each train step\n",
    "#         tf.summary.scalar('total_loss', total_loss, step=epoch)\n",
    "#         tf.summary.scalar('CE_loss', CE_loss, step=epoch)\n",
    "#         tf.summary.scalar('L1_loss', L1_loss, step=epoch) \n",
    "@tf.function()\n",
    "def train_step(model, input, target, epoch):\n",
    "    print(\"in trianing step\")\n",
    "    with tf.GradientTape() as tape:  # very interesting\n",
    "        decoded_img = model(input, training=True)\n",
    "        total_loss, CE_loss, L1_loss = compute_loss(decoded_img, target)\n",
    "    gradients = tape.gradient(total_loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    return total_loss, CE_loss, L1_loss\n",
    "\n",
    "@tf.function()\n",
    "def test_step(model, input, target, epoch):\n",
    "    with tf.GradientTape() as tape:  # very interesting\n",
    "        decoded_img = model(input, training=True)\n",
    "        total_loss, CE_loss, L1_loss = compute_loss(decoded_img, target)\n",
    "    return total_loss, CE_loss, L1_loss\n",
    "#     train_avg_loss(train_loss)\n",
    "#     train_avg_metric(metric)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the AE OUTPUT\n",
    "print(\"inp.shape:\", cliped_norm_input.shape)\n",
    "print(\"inp[tf.newaxis,...]:\", cliped_norm_input[tf.newaxis,...].shape)\n",
    "AE_output = autoencoder(resized_input[tf.newaxis,...], training=False)  # inp is the image sample from cell code 6 ; \n",
    "print(AE_output.shape)\n",
    "plt.imshow(np.squeeze(AE_output[0,...]), cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training PREPARING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# define the epoch image check\n",
    "def generate_images(model, test_input, tar, path, total_loss, batch_idx, epoch):\n",
    "    prediction = model(test_input, training=True)\n",
    "\n",
    "   \n",
    "\n",
    "    display_list = [np.squeeze(test_input[0]), np.squeeze(tar[0]), np.squeeze(prediction[0])]\n",
    "    title = ['Input range:[{},{}]'.format(test_input[0].numpy().min(), test_input[0].numpy().max()), \n",
    "           'GT range:[{},{}]'.format(tar[0].numpy().min(), tar[0].numpy().max()), 'Pred. [B:{}/E:{}]: loss->'.format(batch_idx,epoch)+ str(total_loss)]\n",
    "    \n",
    "    fig5 = plt.figure(figsize=(15,5))\n",
    "    fig5.suptitle(str(path[0].numpy(), 'utf-8')) # decode convert byte b'' to normal ''\n",
    "  \n",
    "    for i in range(3):\n",
    "        ax = fig5.add_subplot(1,3, i+1,title=title[i])\n",
    "        # getting the pixel values between [0, 1] to plot it.\n",
    "        ax.imshow(display_list[i], cmap=\"gray\") # prediction in range[-1, 1]*0.5 = [-0.5, 0.5],+ 0.5=[0, 1]\n",
    "        ax.axis('off')\n",
    " \n",
    "    fig5.savefig(save_figure_path + \"/image_at_epoch_{:04d}_batch_{}.png\".format(epoch,batch_idx+1))\n",
    "    \n",
    "    plt.figure(figsize=(8,8))\n",
    "    plt.title('Pred. [B:{}/E:{}]: loss->'.format(batch_idx,epoch)+ str(total_loss))\n",
    "    plt.imshow(np.squeeze(prediction[0]), cmap=\"gray\") # prediction in range[-1, 1]*0.5 = [-0.5, 0.5],+ 0.5=[0, 1]\n",
    "    plt.axis('off')\n",
    "    plt.savefig(save_figure_path + \"/Predictions/pred_only_at_epoch_{:04d}_batch_{}.png\".format(epoch,batch_idx+1))\n",
    "    plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "epochs = 100\n",
    "\n",
    "## get current working directory\n",
    "cwd = os.getcwd()\n",
    "print(\"current working directory:\", cwd)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "full_AE_saves =  os.path.join(cwd, save_figure_path)\n",
    "print(full_AE_saves)\n",
    "if not os.path.exists(full_AE_saves):\n",
    "    os.makedirs(full_AE_saves)\n",
    "predictions_save_path =os.path.join(full_AE_saves, \"Predictions\")\n",
    "\n",
    "if not os.path.exists(predictions_save_path):\n",
    "    os.makedirs(predictions_save_path)\n",
    "\n",
    "# define check points\n",
    "\n",
    "\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "if not os.path.exists(checkpoint_prefix):\n",
    "    os.makedirs(checkpoint_prefix)\n",
    "#  contents of states to be saved as attributes on the checkpoint object\n",
    "checkpoint_ob = tf.train.Checkpoint(step= tf.Variable(1),\n",
    "                                    epoch=  tf.Variable(1),\n",
    "                                    optimizer=optimizer,\n",
    "                                 encoder=encoder,\n",
    "                                 decoder =  decoder,\n",
    "                                 autoencoder = autoencoder\n",
    "                                 )\n",
    "# define checkpoint manager\n",
    "manager =  tf.train.CheckpointManager(checkpoint_ob, checkpoint_prefix, max_to_keep=3)\n",
    "\n",
    "\n",
    "datetime_rec =  datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "train_summary_writer = tf.summary.create_file_writer(\n",
    "  log_dir + \"train\")\n",
    "val_summary_writer = tf.summary.create_file_writer(\n",
    "  log_dir + \"val\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start tensorboard\n",
    "# !kill 5032\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir {log_dir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# check the whether there is a checkpoint in the checkpoint folder, if it is restore from it\n",
    "if manager.latest_checkpoint:\n",
    "    print(\"Restored from {}\".format(manager.latest_checkpoint))\n",
    "    checkpoint_ob.restore(manager.latest_checkpoint)\n",
    "else:\n",
    "    print(\"Initializing from scratch.\")\n",
    "    \n",
    "for epoch in range(1, epochs+1):\n",
    "    train_total_loss_mean = tf.keras.metrics.Mean()\n",
    "    train_CE_loss_mean = tf.keras.metrics.Mean()\n",
    "    train_L1_loss_mean = tf.keras.metrics.Mean()\n",
    "    \n",
    "    # reset checkcpoint.step for each epoch\n",
    "    step =  checkpoint_ob.step\n",
    "    step.assign(1)\n",
    "    ckpt_epoch =  checkpoint_ob.epoch\n",
    "    # initial saving of checkpoints\n",
    "    if epoch % 1 == 0:\n",
    "        save_path =  manager.save()\n",
    "        print(\"Initial saving checkpoints at epoch {} batch {} at path {}\".format(int(ckpt_epoch),  int(step) ,save_path))\n",
    "       \n",
    "        \n",
    "    \n",
    "    start_time =  time.time()\n",
    "    for train_x, target, _ in train_dataset:\n",
    "#         print(train_x.shape)\n",
    "#         print(target.shape)\n",
    "#         print(autoencoder)\n",
    "        # one training step\n",
    "        total_loss, CE_loss, L1_loss = train_step(autoencoder, train_x, target, epoch)\n",
    "        end_time =  time.time()\n",
    "        \n",
    "        # calculate mean value for trainning losses\n",
    "        train_total_loss_mean(total_loss)\n",
    "        train_CE_loss_mean(CE_loss)   \n",
    "        train_L1_loss_mean(L1_loss) \n",
    "       \n",
    "        #\n",
    "        if int(step) == 1:\n",
    "            for example_input, example_target, example_path in test_dataset.take(1):\n",
    "                generate_images(autoencoder, example_input, example_target, example_path,total_loss.numpy(), int(step), epoch)\n",
    "        \n",
    "        # every 100 batch step show the generate test image to check.\n",
    "        if int(step) % 100 == 0:\n",
    "            display.clear_output(wait=True)   \n",
    "            for example_input, example_target, example_path in test_dataset.take(1):\n",
    "                generate_images(autoencoder, example_input, example_target,example_path,total_loss.numpy(), int(step), epoch)\n",
    "                # every 400 steps save the genrate images\n",
    "          \n",
    "            print('Epoch{}: {}/{}: total_loss: {} ; CE_loss:{}; L1_loss:{}  '\n",
    "          'time elapse {}'.format(int(ckpt_epoch), int(step), total_num_batches_per_epoch, total_loss, CE_loss, L1_loss, end_time-start_time))\n",
    "                       \n",
    "             \n",
    "        \n",
    "          \n",
    "        \n",
    "        step.assign_add(1)\n",
    "        \n",
    "    # print validation step\n",
    "    if epoch % 1 == 0:\n",
    "        test_total_loss_mean = tf.keras.metrics.Mean()\n",
    "        test_CE_loss_mean = tf.keras.metrics.Mean()\n",
    "        test_L1_loss_mean = tf.keras.metrics.Mean()\n",
    "        for (test_x, test_target, example_path) in test_dataset:\n",
    "            test_total_loss, test_CE_loss, test_L1_loss = test_step(autoencoder, test_x, test_target, epoch)\n",
    "            test_total_loss_mean(test_total_loss)\n",
    "            test_CE_loss_mean(test_CE_loss)   \n",
    "            test_L1_loss_mean(test_L1_loss)                                                            \n",
    "      \n",
    "        \n",
    "        print('Epoch: {}, Test total_loss set loss: {},  time elapse for current epoch {}'.format(int(ckpt_epoch),\n",
    "                                                    test_total_loss_mean.result(),\n",
    "                                                    end_time - start_time))\n",
    "        \n",
    "        with train_summary_writer.as_default():\n",
    "            print(\"writing train logs to tensorboard...\")                                                       \n",
    "            # write scalars to the tensorboard after each train step\n",
    "            tf.summary.scalar('total_loss', train_total_loss_mean.result(), step=epoch)\n",
    "            tf.summary.scalar('CE_loss', train_CE_loss_mean.result(), step=epoch)\n",
    "            tf.summary.scalar('L1_loss', train_L1_loss_mean.result(), step=epoch) \n",
    "        \n",
    "        with val_summary_writer.as_default():\n",
    "            print(\"writing val logs to tensorboard...\")                                                        \n",
    "            # write scalars to the tensorboard after each train step\n",
    "            tf.summary.scalar('total_loss', test_total_loss_mean.result(), step=epoch)\n",
    "            tf.summary.scalar('CE_loss', test_CE_loss_mean.result(), step=epoch)\n",
    "            tf.summary.scalar('L1_loss', test_L1_loss_mean.result() , step=epoch)\n",
    "        \n",
    "        save_path =  manager.save() # save the checkpoint and return the save path\n",
    "        print(\"Saved checkpoint for epoch {}-  step {}: {}\".format(int(ckpt_epoch), int(step), save_path))\n",
    "        \n",
    "    ckpt_epoch.assign_add(1)     \n",
    "    \n",
    "#     # saving (checkpoint) the model every 5 epochs\n",
    "#     if epoch % 5 == 0:\n",
    "#         print(\"saving checkpoints at epoch {}\".format(epoch))\n",
    "#         checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "\n",
    "print(\"training finished\")   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate a GIF of all the saved images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"found save figures #: \",len(glob(full_AE_saves+ '/*.png')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "with imageio.get_writer(anim_file, mode='I') as writer:\n",
    "  filenames = glob(full_AE_saves+ '/*.png')\n",
    "  filenames = sorted(filenames)\n",
    "  last = -1\n",
    "  for i,filename in enumerate(filenames):\n",
    "    frame = 2*(i**0.5)\n",
    "    if round(frame) > round(last):\n",
    "      last = frame\n",
    "    else:\n",
    "      continue\n",
    "    image = imageio.imread(filename)\n",
    "    writer.append_data(image)\n",
    "  image = imageio.imread(filename)\n",
    "  writer.append_data(image)\n",
    "\n",
    "import IPython\n",
    "if IPython.version_info >= (6,2,0,''):\n",
    "  display.Image(filename=anim_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}