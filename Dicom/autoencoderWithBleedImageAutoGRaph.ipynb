{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# auto graph program  with bleeding train for auto encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D, Reshape, Dropout, UpSampling2D\n",
    "from tensorflow.keras.callbacks import LambdaCallback\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "from glob import glob\n",
    "import math\n",
    "from tensorflow import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_images_root:  ['/media/ytx/Japan_Deep_Data/dataset/LeiSang/myTry/BleedingDataDCM/train/ZA-006_000', '/media/ytx/Japan_Deep_Data/dataset/LeiSang/myTry/BleedingDataDCM/train/ZA-008_000', '/media/ytx/Japan_Deep_Data/dataset/LeiSang/myTry/BleedingDataDCM/train/ZA-012_000', '/media/ytx/Japan_Deep_Data/dataset/LeiSang/myTry/BleedingDataDCM/train/ZA-015_001', '/media/ytx/Japan_Deep_Data/dataset/LeiSang/myTry/BleedingDataDCM/train/ZA-017_001', '/media/ytx/Japan_Deep_Data/dataset/LeiSang/myTry/BleedingDataDCM/train/ZA-019_001', '/media/ytx/Japan_Deep_Data/dataset/LeiSang/myTry/BleedingDataDCM/train/ZA-020_002', '/media/ytx/Japan_Deep_Data/dataset/LeiSang/myTry/BleedingDataDCM/train/ZA-021_001', '/media/ytx/Japan_Deep_Data/dataset/LeiSang/myTry/BleedingDataDCM/train/ZA-022_001', '/media/ytx/Japan_Deep_Data/dataset/LeiSang/myTry/BleedingDataDCM/train/ZA-023_001', '/media/ytx/Japan_Deep_Data/dataset/LeiSang/myTry/BleedingDataDCM/train/ZA-024_001', '/media/ytx/Japan_Deep_Data/dataset/LeiSang/myTry/BleedingDataDCM/train/ZA-027_001', '/media/ytx/Japan_Deep_Data/dataset/LeiSang/myTry/BleedingDataDCM/train/ZA-029_001', '/media/ytx/Japan_Deep_Data/dataset/LeiSang/myTry/BleedingDataDCM/train/ZA-031_001', '/media/ytx/Japan_Deep_Data/dataset/LeiSang/myTry/BleedingDataDCM/train/ZA-032_001', '/media/ytx/Japan_Deep_Data/dataset/LeiSang/myTry/BleedingDataDCM/train/ZA-033_001', '/media/ytx/Japan_Deep_Data/dataset/LeiSang/myTry/BleedingDataDCM/train/ZA-035_001', '/media/ytx/Japan_Deep_Data/dataset/LeiSang/myTry/BleedingDataDCM/train/ZA-036_001', '/media/ytx/Japan_Deep_Data/dataset/LeiSang/myTry/BleedingDataDCM/train/ZA-037_001', '/media/ytx/Japan_Deep_Data/dataset/LeiSang/myTry/BleedingDataDCM/train/ZA-038_001']\n",
      "val_images_root:  ['/media/ytx/Japan_Deep_Data/dataset/LeiSang/myTry/BleedingDataDCM/val/ZA-039_001', '/media/ytx/Japan_Deep_Data/dataset/LeiSang/myTry/BleedingDataDCM/val/ZA-040_001', '/media/ytx/Japan_Deep_Data/dataset/LeiSang/myTry/BleedingDataDCM/val/ZA-041_001']\n",
      "train paths--------------------------------------------------------------->\n",
      "root: /media/ytx/Japan_Deep_Data/dataset/LeiSang/myTry/BleedingDataDCM/train/ZA-006_000\n",
      "one_root_paths: 1251\n",
      "\n",
      "train_all_image_paths: 1251\n",
      "root: /media/ytx/Japan_Deep_Data/dataset/LeiSang/myTry/BleedingDataDCM/train/ZA-008_000\n",
      "one_root_paths: 1182\n",
      "\n",
      "train_all_image_paths: 2433\n",
      "root: /media/ytx/Japan_Deep_Data/dataset/LeiSang/myTry/BleedingDataDCM/train/ZA-012_000\n",
      "one_root_paths: 665\n",
      "\n",
      "train_all_image_paths: 3098\n",
      "root: /media/ytx/Japan_Deep_Data/dataset/LeiSang/myTry/BleedingDataDCM/train/ZA-015_001\n",
      "one_root_paths: 1132\n",
      "\n",
      "train_all_image_paths: 4230\n",
      "root: /media/ytx/Japan_Deep_Data/dataset/LeiSang/myTry/BleedingDataDCM/train/ZA-017_001\n",
      "one_root_paths: 1226\n",
      "\n",
      "train_all_image_paths: 5456\n",
      "root: /media/ytx/Japan_Deep_Data/dataset/LeiSang/myTry/BleedingDataDCM/train/ZA-019_001\n",
      "one_root_paths: 1176\n",
      "\n",
      "train_all_image_paths: 6632\n",
      "root: /media/ytx/Japan_Deep_Data/dataset/LeiSang/myTry/BleedingDataDCM/train/ZA-020_002\n",
      "one_root_paths: 1251\n",
      "\n",
      "train_all_image_paths: 7883\n",
      "root: /media/ytx/Japan_Deep_Data/dataset/LeiSang/myTry/BleedingDataDCM/train/ZA-021_001\n",
      "one_root_paths: 1219\n",
      "\n",
      "train_all_image_paths: 9102\n",
      "root: /media/ytx/Japan_Deep_Data/dataset/LeiSang/myTry/BleedingDataDCM/train/ZA-022_001\n",
      "one_root_paths: 1251\n",
      "\n",
      "train_all_image_paths: 10353\n",
      "root: /media/ytx/Japan_Deep_Data/dataset/LeiSang/myTry/BleedingDataDCM/train/ZA-023_001\n",
      "one_root_paths: 869\n",
      "\n",
      "train_all_image_paths: 11222\n",
      "root: /media/ytx/Japan_Deep_Data/dataset/LeiSang/myTry/BleedingDataDCM/train/ZA-024_001\n",
      "one_root_paths: 1113\n",
      "\n",
      "train_all_image_paths: 12335\n",
      "root: /media/ytx/Japan_Deep_Data/dataset/LeiSang/myTry/BleedingDataDCM/train/ZA-027_001\n",
      "one_root_paths: 1232\n",
      "\n",
      "train_all_image_paths: 13567\n",
      "root: /media/ytx/Japan_Deep_Data/dataset/LeiSang/myTry/BleedingDataDCM/train/ZA-029_001\n",
      "one_root_paths: 1251\n",
      "\n",
      "train_all_image_paths: 14818\n",
      "root: /media/ytx/Japan_Deep_Data/dataset/LeiSang/myTry/BleedingDataDCM/train/ZA-031_001\n",
      "one_root_paths: 1169\n",
      "\n",
      "train_all_image_paths: 15987\n",
      "root: /media/ytx/Japan_Deep_Data/dataset/LeiSang/myTry/BleedingDataDCM/train/ZA-032_001\n",
      "one_root_paths: 1013\n",
      "\n",
      "train_all_image_paths: 17000\n",
      "root: /media/ytx/Japan_Deep_Data/dataset/LeiSang/myTry/BleedingDataDCM/train/ZA-033_001\n",
      "one_root_paths: 1076\n",
      "\n",
      "train_all_image_paths: 18076\n",
      "root: /media/ytx/Japan_Deep_Data/dataset/LeiSang/myTry/BleedingDataDCM/train/ZA-035_001\n",
      "one_root_paths: 1182\n",
      "\n",
      "train_all_image_paths: 19258\n",
      "root: /media/ytx/Japan_Deep_Data/dataset/LeiSang/myTry/BleedingDataDCM/train/ZA-036_001\n",
      "one_root_paths: 1751\n",
      "\n",
      "train_all_image_paths: 21009\n",
      "root: /media/ytx/Japan_Deep_Data/dataset/LeiSang/myTry/BleedingDataDCM/train/ZA-037_001\n",
      "one_root_paths: 1213\n",
      "\n",
      "train_all_image_paths: 22222\n",
      "root: /media/ytx/Japan_Deep_Data/dataset/LeiSang/myTry/BleedingDataDCM/train/ZA-038_001\n",
      "one_root_paths: 1013\n",
      "\n",
      "train_all_image_paths: 23235\n",
      "val paths--------------------------------------------------------------->\n",
      "root: /media/ytx/Japan_Deep_Data/dataset/LeiSang/myTry/BleedingDataDCM/val/ZA-039_001\n",
      "one_root_paths: 1344\n",
      "\n",
      "root: /media/ytx/Japan_Deep_Data/dataset/LeiSang/myTry/BleedingDataDCM/val/ZA-040_001\n",
      "one_root_paths: 1369\n",
      "\n",
      "root: /media/ytx/Japan_Deep_Data/dataset/LeiSang/myTry/BleedingDataDCM/val/ZA-041_001\n",
      "one_root_paths: 17\n",
      "\n",
      "Found 23235 training images\n",
      "Found 2730 validation images\n"
     ]
    }
   ],
   "source": [
    "#  set dataset path\n",
    "train_images_root = sorted(glob('/media/ytx/Japan_Deep_Data/dataset/LeiSang/myTry/BleedingDataDCM/train/*'))\n",
    "# train_masks = sorted(glob('I:/dataset/infaredSublingualVein/train/tongue_labels/*'))\n",
    "\n",
    "val_images_root = sorted(glob('/media/ytx/Japan_Deep_Data/dataset/LeiSang/myTry/BleedingDataDCM/val/*'))\n",
    "# val_masks = sorted(glob('I:/dataset/infaredSublingualVein/validation/tongue_labels/*'))\n",
    "print(\"train_images_root: \", train_images_root)\n",
    "print(\"val_images_root: \", val_images_root)\n",
    "\n",
    "train_all_image_paths =  []\n",
    "val_all_image_paths = []\n",
    "\n",
    "print(\"train paths--------------------------------------------------------------->\")\n",
    "for each in train_images_root:\n",
    "    print(\"root:\", each)\n",
    "    one_root_paths =  sorted(glob(each +'/*.DCM'))\n",
    "    print(\"one_root_paths:\", len(one_root_paths))\n",
    "    print()\n",
    "    train_all_image_paths = train_all_image_paths+ one_root_paths\n",
    "    print(\"train_all_image_paths:\", len(train_all_image_paths))\n",
    "\n",
    "print(\"val paths--------------------------------------------------------------->\")\n",
    "for each in val_images_root:\n",
    "    print(\"root:\", each)\n",
    "    one_root_paths =  sorted(glob(each +'/*.DCM'))\n",
    "    print(\"one_root_paths:\", len(one_root_paths))\n",
    "    print()\n",
    "    val_all_image_paths = val_all_image_paths+ one_root_paths\n",
    "print(f'Found {len(train_all_image_paths)} training images')\n",
    "print(f'Found {len(val_all_image_paths)} validation images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch size: 2\n",
      "total_num_batches per epoch: 11618\n",
      "input image_size: 512\n"
     ]
    }
   ],
   "source": [
    "# hyperparameter\n",
    "encoding_size = 32\n",
    "batch_size =  2\n",
    "image_size = 512\n",
    "img_width = img_height = image_size \n",
    "ckp_log_root = \"ckpts\"\n",
    "EPOCHS =30\n",
    "total_num_batches_per_epoch = math.ceil(len(train_all_image_paths) / batch_size)\n",
    "print(\"batch size:\", batch_size)\n",
    "print(\"total_num_batches per epoch:\", total_num_batches_per_epoch)\n",
    "print(\"input image_size:\", image_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dcm \n",
    "# !pip install -q tensorflow-io-nightly\n",
    "\n",
    "import tensorflow_io as tfio\n",
    "import matplotlib.pyplot as plt\n",
    "def load_dcm(dcm_path, dcm=False):\n",
    "   \n",
    "    if dcm:\n",
    "        \n",
    "        print(\"read dcm data\")\n",
    "        _bytes = tf.io.read_file(dcm_path)\n",
    "        img = tfio.image.decode_dicom_image( _bytes)  # defualt on_error= \"strict\": throw an error if can not throw one except; scale =  perserve defulat means keeps the value they are\n",
    "        print(img.shape)  # (1, 512, 512) after decode image shape is this, WHICH CAN BE SEEN FROM PLTDicom.ipynb\n",
    "        \n",
    "        img =  tf.squeeze(img) # squeez the dimension to (512, 512)\n",
    "        img =  tf.expand_dims(img, axis =-1) # expand the last dimension to 1, as (512, 512, 1)\n",
    "        img.set_shape([None, None, 1])  # need to set the shape because the shape will becomes unknown with preprocessing function load\n",
    "    else:\n",
    "        \n",
    "       raise \"please choose dcm as input formart\"\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing\n",
    "def resize(image):\n",
    "    print(image.shape)\n",
    "    resized_image = tf.image.resize(image, size=[image_size, image_size], method='bilinear')\n",
    "    return resized_image\n",
    "\n",
    "def std_norm(image):\n",
    "    image = tf.image.per_image_standardization(image)\n",
    "    return image\n",
    "\n",
    "\n",
    "def random_flip_auto(image):\n",
    "    flip = tf.random.uniform(\n",
    "        shape=[1, ], minval=0, maxval=2, dtype=tf.int32)[0]\n",
    "    image = tf.case([\n",
    "        (tf.greater(flip, 0), lambda: tf.image.flip_left_right(image))\n",
    "    ], default=lambda: image)\n",
    "    return image\n",
    "\n",
    "@tf.function()\n",
    "def train_preprocess_inputs_auto(image_path):\n",
    "    print(image_path)\n",
    "    with tf.device('/cpu:0'):\n",
    "      \n",
    "        # image = load_image(image_path) # infraed image input. there for 8 bit input\n",
    "        image = tf.cast(load_dcm(image_path, dcm=True), tf.float32)  # infraed image input. there for 8 bit input\n",
    "        \n",
    "        print(\"load image shape:\", image.shape)\n",
    "#         mask = load_image(mask_path, mask=True)\n",
    "#         mask = tf.cast(mask > 0, dtype=tf.float32)\n",
    "        print(image)\n",
    "#         image = resize(image)\n",
    "        # image, mask = random_scale(image, mask) # random resize\n",
    "        image = std_norm(image)  # norm before padding and crop_pad\n",
    "        # image, mask = pad_inputs(image, mask)  # and pad to raw size\n",
    "        # image, mask = random_crop(image, mask)  #\n",
    "        image = random_flip_auto(image)\n",
    "        print(\"prepro image shape:\", image.shape)\n",
    "        return image, image\n",
    "\n",
    "#         image = resize(image)import tensorflow_io as tfio\n",
    "@tf.function()\n",
    "def val_preprocess_inputs_auto(image_path):\n",
    "    print(image_path)\n",
    "    with tf.device('/cpu:0'):\n",
    "      \n",
    "        # image = load_image(image_path) # infraed image input. there for 8 bit input\n",
    "        image = tf.cast(load_dcm(image_path, dcm=True), tf.float32)  # infraed image input. there for 8 bit input\n",
    "        \n",
    "        print(\"load image shape:\", image.shape)\n",
    "#         mask = load_image(mask_path, mask=True)\n",
    "#         mask = tf.cast(mask > 0, dtype=tf.float32)\n",
    "        print(image)\n",
    "#         image = resize(image)\n",
    "        # image, mask = random_scale(image, mask) # random resize\n",
    "        image = std_norm(image)  # norm before padding and crop_pad\n",
    "        # image, mask = pad_inputs(image, mask)  # and pad to raw size\n",
    "        # image, mask = random_crop(image, mask)  #\n",
    "        image = random_flip_auto(image)\n",
    "        print(\"prepro image shape:\", image.shape)\n",
    "        return image, image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The entire training dataset is: 23235\n",
      "The entire validation dataset is: 2730\n",
      "Tensor(\"image_path:0\", shape=(), dtype=string)\n",
      "read dcm data\n",
      "(None, None, None, None)\n",
      "load image shape: (None, None, 1)\n",
      "Tensor(\"Cast:0\", shape=(None, None, 1), dtype=float32, device=/device:CPU:0)\n",
      "WARNING:tensorflow:From /home/ytx/anaconda3/envs/TF21Wio/lib/python3.7/site-packages/tensorflow_core/python/ops/image_ops_impl.py:1556: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "prepro image shape: (None, None, 1)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'Epoches' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-2d1460d82080>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m                                   num_parallel_calls=tf.data.experimental.AUTOTUNE)\n\u001b[1;32m     12\u001b[0m \u001b[0mtrain_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop_remainder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# drop reminder... if true batch= 6 otherwise =7\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mtrain_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEpoches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0mtrain_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprefetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAUTOTUNE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Epoches' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"The entire training dataset is:\", len(train_all_image_paths))\n",
    "print(\"The entire validation dataset is:\", len(val_all_image_paths))\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(train_all_image_paths)\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices(val_all_image_paths)\n",
    "# for idx, data in enumerate(train_dataset):\n",
    "#     print(idx)\n",
    "#     print(data)\n",
    "    \n",
    "# train_dataset = train_dataset.shuffle(1024)\n",
    "train_dataset = train_dataset.map(map_func=train_preprocess_inputs_auto,\n",
    "                                  num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "train_dataset = train_dataset.batch(batch_size=batch_size, drop_remainder=False) # drop reminder... if true batch= 6 otherwise =7\n",
    "train_dataset = train_dataset.repeat(EPOCHS)\n",
    "train_dataset = train_dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "val_dataset = val_dataset.map(map_func=val_preprocess_inputs_auto,\n",
    "                                  num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "val_dataset = val_dataset.batch(batch_size=batch_size, drop_remainder=False) # drop reminder... if true batch= 6 otherwise =7\n",
    "val_dataset = val_dataset.repeat(EPOCHS)\n",
    "val_dataset = val_dataset.prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 512, 512, 1)]     0         \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 512, 512, 64)      640       \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 512, 512, 64)      36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 256, 256, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 256, 256, 128)     73856     \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 256, 256, 128)     147584    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 128, 128, 128)     0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 128, 128, 256)     295168    \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 128, 128, 256)     590080    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 64, 64, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 64, 64, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 64, 64, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 64, 64, 512)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 32, 32, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 32, 32, 1024)      4719616   \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 32, 32, 1024)      9438208   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 32, 32, 1024)      0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d (UpSampling2D) (None, 64, 64, 1024)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 64, 64, 512)       2097664   \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 64, 64, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 64, 64, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "up_sampling2d_1 (UpSampling2 (None, 128, 128, 512)     0         \n",
      "_________________________________________________________________\n",
      "conv2d_13 (Conv2D)           (None, 128, 128, 256)     524544    \n",
      "_________________________________________________________________\n",
      "conv2d_14 (Conv2D)           (None, 128, 128, 256)     590080    \n",
      "_________________________________________________________________\n",
      "conv2d_15 (Conv2D)           (None, 128, 128, 256)     590080    \n",
      "_________________________________________________________________\n",
      "up_sampling2d_2 (UpSampling2 (None, 256, 256, 256)     0         \n",
      "_________________________________________________________________\n",
      "conv2d_16 (Conv2D)           (None, 256, 256, 128)     131200    \n",
      "_________________________________________________________________\n",
      "conv2d_17 (Conv2D)           (None, 256, 256, 128)     147584    \n",
      "_________________________________________________________________\n",
      "conv2d_18 (Conv2D)           (None, 256, 256, 128)     147584    \n",
      "_________________________________________________________________\n",
      "up_sampling2d_3 (UpSampling2 (None, 512, 512, 128)     0         \n",
      "_________________________________________________________________\n",
      "conv2d_19 (Conv2D)           (None, 512, 512, 64)      32832     \n",
      "_________________________________________________________________\n",
      "conv2d_20 (Conv2D)           (None, 512, 512, 64)      36928     \n",
      "_________________________________________________________________\n",
      "conv2d_21 (Conv2D)           (None, 512, 512, 64)      36928     \n",
      "_________________________________________________________________\n",
      "conv2d_22 (Conv2D)           (None, 512, 512, 2)       1154      \n",
      "_________________________________________________________________\n",
      "conv2d_23 (Conv2D)           (None, 512, 512, 1)       3         \n",
      "=================================================================\n",
      "Total params: 27,898,245\n",
      "Trainable params: 27,898,245\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Failed to import pydot. You must install pydot and graphviz for `pydotprint` to work.\n"
     ]
    }
   ],
   "source": [
    "# # create model\n",
    "def U_NetV2():\n",
    "    inputs = keras.layers.Input((image_size, image_size, 1))\n",
    "    conv1 = Conv2D(64, 3, activation='relu', padding='same', kernel_initializer='he_normal')(inputs)\n",
    "    conv1 = Conv2D(64, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv1)\n",
    "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "    conv2 = Conv2D(128, 3, activation='relu', padding='same', kernel_initializer='he_normal')(pool1)\n",
    "    conv2 = Conv2D(128, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv2)\n",
    "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "    conv3 = Conv2D(256, 3, activation='relu', padding='same', kernel_initializer='he_normal')(pool2)\n",
    "    conv3 = Conv2D(256, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv3)\n",
    "    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
    "    conv4 = Conv2D(512, 3, activation='relu', padding='same', kernel_initializer='he_normal')(pool3)\n",
    "    conv4 = Conv2D(512, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv4)\n",
    "    drop4 = Dropout(0.5)(conv4)\n",
    "    pool4 = MaxPooling2D(pool_size=(2, 2))(drop4)\n",
    "\n",
    "    conv5 = Conv2D(1024, 3, activation='relu', padding='same', kernel_initializer='he_normal')(pool4)\n",
    "    conv5 = Conv2D(1024, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv5)\n",
    "    drop5 = Dropout(0.5)(conv5)\n",
    "\n",
    "    up6 = Conv2D(512, 2, activation='relu', padding='same', kernel_initializer='he_normal')(\n",
    "        UpSampling2D(size=(2, 2))(drop5))\n",
    "#     merge6 = concatenate([drop4, up6], axis=3)\n",
    "    conv6 = Conv2D(512, 3, activation='relu', padding='same', kernel_initializer='he_normal')(up6)\n",
    "    conv6 = Conv2D(512, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv6)\n",
    "\n",
    "    up7 = Conv2D(256, 2, activation='relu', padding='same', kernel_initializer='he_normal')(\n",
    "        UpSampling2D(size=(2, 2))(conv6))\n",
    "#     merge7 = concatenate([conv3, up7], axis=3)\n",
    "    conv7 = Conv2D(256, 3, activation='relu', padding='same', kernel_initializer='he_normal')(up7)\n",
    "    conv7 = Conv2D(256, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv7)\n",
    "\n",
    "    up8 = Conv2D(128, 2, activation='relu', padding='same', kernel_initializer='he_normal')(\n",
    "        UpSampling2D(size=(2, 2))(conv7))\n",
    "#     merge8 = concatenate([conv2, up8], axis=3)\n",
    "    conv8 = Conv2D(128, 3, activation='relu', padding='same', kernel_initializer='he_normal')(up8)\n",
    "    conv8 = Conv2D(128, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv8)\n",
    "\n",
    "    up9 = Conv2D(64, 2, activation='relu', padding='same', kernel_initializer='he_normal')(\n",
    "        UpSampling2D(size=(2, 2))(conv8))\n",
    "#     merge9 = concatenate([conv1, up9], axis=3)\n",
    "    conv9 = Conv2D(64, 3, activation='relu', padding='same', kernel_initializer='he_normal')(up9)\n",
    "    conv9 = Conv2D(64, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv9)\n",
    "    conv9 = Conv2D(2, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv9)\n",
    "    outputs = Conv2D(1, 1, activation='linear')(conv9)\n",
    "    model = keras.models.Model(inputs, outputs)\n",
    "    return model\n",
    "model =U_NetV2()\n",
    "model.compile(loss='mse', optimizer='adam',\n",
    "              metrics=['mse'])\n",
    "model.summary()\n",
    "\n",
    "keras.utils.plot_model(model, show_shapes=True, dpi=200, expand_nested=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation -------------------------------------------->\n",
    "train_avg_loss = tf.keras.metrics.Mean(name='train_avg_loss')\n",
    "train_avg_metric = tf.keras.metrics.Mean(name='train_avg_metric')\n",
    "test_avg_loss = tf.keras.metrics.Mean(name='test_avg_metric')\n",
    "test_avg_metric = tf.keras.metrics.Mean(name='test_avg_metric')\n",
    "\n",
    "\n",
    "# define loss\n",
    "@tf.function\n",
    "def mse_loss(y_true, y_pred, smooth=1):\n",
    "    loss =tf.keras.losses.MSE(y_true,y_pred)\n",
    "    return loss\n",
    "\n",
    "@tf.function\n",
    "def mae_loss(y_true, y_pred, smooth=1):\n",
    "    loss =tf.keras.losses.MAE(y_true,y_pred)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## #\n",
    "# above is global zone "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_tb_logs_image(writer, name_list, value_list, step,max_outs):\n",
    "    with writer.as_default():\n",
    "        # optimizer.iterations is actually the entire counter from step 1 to step total batch\n",
    "        for i in range(len(name_list)):\n",
    "            # print(value_list[i].shape)\n",
    "            # batch_images = np.expand_dims(value_list[i], -1)\n",
    "            # print(batch_images.shape)\n",
    "            tf.summary.image(name_list[i], value_list[i], step=step, max_outputs=max_outs)\n",
    "            # value_list[i].reset_states()  # Clear accumulated values with .reset_states()\n",
    "        writer.flush()\n",
    "\n",
    "def write_tb_logs_scaler(writer, name_list, value_list, step):\n",
    "    with writer.as_default():\n",
    "        # optimizer.iterations is actually the entire counter from step 1 to step total batch\n",
    "        for i in range(len(name_list)):\n",
    "            tf.summary.scalar(name_list[i], value_list[i], step=step)\n",
    "            # print(value_list[i].result())\n",
    "            # value_list[i].reset_states()  # Clear accumulated values with .reset_states()\n",
    "            # print(value_list[i].result())\n",
    "        writer.flush()\n",
    "\n",
    "@tf.function\n",
    "def train_step(input_feature, labels, model, optimizer):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(input_feature)\n",
    "        train_loss = mse_loss(labels, predictions)\n",
    "        metric = mae_loss(labels, predictions)\n",
    "    gradients = tape.gradient(train_loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    train_avg_loss(train_loss)\n",
    "    train_avg_metric(metric)\n",
    "\n",
    "@tf.function\n",
    "def test_step(input_feature, labels):\n",
    "\n",
    "    predictions = model(input_feature)\n",
    "    print(\"prediction shape:\", predictions.shape)\n",
    "\n",
    "    t_loss = mse_loss(labels, predictions)\n",
    "    metric = mae_loss(labels, predictions)\n",
    "    test_avg_loss(t_loss)\n",
    "    test_avg_metric(metric)\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def  train_and_checkpoint(train_dataset, model, EPOCHS, opt,\n",
    "                         train_summary_writer, test_summary_writer, graph_writer=None,\n",
    "                         ckpt=None, ckp_freq=0, manager=None):\n",
    "    temp_dice = 0 # mae the less the better\n",
    "\n",
    "    ckpt.restore(manager.latest_checkpoint)\n",
    "    #\n",
    "    if manager.latest_checkpoint:\n",
    "        print(\"Restored from {}\".format(manager.latest_checkpoint))\n",
    "    else:\n",
    "        print(\"Initializing from scratch.\")\n",
    "\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        if epoch == 0:\n",
    "            tf.summary.trace_on(graph=True, profiler=False)\n",
    "        # lr_epoch= epoch\n",
    "        epoch+=1\n",
    "        test_avg_metric_list = []\n",
    "        batch_count = 0\n",
    "        # each train epoch\n",
    "        for x, y in train_dataset:\n",
    "            print(\"ckpt.step:\", int(ckpt.step))\n",
    "            print(\"x.shape:\", x.shape)\n",
    "            print(\"y.shape:\", y.shape)\n",
    "            write_tb_logs_image(train_summary_writer, [\"input_image\"], [x], opt.iterations, batch_size)\n",
    "            write_tb_logs_image(train_summary_writer, [\"input_target\"], [y], opt.iterations, batch_size)\n",
    "            batch_count+=1\n",
    "            # print(x.shape)\n",
    "            # print(y.shape)\n",
    "\n",
    "            # train step ---- for batch training\n",
    "            train_step(x, y, model, opt)\n",
    "\n",
    "            batch_template = 'Step: {} Epoch {}- Batch[{}/{}], Train Avg Loss: {}, Train Avg dice: {}'\n",
    "        # #\n",
    "            print(batch_template.format(int(ckpt.step),\n",
    "                                        epoch,\n",
    "                                        batch_count,\n",
    "                                        total_num_batches_per_epoch,\n",
    "                                        train_avg_loss.result(),\n",
    "                                        train_avg_metric.result()))\n",
    "\n",
    "\n",
    "            print(\"lr:\", opt._decayed_lr(tf.float32).numpy())\n",
    "            write_tb_logs_scaler(train_summary_writer, [\"lr\"],\n",
    "                                 [opt._decayed_lr(tf.float32)], int(ckpt.step))\n",
    "            ckpt.step.assign_add(1)\n",
    "\n",
    "        # val dataset per epoch end\n",
    "        for x_val, y_val in val_dataset:\n",
    "            # print(\"x_val.shape:\", x_val.shape)\n",
    "            # print(\"y_val.shape:\", y_val.shape)\n",
    "            predictions = test_step(x_val, y_val)\n",
    "            write_tb_logs_image(test_summary_writer, [\"val_input_image\"], [x_val], opt.iterations, batch_size)\n",
    "            write_tb_logs_image(test_summary_writer, [\"val_input_target\"], [y_val], opt.iterations, batch_size)\n",
    "            write_tb_logs_image(test_summary_writer, [\"predictions\"], [predictions], opt.iterations, batch_size)\n",
    "\n",
    "        epoch_template = 'Val ------> Epoch {}, Loss: {}, Dice: {}, Val Loss: {}, Val Dice: {}'\n",
    "        print(\"*\"*130)\n",
    "        print(epoch_template.format(epoch,\n",
    "                              train_avg_loss.result(),\n",
    "                              train_avg_metric.result(),\n",
    "                              test_avg_loss.result(),\n",
    "                              test_avg_metric.result()))\n",
    "        print(\"*\"*130)\n",
    "        # write train logs # with the same name for train and test write will write multiple curves into one plot\n",
    "        write_tb_logs_scaler(train_summary_writer, [\"epoch_avg_loss\", \"epoch_avg_Dice\"],  # validation and train name need to be the same otherwise wont plot in one figure\n",
    "                             [train_avg_loss.result(), train_avg_metric.result()], epoch)\n",
    "\n",
    "        write_tb_logs_scaler(test_summary_writer, [\"epoch_avg_loss\", \"epoch_avg_Dice\"],\n",
    "                             [test_avg_loss.result(), test_avg_metric.result()], epoch)\n",
    "\n",
    "        if test_avg_metric.result() > temp_dice:\n",
    "            # 保存模型\n",
    "            # print(\"saving model...\")\n",
    "            # model.save('my_model.h5')\n",
    "            # print(\"model saved\")\n",
    "            save_path = manager.save()\n",
    "            print(\"Saved checkpoint for step {}: {}\".format(int(ckpt.step), save_path))\n",
    "            temp_dice = test_avg_metric.result()\n",
    "            print(\"temp_avg_val_dice:\", temp_dice.numpy())\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "        if epoch == 1:\n",
    "\n",
    "            # at epoch end write graph of the all the computation model\n",
    "            with graph_writer.as_default():\n",
    "                tf.summary.trace_export(\n",
    "                    name=\"my_func_trace\",\n",
    "                    step=0,\n",
    "                    profiler_outdir=os.path.join('logs', 'graph'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 733 Epoch 1- Batch[733/11618], Train Avg Loss: 0.5592132806777954, Train Avg dice: 0.4856517016887665\n",
      "lr: 9.5999996e-05\n",
      "ckpt.step: 734\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 734 Epoch 1- Batch[734/11618], Train Avg Loss: 0.5590329170227051, Train Avg dice: 0.4855446517467499\n",
      "lr: 9.5999996e-05\n",
      "ckpt.step: 735\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 735 Epoch 1- Batch[735/11618], Train Avg Loss: 0.5588535070419312, Train Avg dice: 0.4854353070259094\n",
      "lr: 9.5999996e-05\n",
      "ckpt.step: 736\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 736 Epoch 1- Batch[736/11618], Train Avg Loss: 0.5586751103401184, Train Avg dice: 0.48532557487487793\n",
      "lr: 9.5999996e-05\n",
      "ckpt.step: 737\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 737 Epoch 1- Batch[737/11618], Train Avg Loss: 0.5584977865219116, Train Avg dice: 0.48521602153778076\n",
      "lr: 9.5999996e-05\n",
      "ckpt.step: 738\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 738 Epoch 1- Batch[738/11618], Train Avg Loss: 0.5583178997039795, Train Avg dice: 0.4851049780845642\n",
      "lr: 9.5999996e-05\n",
      "ckpt.step: 739\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 739 Epoch 1- Batch[739/11618], Train Avg Loss: 0.5581356883049011, Train Avg dice: 0.48499393463134766\n",
      "lr: 9.5999996e-05\n",
      "ckpt.step: 740\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 740 Epoch 1- Batch[740/11618], Train Avg Loss: 0.5579534769058228, Train Avg dice: 0.484884113073349\n",
      "lr: 9.5999996e-05\n",
      "ckpt.step: 741\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 741 Epoch 1- Batch[741/11618], Train Avg Loss: 0.5577718615531921, Train Avg dice: 0.4847744405269623\n",
      "lr: 9.5999996e-05\n",
      "ckpt.step: 742\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 742 Epoch 1- Batch[742/11618], Train Avg Loss: 0.5575908422470093, Train Avg dice: 0.48466429114341736\n",
      "lr: 9.5999996e-05\n",
      "ckpt.step: 743\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 743 Epoch 1- Batch[743/11618], Train Avg Loss: 0.5574101805686951, Train Avg dice: 0.48455625772476196\n",
      "lr: 9.5999996e-05\n",
      "ckpt.step: 744\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 744 Epoch 1- Batch[744/11618], Train Avg Loss: 0.5572298765182495, Train Avg dice: 0.484447717666626\n",
      "lr: 9.5999996e-05\n",
      "ckpt.step: 745\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 745 Epoch 1- Batch[745/11618], Train Avg Loss: 0.5570501685142517, Train Avg dice: 0.4843392074108124\n",
      "lr: 9.5999996e-05\n",
      "ckpt.step: 746\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 746 Epoch 1- Batch[746/11618], Train Avg Loss: 0.5568712949752808, Train Avg dice: 0.4842308759689331\n",
      "lr: 9.5999996e-05\n",
      "ckpt.step: 747\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 747 Epoch 1- Batch[747/11618], Train Avg Loss: 0.5566927790641785, Train Avg dice: 0.48412439227104187\n",
      "lr: 9.5999996e-05\n",
      "ckpt.step: 748\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 748 Epoch 1- Batch[748/11618], Train Avg Loss: 0.5565147995948792, Train Avg dice: 0.4840170443058014\n",
      "lr: 9.5999996e-05\n",
      "ckpt.step: 749\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 749 Epoch 1- Batch[749/11618], Train Avg Loss: 0.5563376545906067, Train Avg dice: 0.48391014337539673\n",
      "lr: 9.5999996e-05\n",
      "ckpt.step: 750\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 750 Epoch 1- Batch[750/11618], Train Avg Loss: 0.5561614036560059, Train Avg dice: 0.4838036298751831\n",
      "lr: 9.5999996e-05\n",
      "ckpt.step: 751\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 751 Epoch 1- Batch[751/11618], Train Avg Loss: 0.5559859275817871, Train Avg dice: 0.48369625210762024\n",
      "lr: 9.5999996e-05\n",
      "ckpt.step: 752\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 752 Epoch 1- Batch[752/11618], Train Avg Loss: 0.5558111071586609, Train Avg dice: 0.4835899770259857\n",
      "lr: 9.5999996e-05\n",
      "ckpt.step: 753\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 753 Epoch 1- Batch[753/11618], Train Avg Loss: 0.5556364059448242, Train Avg dice: 0.4834842085838318\n",
      "lr: 9.5999996e-05\n",
      "ckpt.step: 754\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 754 Epoch 1- Batch[754/11618], Train Avg Loss: 0.5554617047309875, Train Avg dice: 0.48337942361831665\n",
      "lr: 9.5999996e-05\n",
      "ckpt.step: 755\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 755 Epoch 1- Batch[755/11618], Train Avg Loss: 0.5552870035171509, Train Avg dice: 0.4832751452922821\n",
      "lr: 9.5999996e-05\n",
      "ckpt.step: 756\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 756 Epoch 1- Batch[756/11618], Train Avg Loss: 0.5551124215126038, Train Avg dice: 0.48317232728004456\n",
      "lr: 9.5999996e-05\n",
      "ckpt.step: 757\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 757 Epoch 1- Batch[757/11618], Train Avg Loss: 0.5549374222755432, Train Avg dice: 0.4830702841281891\n",
      "lr: 9.5999996e-05\n",
      "ckpt.step: 758\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 758 Epoch 1- Batch[758/11618], Train Avg Loss: 0.5547617673873901, Train Avg dice: 0.48296862840652466\n",
      "lr: 9.5999996e-05\n",
      "ckpt.step: 759\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 759 Epoch 1- Batch[759/11618], Train Avg Loss: 0.5545861124992371, Train Avg dice: 0.48286762833595276\n",
      "lr: 9.5999996e-05\n",
      "ckpt.step: 760\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 760 Epoch 1- Batch[760/11618], Train Avg Loss: 0.554410994052887, Train Avg dice: 0.4827665686607361\n",
      "lr: 9.5999996e-05\n",
      "ckpt.step: 761\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 761 Epoch 1- Batch[761/11618], Train Avg Loss: 0.5542362928390503, Train Avg dice: 0.48266589641571045\n",
      "lr: 9.5999996e-05\n",
      "ckpt.step: 762\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 762 Epoch 1- Batch[762/11618], Train Avg Loss: 0.5540621280670166, Train Avg dice: 0.4825648367404938\n",
      "lr: 9.5999996e-05\n",
      "ckpt.step: 763\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 763 Epoch 1- Batch[763/11618], Train Avg Loss: 0.5538884997367859, Train Avg dice: 0.4824642241001129\n",
      "lr: 9.5999996e-05\n",
      "ckpt.step: 764\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 764 Epoch 1- Batch[764/11618], Train Avg Loss: 0.5537154078483582, Train Avg dice: 0.4823639988899231\n",
      "lr: 9.5999996e-05\n",
      "ckpt.step: 765\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 765 Epoch 1- Batch[765/11618], Train Avg Loss: 0.5535423755645752, Train Avg dice: 0.4822642207145691\n",
      "lr: 9.5999996e-05\n",
      "ckpt.step: 766\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 766 Epoch 1- Batch[766/11618], Train Avg Loss: 0.5533692240715027, Train Avg dice: 0.482163667678833\n",
      "lr: 9.5999996e-05\n",
      "ckpt.step: 767\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 767 Epoch 1- Batch[767/11618], Train Avg Loss: 0.5531958937644958, Train Avg dice: 0.4820636212825775\n",
      "lr: 9.5999996e-05\n",
      "ckpt.step: 768\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 768 Epoch 1- Batch[768/11618], Train Avg Loss: 0.5530228018760681, Train Avg dice: 0.48196324706077576\n",
      "lr: 9.5999996e-05\n",
      "ckpt.step: 769\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 769 Epoch 1- Batch[769/11618], Train Avg Loss: 0.5528501868247986, Train Avg dice: 0.48186248540878296\n",
      "lr: 9.5999996e-05\n",
      "ckpt.step: 770\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 770 Epoch 1- Batch[770/11618], Train Avg Loss: 0.5526774525642395, Train Avg dice: 0.48176109790802\n",
      "lr: 9.5999996e-05\n",
      "ckpt.step: 771\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 771 Epoch 1- Batch[771/11618], Train Avg Loss: 0.5525053143501282, Train Avg dice: 0.4816594421863556\n",
      "lr: 9.5999996e-05\n",
      "ckpt.step: 772\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 772 Epoch 1- Batch[772/11618], Train Avg Loss: 0.552333652973175, Train Avg dice: 0.481558620929718\n",
      "lr: 9.5999996e-05\n",
      "ckpt.step: 773\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 773 Epoch 1- Batch[773/11618], Train Avg Loss: 0.5521624088287354, Train Avg dice: 0.4814571440219879\n",
      "lr: 9.5999996e-05\n",
      "ckpt.step: 774\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 774 Epoch 1- Batch[774/11618], Train Avg Loss: 0.5519917607307434, Train Avg dice: 0.48135510087013245\n",
      "lr: 9.5999996e-05\n",
      "ckpt.step: 775\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 775 Epoch 1- Batch[775/11618], Train Avg Loss: 0.5518217086791992, Train Avg dice: 0.4812539517879486\n",
      "lr: 9.5999996e-05\n",
      "ckpt.step: 776\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 776 Epoch 1- Batch[776/11618], Train Avg Loss: 0.5516518950462341, Train Avg dice: 0.4811520278453827\n",
      "lr: 9.5999996e-05\n",
      "ckpt.step: 777\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 777 Epoch 1- Batch[777/11618], Train Avg Loss: 0.551482617855072, Train Avg dice: 0.4810495972633362\n",
      "lr: 9.5999996e-05\n",
      "ckpt.step: 778\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 778 Epoch 1- Batch[778/11618], Train Avg Loss: 0.5513142347335815, Train Avg dice: 0.48094624280929565\n",
      "lr: 9.5999996e-05\n",
      "ckpt.step: 779\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 779 Epoch 1- Batch[779/11618], Train Avg Loss: 0.5511465668678284, Train Avg dice: 0.4808419346809387\n",
      "lr: 9.5999996e-05\n",
      "ckpt.step: 780\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 780 Epoch 1- Batch[780/11618], Train Avg Loss: 0.550979733467102, Train Avg dice: 0.48073819279670715\n",
      "lr: 9.5999996e-05\n",
      "ckpt.step: 781\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 781 Epoch 1- Batch[781/11618], Train Avg Loss: 0.5508137345314026, Train Avg dice: 0.480633944272995\n",
      "lr: 9.5999996e-05\n",
      "ckpt.step: 782\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 782 Epoch 1- Batch[782/11618], Train Avg Loss: 0.55064857006073, Train Avg dice: 0.4805287718772888\n",
      "lr: 9.5999996e-05\n",
      "ckpt.step: 783\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 783 Epoch 1- Batch[783/11618], Train Avg Loss: 0.5504842400550842, Train Avg dice: 0.48042312264442444\n",
      "lr: 9.5999996e-05\n",
      "ckpt.step: 784\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 784 Epoch 1- Batch[784/11618], Train Avg Loss: 0.5503208637237549, Train Avg dice: 0.4803171753883362\n",
      "lr: 9.5999996e-05\n",
      "ckpt.step: 785\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 785 Epoch 1- Batch[785/11618], Train Avg Loss: 0.5501585602760315, Train Avg dice: 0.48021137714385986\n",
      "lr: 9.5999996e-05\n",
      "ckpt.step: 786\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 786 Epoch 1- Batch[786/11618], Train Avg Loss: 0.5499967932701111, Train Avg dice: 0.48010486364364624\n",
      "lr: 9.5999996e-05\n",
      "ckpt.step: 787\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 787 Epoch 1- Batch[787/11618], Train Avg Loss: 0.5498355031013489, Train Avg dice: 0.47999802231788635\n",
      "lr: 9.5999996e-05\n",
      "ckpt.step: 788\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 788 Epoch 1- Batch[788/11618], Train Avg Loss: 0.5496755838394165, Train Avg dice: 0.4798903167247772\n",
      "lr: 9.5999996e-05\n",
      "ckpt.step: 789\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 789 Epoch 1- Batch[789/11618], Train Avg Loss: 0.5495172142982483, Train Avg dice: 0.47978124022483826\n",
      "lr: 9.5999996e-05\n",
      "ckpt.step: 790\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 790 Epoch 1- Batch[790/11618], Train Avg Loss: 0.549359917640686, Train Avg dice: 0.47967180609703064\n",
      "lr: 9.5999996e-05\n",
      "ckpt.step: 791\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 791 Epoch 1- Batch[791/11618], Train Avg Loss: 0.5492027997970581, Train Avg dice: 0.47956225275993347\n",
      "lr: 9.5999996e-05\n",
      "ckpt.step: 792\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 792 Epoch 1- Batch[792/11618], Train Avg Loss: 0.549045741558075, Train Avg dice: 0.47945207357406616\n",
      "lr: 9.5999996e-05\n",
      "ckpt.step: 793\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 793 Epoch 1- Batch[793/11618], Train Avg Loss: 0.5488895177841187, Train Avg dice: 0.4793422818183899\n",
      "lr: 9.5999996e-05\n",
      "ckpt.step: 794\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 794 Epoch 1- Batch[794/11618], Train Avg Loss: 0.5487345457077026, Train Avg dice: 0.47923141717910767\n",
      "lr: 9.5999996e-05\n",
      "ckpt.step: 795\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 795 Epoch 1- Batch[795/11618], Train Avg Loss: 0.5485813617706299, Train Avg dice: 0.4791201055049896\n",
      "lr: 9.5999996e-05\n",
      "ckpt.step: 796\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 796 Epoch 1- Batch[796/11618], Train Avg Loss: 0.548429548740387, Train Avg dice: 0.4790084660053253\n",
      "lr: 9.5999996e-05\n",
      "ckpt.step: 797\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 797 Epoch 1- Batch[797/11618], Train Avg Loss: 0.5482782125473022, Train Avg dice: 0.4788953959941864\n",
      "lr: 9.5999996e-05\n",
      "ckpt.step: 798\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 798 Epoch 1- Batch[798/11618], Train Avg Loss: 0.5481273531913757, Train Avg dice: 0.47878164052963257\n",
      "lr: 9.5999996e-05\n",
      "ckpt.step: 799\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 799 Epoch 1- Batch[799/11618], Train Avg Loss: 0.5479772090911865, Train Avg dice: 0.4786684811115265\n",
      "lr: 9.5999996e-05\n",
      "ckpt.step: 800\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 800 Epoch 1- Batch[800/11618], Train Avg Loss: 0.5478275418281555, Train Avg dice: 0.47855496406555176\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 801\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 801 Epoch 1- Batch[801/11618], Train Avg Loss: 0.5476782321929932, Train Avg dice: 0.47844141721725464\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 802\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 802 Epoch 1- Batch[802/11618], Train Avg Loss: 0.5475303530693054, Train Avg dice: 0.4783273935317993\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 803\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 803 Epoch 1- Batch[803/11618], Train Avg Loss: 0.5473842024803162, Train Avg dice: 0.4782126545906067\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 804\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 804 Epoch 1- Batch[804/11618], Train Avg Loss: 0.5472396612167358, Train Avg dice: 0.4780977666378021\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 805\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 805 Epoch 1- Batch[805/11618], Train Avg Loss: 0.5470960140228271, Train Avg dice: 0.47798192501068115\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 806\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 806 Epoch 1- Batch[806/11618], Train Avg Loss: 0.5469539165496826, Train Avg dice: 0.4778650104999542\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 807\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 807 Epoch 1- Batch[807/11618], Train Avg Loss: 0.5468133091926575, Train Avg dice: 0.4777471721172333\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 808\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 808 Epoch 1- Batch[808/11618], Train Avg Loss: 0.5466740727424622, Train Avg dice: 0.4776268005371094\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 809\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 809 Epoch 1- Batch[809/11618], Train Avg Loss: 0.5465365648269653, Train Avg dice: 0.477504700422287\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 810\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 810 Epoch 1- Batch[810/11618], Train Avg Loss: 0.5464006066322327, Train Avg dice: 0.4773808717727661\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 811\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 811 Epoch 1- Batch[811/11618], Train Avg Loss: 0.5462654232978821, Train Avg dice: 0.47725555300712585\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 812\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 812 Epoch 1- Batch[812/11618], Train Avg Loss: 0.5461304783821106, Train Avg dice: 0.4771289825439453\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 813\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 813 Epoch 1- Batch[813/11618], Train Avg Loss: 0.5459964275360107, Train Avg dice: 0.4770016372203827\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 814\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 814 Epoch 1- Batch[814/11618], Train Avg Loss: 0.5458641648292542, Train Avg dice: 0.4768729507923126\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 815\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 815 Epoch 1- Batch[815/11618], Train Avg Loss: 0.5457338094711304, Train Avg dice: 0.4767422676086426\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 816\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 816 Epoch 1- Batch[816/11618], Train Avg Loss: 0.5456044673919678, Train Avg dice: 0.47660911083221436\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 817\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 817 Epoch 1- Batch[817/11618], Train Avg Loss: 0.5454756617546082, Train Avg dice: 0.47647348046302795\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 818\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 818 Epoch 1- Batch[818/11618], Train Avg Loss: 0.5453481078147888, Train Avg dice: 0.47633594274520874\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 819\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 819 Epoch 1- Batch[819/11618], Train Avg Loss: 0.5452224016189575, Train Avg dice: 0.47619715332984924\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 820\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 820 Epoch 1- Batch[820/11618], Train Avg Loss: 0.545098066329956, Train Avg dice: 0.4760585129261017\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 821\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 821 Epoch 1- Batch[821/11618], Train Avg Loss: 0.5449744462966919, Train Avg dice: 0.4759199619293213\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 822\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 822 Epoch 1- Batch[822/11618], Train Avg Loss: 0.5448516011238098, Train Avg dice: 0.475782185792923\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 823\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 823 Epoch 1- Batch[823/11618], Train Avg Loss: 0.544730544090271, Train Avg dice: 0.4756450951099396\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 824\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 824 Epoch 1- Batch[824/11618], Train Avg Loss: 0.5446112155914307, Train Avg dice: 0.475507915019989\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 825\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 825 Epoch 1- Batch[825/11618], Train Avg Loss: 0.5444924235343933, Train Avg dice: 0.4753705859184265\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 826\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 826 Epoch 1- Batch[826/11618], Train Avg Loss: 0.5443737506866455, Train Avg dice: 0.475233256816864\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 827\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 827 Epoch 1- Batch[827/11618], Train Avg Loss: 0.544255793094635, Train Avg dice: 0.47509685158729553\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 828\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 828 Epoch 1- Batch[828/11618], Train Avg Loss: 0.5441378951072693, Train Avg dice: 0.4749603867530823\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 829\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 829 Epoch 1- Batch[829/11618], Train Avg Loss: 0.5440208911895752, Train Avg dice: 0.47482427954673767\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 830\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 830 Epoch 1- Batch[830/11618], Train Avg Loss: 0.5439043045043945, Train Avg dice: 0.47468870878219604\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 831\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 831 Epoch 1- Batch[831/11618], Train Avg Loss: 0.5437883734703064, Train Avg dice: 0.4745539128780365\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 832\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 832 Epoch 1- Batch[832/11618], Train Avg Loss: 0.5436723828315735, Train Avg dice: 0.47441980242729187\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 833\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 833 Epoch 1- Batch[833/11618], Train Avg Loss: 0.5435571670532227, Train Avg dice: 0.47428640723228455\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 834\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 834 Epoch 1- Batch[834/11618], Train Avg Loss: 0.5434434413909912, Train Avg dice: 0.4741532504558563\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 835\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 835 Epoch 1- Batch[835/11618], Train Avg Loss: 0.5433310270309448, Train Avg dice: 0.47402021288871765\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 836\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 836 Epoch 1- Batch[836/11618], Train Avg Loss: 0.5432183742523193, Train Avg dice: 0.4738878011703491\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 837\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 837 Epoch 1- Batch[837/11618], Train Avg Loss: 0.5431055426597595, Train Avg dice: 0.47375601530075073\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 838\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 838 Epoch 1- Batch[838/11618], Train Avg Loss: 0.5429925322532654, Train Avg dice: 0.47362443804740906\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 839\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 839 Epoch 1- Batch[839/11618], Train Avg Loss: 0.5428803563117981, Train Avg dice: 0.473493367433548\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 840\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 840 Epoch 1- Batch[840/11618], Train Avg Loss: 0.5427688360214233, Train Avg dice: 0.47336286306381226\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 841\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 841 Epoch 1- Batch[841/11618], Train Avg Loss: 0.5426579713821411, Train Avg dice: 0.4732324182987213\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 842\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 842 Epoch 1- Batch[842/11618], Train Avg Loss: 0.5425471663475037, Train Avg dice: 0.473102331161499\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 843\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 843 Epoch 1- Batch[843/11618], Train Avg Loss: 0.542436420917511, Train Avg dice: 0.47297239303588867\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 844\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 844 Epoch 1- Batch[844/11618], Train Avg Loss: 0.542326033115387, Train Avg dice: 0.4728427827358246\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 845\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 845 Epoch 1- Batch[845/11618], Train Avg Loss: 0.5422158241271973, Train Avg dice: 0.4727133810520172\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 846\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 846 Epoch 1- Batch[846/11618], Train Avg Loss: 0.5421063899993896, Train Avg dice: 0.47258394956588745\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 847\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 847 Epoch 1- Batch[847/11618], Train Avg Loss: 0.5419982671737671, Train Avg dice: 0.47245433926582336\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 848\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 848 Epoch 1- Batch[848/11618], Train Avg Loss: 0.5418910980224609, Train Avg dice: 0.4723242223262787\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 849\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 849 Epoch 1- Batch[849/11618], Train Avg Loss: 0.5417842268943787, Train Avg dice: 0.47219371795654297\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 850\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 850 Epoch 1- Batch[850/11618], Train Avg Loss: 0.5416777729988098, Train Avg dice: 0.4720636010169983\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 851\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 851 Epoch 1- Batch[851/11618], Train Avg Loss: 0.5415703058242798, Train Avg dice: 0.4719336926937103\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 852\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 852 Epoch 1- Batch[852/11618], Train Avg Loss: 0.541463315486908, Train Avg dice: 0.4718037247657776\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 853\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 853 Epoch 1- Batch[853/11618], Train Avg Loss: 0.5413577556610107, Train Avg dice: 0.4716739058494568\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 854\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 854 Epoch 1- Batch[854/11618], Train Avg Loss: 0.5412532091140747, Train Avg dice: 0.47154414653778076\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 855\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 855 Epoch 1- Batch[855/11618], Train Avg Loss: 0.541148841381073, Train Avg dice: 0.47141510248184204\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 856\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 856 Epoch 1- Batch[856/11618], Train Avg Loss: 0.5410453081130981, Train Avg dice: 0.47128763794898987\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 857\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 857 Epoch 1- Batch[857/11618], Train Avg Loss: 0.5409420132637024, Train Avg dice: 0.47116175293922424\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 858\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 858 Epoch 1- Batch[858/11618], Train Avg Loss: 0.5408386588096619, Train Avg dice: 0.4710372984409332\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 859\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 859 Epoch 1- Batch[859/11618], Train Avg Loss: 0.5407347083091736, Train Avg dice: 0.4709148705005646\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 860\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 860 Epoch 1- Batch[860/11618], Train Avg Loss: 0.5406298637390137, Train Avg dice: 0.47079363465309143\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 861\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 861 Epoch 1- Batch[861/11618], Train Avg Loss: 0.5405242443084717, Train Avg dice: 0.4706745445728302\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 862\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 862 Epoch 1- Batch[862/11618], Train Avg Loss: 0.5404186844825745, Train Avg dice: 0.47055670619010925\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 863\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 863 Epoch 1- Batch[863/11618], Train Avg Loss: 0.5403125882148743, Train Avg dice: 0.47044023871421814\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 864\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 864 Epoch 1- Batch[864/11618], Train Avg Loss: 0.5402064323425293, Train Avg dice: 0.4703264832496643\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 865\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 865 Epoch 1- Batch[865/11618], Train Avg Loss: 0.5400999784469604, Train Avg dice: 0.4702144265174866\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 866\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 866 Epoch 1- Batch[866/11618], Train Avg Loss: 0.5399931073188782, Train Avg dice: 0.4701042175292969\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 867\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 867 Epoch 1- Batch[867/11618], Train Avg Loss: 0.539885938167572, Train Avg dice: 0.46999508142471313\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 868\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 868 Epoch 1- Batch[868/11618], Train Avg Loss: 0.5397780537605286, Train Avg dice: 0.469887375831604\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 869\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 869 Epoch 1- Batch[869/11618], Train Avg Loss: 0.5396701097488403, Train Avg dice: 0.4697829484939575\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 870\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 870 Epoch 1- Batch[870/11618], Train Avg Loss: 0.5395616292953491, Train Avg dice: 0.4696788787841797\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 871\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 871 Epoch 1- Batch[871/11618], Train Avg Loss: 0.5394529700279236, Train Avg dice: 0.4695761799812317\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 872\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 872 Epoch 1- Batch[872/11618], Train Avg Loss: 0.539344310760498, Train Avg dice: 0.4694731831550598\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 873\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 873 Epoch 1- Batch[873/11618], Train Avg Loss: 0.5392361879348755, Train Avg dice: 0.4693714678287506\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 874\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 874 Epoch 1- Batch[874/11618], Train Avg Loss: 0.5391278862953186, Train Avg dice: 0.4692712426185608\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 875\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 875 Epoch 1- Batch[875/11618], Train Avg Loss: 0.5390182733535767, Train Avg dice: 0.46917057037353516\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 876\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 876 Epoch 1- Batch[876/11618], Train Avg Loss: 0.5389071702957153, Train Avg dice: 0.4690718948841095\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 877\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 877 Epoch 1- Batch[877/11618], Train Avg Loss: 0.5387948155403137, Train Avg dice: 0.4689723253250122\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 878\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 878 Epoch 1- Batch[878/11618], Train Avg Loss: 0.5386810302734375, Train Avg dice: 0.46887290477752686\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 879\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 879 Epoch 1- Batch[879/11618], Train Avg Loss: 0.5385663509368896, Train Avg dice: 0.46877530217170715\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 880\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 880 Epoch 1- Batch[880/11618], Train Avg Loss: 0.5384515523910522, Train Avg dice: 0.46867796778678894\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 881\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 881 Epoch 1- Batch[881/11618], Train Avg Loss: 0.5383368134498596, Train Avg dice: 0.4685790240764618\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 882\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 882 Epoch 1- Batch[882/11618], Train Avg Loss: 0.5382220149040222, Train Avg dice: 0.46848031878471375\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 883\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 883 Epoch 1- Batch[883/11618], Train Avg Loss: 0.5381070375442505, Train Avg dice: 0.46838024258613586\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 884\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 884 Epoch 1- Batch[884/11618], Train Avg Loss: 0.5379921793937683, Train Avg dice: 0.4682800769805908\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 885\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 885 Epoch 1- Batch[885/11618], Train Avg Loss: 0.5378775596618652, Train Avg dice: 0.4681790769100189\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 886\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 886 Epoch 1- Batch[886/11618], Train Avg Loss: 0.5377631187438965, Train Avg dice: 0.46807730197906494\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 887\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 887 Epoch 1- Batch[887/11618], Train Avg Loss: 0.5376494526863098, Train Avg dice: 0.46797406673431396\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 888\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 888 Epoch 1- Batch[888/11618], Train Avg Loss: 0.5375363230705261, Train Avg dice: 0.4678694009780884\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 889\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 889 Epoch 1- Batch[889/11618], Train Avg Loss: 0.537423849105835, Train Avg dice: 0.4677637219429016\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 890\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 890 Epoch 1- Batch[890/11618], Train Avg Loss: 0.5373123288154602, Train Avg dice: 0.4676578938961029\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 891\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 891 Epoch 1- Batch[891/11618], Train Avg Loss: 0.5372008085250854, Train Avg dice: 0.46754953265190125\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 892\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 892 Epoch 1- Batch[892/11618], Train Avg Loss: 0.5370888710021973, Train Avg dice: 0.4674384295940399\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 893\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 893 Epoch 1- Batch[893/11618], Train Avg Loss: 0.5369769930839539, Train Avg dice: 0.4673263132572174\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 894\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 894 Epoch 1- Batch[894/11618], Train Avg Loss: 0.5368654131889343, Train Avg dice: 0.4672127068042755\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 895\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 895 Epoch 1- Batch[895/11618], Train Avg Loss: 0.5367536544799805, Train Avg dice: 0.4670978784561157\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 896\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 896 Epoch 1- Batch[896/11618], Train Avg Loss: 0.5366417765617371, Train Avg dice: 0.4669817388057709\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 897\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 897 Epoch 1- Batch[897/11618], Train Avg Loss: 0.5365298986434937, Train Avg dice: 0.4668629467487335\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 898\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 898 Epoch 1- Batch[898/11618], Train Avg Loss: 0.5364177823066711, Train Avg dice: 0.46674150228500366\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 899\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 899 Epoch 1- Batch[899/11618], Train Avg Loss: 0.5363048911094666, Train Avg dice: 0.46661728620529175\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 900\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 900 Epoch 1- Batch[900/11618], Train Avg Loss: 0.5361912846565247, Train Avg dice: 0.46648970246315\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 901\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 901 Epoch 1- Batch[901/11618], Train Avg Loss: 0.5360782742500305, Train Avg dice: 0.4663598835468292\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 902\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 902 Epoch 1- Batch[902/11618], Train Avg Loss: 0.5359655618667603, Train Avg dice: 0.4662286341190338\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 903\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 903 Epoch 1- Batch[903/11618], Train Avg Loss: 0.5358531475067139, Train Avg dice: 0.46609732508659363\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 904\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 904 Epoch 1- Batch[904/11618], Train Avg Loss: 0.5357402563095093, Train Avg dice: 0.46596550941467285\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 905\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 905 Epoch 1- Batch[905/11618], Train Avg Loss: 0.5356264710426331, Train Avg dice: 0.4658341109752655\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 906\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 906 Epoch 1- Batch[906/11618], Train Avg Loss: 0.5355131030082703, Train Avg dice: 0.46570244431495667\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 907\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 907 Epoch 1- Batch[907/11618], Train Avg Loss: 0.5354013442993164, Train Avg dice: 0.46557098627090454\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 908\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 908 Epoch 1- Batch[908/11618], Train Avg Loss: 0.5352901220321655, Train Avg dice: 0.46543821692466736\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 909\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 909 Epoch 1- Batch[909/11618], Train Avg Loss: 0.5351794958114624, Train Avg dice: 0.4653056859970093\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 910\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 910 Epoch 1- Batch[910/11618], Train Avg Loss: 0.5350696444511414, Train Avg dice: 0.46517214179039\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 911\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 911 Epoch 1- Batch[911/11618], Train Avg Loss: 0.5349606275558472, Train Avg dice: 0.4650379419326782\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 912\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 912 Epoch 1- Batch[912/11618], Train Avg Loss: 0.5348518490791321, Train Avg dice: 0.4649040102958679\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 913\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 913 Epoch 1- Batch[913/11618], Train Avg Loss: 0.5347428917884827, Train Avg dice: 0.46477052569389343\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 914\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 914 Epoch 1- Batch[914/11618], Train Avg Loss: 0.534633994102478, Train Avg dice: 0.46463730931282043\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 915\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 915 Epoch 1- Batch[915/11618], Train Avg Loss: 0.5345253944396973, Train Avg dice: 0.4645043909549713\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 916\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 916 Epoch 1- Batch[916/11618], Train Avg Loss: 0.5344171524047852, Train Avg dice: 0.46437209844589233\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 917\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 917 Epoch 1- Batch[917/11618], Train Avg Loss: 0.5343101024627686, Train Avg dice: 0.4642401933670044\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 918\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 918 Epoch 1- Batch[918/11618], Train Avg Loss: 0.5342047810554504, Train Avg dice: 0.4641091227531433\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 919\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 919 Epoch 1- Batch[919/11618], Train Avg Loss: 0.5340991020202637, Train Avg dice: 0.46397772431373596\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 920\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 920 Epoch 1- Batch[920/11618], Train Avg Loss: 0.5339933633804321, Train Avg dice: 0.46384698152542114\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 921\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 921 Epoch 1- Batch[921/11618], Train Avg Loss: 0.5338881015777588, Train Avg dice: 0.46371689438819885\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 922\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 922 Epoch 1- Batch[922/11618], Train Avg Loss: 0.5337835550308228, Train Avg dice: 0.4635869264602661\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 923\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 923 Epoch 1- Batch[923/11618], Train Avg Loss: 0.5336788296699524, Train Avg dice: 0.463456928730011\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 924\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 924 Epoch 1- Batch[924/11618], Train Avg Loss: 0.533574104309082, Train Avg dice: 0.4633273184299469\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 925\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 925 Epoch 1- Batch[925/11618], Train Avg Loss: 0.533469021320343, Train Avg dice: 0.46319764852523804\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 926\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 926 Epoch 1- Batch[926/11618], Train Avg Loss: 0.5333641171455383, Train Avg dice: 0.4630688726902008\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 927\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 927 Epoch 1- Batch[927/11618], Train Avg Loss: 0.5332605242729187, Train Avg dice: 0.46294036507606506\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 928\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 928 Epoch 1- Batch[928/11618], Train Avg Loss: 0.5331572890281677, Train Avg dice: 0.4628123641014099\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 929\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 929 Epoch 1- Batch[929/11618], Train Avg Loss: 0.5330533981323242, Train Avg dice: 0.46268483996391296\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 930\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 930 Epoch 1- Batch[930/11618], Train Avg Loss: 0.5329490303993225, Train Avg dice: 0.46255818009376526\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 931\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 931 Epoch 1- Batch[931/11618], Train Avg Loss: 0.5328447818756104, Train Avg dice: 0.4624319076538086\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 932\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 932 Epoch 1- Batch[932/11618], Train Avg Loss: 0.532741367816925, Train Avg dice: 0.4623054563999176\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 933\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 933 Epoch 1- Batch[933/11618], Train Avg Loss: 0.5326371788978577, Train Avg dice: 0.4621797502040863\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 934\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 934 Epoch 1- Batch[934/11618], Train Avg Loss: 0.5325309038162231, Train Avg dice: 0.46205464005470276\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 935\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 935 Epoch 1- Batch[935/11618], Train Avg Loss: 0.5324246287345886, Train Avg dice: 0.4619303047657013\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 936\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 936 Epoch 1- Batch[936/11618], Train Avg Loss: 0.5323194265365601, Train Avg dice: 0.4618072509765625\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 937\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 937 Epoch 1- Batch[937/11618], Train Avg Loss: 0.5322138071060181, Train Avg dice: 0.4616844356060028\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 938\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 938 Epoch 1- Batch[938/11618], Train Avg Loss: 0.5321075916290283, Train Avg dice: 0.46156230568885803\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 939\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 939 Epoch 1- Batch[939/11618], Train Avg Loss: 0.5320011973381042, Train Avg dice: 0.46144038438796997\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 940\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 940 Epoch 1- Batch[940/11618], Train Avg Loss: 0.531895101070404, Train Avg dice: 0.4613199830055237\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 941\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 941 Epoch 1- Batch[941/11618], Train Avg Loss: 0.5317891836166382, Train Avg dice: 0.4611983895301819\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 942\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 942 Epoch 1- Batch[942/11618], Train Avg Loss: 0.5316837430000305, Train Avg dice: 0.4610772728919983\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 943\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 943 Epoch 1- Batch[943/11618], Train Avg Loss: 0.531578779220581, Train Avg dice: 0.46095603704452515\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 944\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 944 Epoch 1- Batch[944/11618], Train Avg Loss: 0.5314732193946838, Train Avg dice: 0.460834801197052\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 945\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 945 Epoch 1- Batch[945/11618], Train Avg Loss: 0.5313675403594971, Train Avg dice: 0.4607132375240326\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 946\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 946 Epoch 1- Batch[946/11618], Train Avg Loss: 0.5312631726264954, Train Avg dice: 0.4605908989906311\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 947\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 947 Epoch 1- Batch[947/11618], Train Avg Loss: 0.5311592817306519, Train Avg dice: 0.4604679346084595\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 948\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 948 Epoch 1- Batch[948/11618], Train Avg Loss: 0.531054675579071, Train Avg dice: 0.46034446358680725\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 949\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 949 Epoch 1- Batch[949/11618], Train Avg Loss: 0.5309516191482544, Train Avg dice: 0.4602206349372864\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 950\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 950 Epoch 1- Batch[950/11618], Train Avg Loss: 0.5308488607406616, Train Avg dice: 0.46009624004364014\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 951\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 951 Epoch 1- Batch[951/11618], Train Avg Loss: 0.5307450890541077, Train Avg dice: 0.4599720537662506\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 952\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 952 Epoch 1- Batch[952/11618], Train Avg Loss: 0.5306410789489746, Train Avg dice: 0.45984724164009094\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 953\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 953 Epoch 1- Batch[953/11618], Train Avg Loss: 0.5305376052856445, Train Avg dice: 0.4597228765487671\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 954\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 954 Epoch 1- Batch[954/11618], Train Avg Loss: 0.5304340720176697, Train Avg dice: 0.45959821343421936\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 955\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 955 Epoch 1- Batch[955/11618], Train Avg Loss: 0.5303314328193665, Train Avg dice: 0.4594742953777313\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 956\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 956 Epoch 1- Batch[956/11618], Train Avg Loss: 0.5302301049232483, Train Avg dice: 0.45934951305389404\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 957\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 957 Epoch 1- Batch[957/11618], Train Avg Loss: 0.5301295518875122, Train Avg dice: 0.45922476053237915\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 958\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 958 Epoch 1- Batch[958/11618], Train Avg Loss: 0.5300284028053284, Train Avg dice: 0.45909878611564636\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 959\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 959 Epoch 1- Batch[959/11618], Train Avg Loss: 0.5299273729324341, Train Avg dice: 0.4589724540710449\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 960\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 960 Epoch 1- Batch[960/11618], Train Avg Loss: 0.5298275351524353, Train Avg dice: 0.45884519815444946\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 961\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 961 Epoch 1- Batch[961/11618], Train Avg Loss: 0.5297262668609619, Train Avg dice: 0.4587182402610779\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 962\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 962 Epoch 1- Batch[962/11618], Train Avg Loss: 0.5296258330345154, Train Avg dice: 0.4585912227630615\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 963\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 963 Epoch 1- Batch[963/11618], Train Avg Loss: 0.5295258164405823, Train Avg dice: 0.4584641754627228\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 964\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 964 Epoch 1- Batch[964/11618], Train Avg Loss: 0.5294246077537537, Train Avg dice: 0.45833802223205566\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 965\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 965 Epoch 1- Batch[965/11618], Train Avg Loss: 0.529323160648346, Train Avg dice: 0.4582120478153229\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 966\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 966 Epoch 1- Batch[966/11618], Train Avg Loss: 0.5292220115661621, Train Avg dice: 0.4580868184566498\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 967\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 967 Epoch 1- Batch[967/11618], Train Avg Loss: 0.5291218757629395, Train Avg dice: 0.4579613506793976\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 968\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 968 Epoch 1- Batch[968/11618], Train Avg Loss: 0.5290229916572571, Train Avg dice: 0.45783624053001404\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 969\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 969 Epoch 1- Batch[969/11618], Train Avg Loss: 0.5289244651794434, Train Avg dice: 0.45771121978759766\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 970\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 970 Epoch 1- Batch[970/11618], Train Avg Loss: 0.5288270711898804, Train Avg dice: 0.4575859606266022\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 971\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 971 Epoch 1- Batch[971/11618], Train Avg Loss: 0.5287317037582397, Train Avg dice: 0.4574604332447052\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 972\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 972 Epoch 1- Batch[972/11618], Train Avg Loss: 0.5286369323730469, Train Avg dice: 0.457334041595459\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 973\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 973 Epoch 1- Batch[973/11618], Train Avg Loss: 0.5285424590110779, Train Avg dice: 0.4572068154811859\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 974\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 974 Epoch 1- Batch[974/11618], Train Avg Loss: 0.5284484028816223, Train Avg dice: 0.4570784270763397\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 975\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 975 Epoch 1- Batch[975/11618], Train Avg Loss: 0.5283536911010742, Train Avg dice: 0.45694980025291443\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 976\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 976 Epoch 1- Batch[976/11618], Train Avg Loss: 0.528258740901947, Train Avg dice: 0.4568205773830414\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 977\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 977 Epoch 1- Batch[977/11618], Train Avg Loss: 0.5281621813774109, Train Avg dice: 0.4566904604434967\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 978\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 978 Epoch 1- Batch[978/11618], Train Avg Loss: 0.5280648469924927, Train Avg dice: 0.45655933022499084\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 979\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 979 Epoch 1- Batch[979/11618], Train Avg Loss: 0.5279676914215088, Train Avg dice: 0.45642709732055664\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 980\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 980 Epoch 1- Batch[980/11618], Train Avg Loss: 0.5278710722923279, Train Avg dice: 0.45629459619522095\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 981\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 981 Epoch 1- Batch[981/11618], Train Avg Loss: 0.527774453163147, Train Avg dice: 0.45616233348846436\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 982\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 982 Epoch 1- Batch[982/11618], Train Avg Loss: 0.5276783108711243, Train Avg dice: 0.4560306668281555\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 983\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 983 Epoch 1- Batch[983/11618], Train Avg Loss: 0.5275833010673523, Train Avg dice: 0.45589926838874817\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 984\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 984 Epoch 1- Batch[984/11618], Train Avg Loss: 0.5274893641471863, Train Avg dice: 0.4557681381702423\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 985\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 985 Epoch 1- Batch[985/11618], Train Avg Loss: 0.5273964405059814, Train Avg dice: 0.4556366801261902\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 986\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 986 Epoch 1- Batch[986/11618], Train Avg Loss: 0.5273051857948303, Train Avg dice: 0.45550596714019775\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 987\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 987 Epoch 1- Batch[987/11618], Train Avg Loss: 0.5272147059440613, Train Avg dice: 0.4553748667240143\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 988\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 988 Epoch 1- Batch[988/11618], Train Avg Loss: 0.5271251797676086, Train Avg dice: 0.45524343848228455\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 989\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 989 Epoch 1- Batch[989/11618], Train Avg Loss: 0.5270360112190247, Train Avg dice: 0.4551115334033966\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 990\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 990 Epoch 1- Batch[990/11618], Train Avg Loss: 0.5269467234611511, Train Avg dice: 0.45497986674308777\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 991\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 991 Epoch 1- Batch[991/11618], Train Avg Loss: 0.526857316493988, Train Avg dice: 0.4548480808734894\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 992\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 992 Epoch 1- Batch[992/11618], Train Avg Loss: 0.526767373085022, Train Avg dice: 0.4547170102596283\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 993\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 993 Epoch 1- Batch[993/11618], Train Avg Loss: 0.5266768336296082, Train Avg dice: 0.4545857012271881\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 994\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 994 Epoch 1- Batch[994/11618], Train Avg Loss: 0.52658611536026, Train Avg dice: 0.4544546604156494\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 995\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 995 Epoch 1- Batch[995/11618], Train Avg Loss: 0.5264952182769775, Train Avg dice: 0.45432382822036743\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 996\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 996 Epoch 1- Batch[996/11618], Train Avg Loss: 0.5264049768447876, Train Avg dice: 0.4541946351528168\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 997\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 997 Epoch 1- Batch[997/11618], Train Avg Loss: 0.526316225528717, Train Avg dice: 0.45406559109687805\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 998\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 998 Epoch 1- Batch[998/11618], Train Avg Loss: 0.5262287855148315, Train Avg dice: 0.45393842458724976\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 999\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 999 Epoch 1- Batch[999/11618], Train Avg Loss: 0.5261427164077759, Train Avg dice: 0.45381200313568115\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 1000\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1000 Epoch 1- Batch[1000/11618], Train Avg Loss: 0.5260580778121948, Train Avg dice: 0.45368537306785583\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 1001\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1001 Epoch 1- Batch[1001/11618], Train Avg Loss: 0.5259743928909302, Train Avg dice: 0.45355841517448425\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 1002\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1002 Epoch 1- Batch[1002/11618], Train Avg Loss: 0.5258915424346924, Train Avg dice: 0.4534320533275604\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 1003\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1003 Epoch 1- Batch[1003/11618], Train Avg Loss: 0.5258087515830994, Train Avg dice: 0.4533067047595978\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 1004\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1004 Epoch 1- Batch[1004/11618], Train Avg Loss: 0.5257259011268616, Train Avg dice: 0.4531823694705963\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 1005\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1005 Epoch 1- Batch[1005/11618], Train Avg Loss: 0.5256426334381104, Train Avg dice: 0.4530603885650635\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 1006\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1006 Epoch 1- Batch[1006/11618], Train Avg Loss: 0.5255589485168457, Train Avg dice: 0.452938437461853\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 1007\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1007 Epoch 1- Batch[1007/11618], Train Avg Loss: 0.5254748463630676, Train Avg dice: 0.4528184235095978\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 1008\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1008 Epoch 1- Batch[1008/11618], Train Avg Loss: 0.5253909230232239, Train Avg dice: 0.45269736647605896\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 1009\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1009 Epoch 1- Batch[1009/11618], Train Avg Loss: 0.5253080129623413, Train Avg dice: 0.45257604122161865\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 1010\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1010 Epoch 1- Batch[1010/11618], Train Avg Loss: 0.525226354598999, Train Avg dice: 0.45245447754859924\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 1011\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1011 Epoch 1- Batch[1011/11618], Train Avg Loss: 0.5251451730728149, Train Avg dice: 0.45233359932899475\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 1012\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1012 Epoch 1- Batch[1012/11618], Train Avg Loss: 0.5250647664070129, Train Avg dice: 0.45221471786499023\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 1013\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1013 Epoch 1- Batch[1013/11618], Train Avg Loss: 0.5249857306480408, Train Avg dice: 0.45209720730781555\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 1014\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1014 Epoch 1- Batch[1014/11618], Train Avg Loss: 0.5249074101448059, Train Avg dice: 0.4519811272621155\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 1015\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1015 Epoch 1- Batch[1015/11618], Train Avg Loss: 0.5248293280601501, Train Avg dice: 0.4518645107746124\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 1016\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1016 Epoch 1- Batch[1016/11618], Train Avg Loss: 0.5247513651847839, Train Avg dice: 0.4517475366592407\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 1017\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1017 Epoch 1- Batch[1017/11618], Train Avg Loss: 0.5246734023094177, Train Avg dice: 0.4516303539276123\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 1018\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1018 Epoch 1- Batch[1018/11618], Train Avg Loss: 0.5245952010154724, Train Avg dice: 0.4515135884284973\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 1019\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1019 Epoch 1- Batch[1019/11618], Train Avg Loss: 0.5245170593261719, Train Avg dice: 0.4513983130455017\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 1020\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1020 Epoch 1- Batch[1020/11618], Train Avg Loss: 0.5244394540786743, Train Avg dice: 0.4512840211391449\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 1021\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1021 Epoch 1- Batch[1021/11618], Train Avg Loss: 0.5243634581565857, Train Avg dice: 0.4511711895465851\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 1022\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1022 Epoch 1- Batch[1022/11618], Train Avg Loss: 0.5242885947227478, Train Avg dice: 0.4510590434074402\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 1023\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1023 Epoch 1- Batch[1023/11618], Train Avg Loss: 0.5242140889167786, Train Avg dice: 0.45094797015190125\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 1024\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1024 Epoch 1- Batch[1024/11618], Train Avg Loss: 0.5241400003433228, Train Avg dice: 0.45083725452423096\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 1025\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1025 Epoch 1- Batch[1025/11618], Train Avg Loss: 0.5240660309791565, Train Avg dice: 0.450727641582489\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 1026\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1026 Epoch 1- Batch[1026/11618], Train Avg Loss: 0.5239923000335693, Train Avg dice: 0.45061901211738586\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 1027\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1027 Epoch 1- Batch[1027/11618], Train Avg Loss: 0.523918867111206, Train Avg dice: 0.45051121711730957\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 1028\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1028 Epoch 1- Batch[1028/11618], Train Avg Loss: 0.5238465070724487, Train Avg dice: 0.4504043161869049\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 1029\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1029 Epoch 1- Batch[1029/11618], Train Avg Loss: 0.523775041103363, Train Avg dice: 0.4502975344657898\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 1030\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1030 Epoch 1- Batch[1030/11618], Train Avg Loss: 0.5237034559249878, Train Avg dice: 0.45019102096557617\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 1031\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1031 Epoch 1- Batch[1031/11618], Train Avg Loss: 0.5236325860023499, Train Avg dice: 0.45008385181427\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 1032\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1032 Epoch 1- Batch[1032/11618], Train Avg Loss: 0.5235629677772522, Train Avg dice: 0.44997677206993103\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 1033\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1033 Epoch 1- Batch[1033/11618], Train Avg Loss: 0.5234937071800232, Train Avg dice: 0.44986963272094727\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 1034\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1034 Epoch 1- Batch[1034/11618], Train Avg Loss: 0.5234247446060181, Train Avg dice: 0.44976234436035156\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 1035\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1035 Epoch 1- Batch[1035/11618], Train Avg Loss: 0.5233563184738159, Train Avg dice: 0.4496551752090454\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 1036\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1036 Epoch 1- Batch[1036/11618], Train Avg Loss: 0.5232890844345093, Train Avg dice: 0.4495478570461273\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 1037\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1037 Epoch 1- Batch[1037/11618], Train Avg Loss: 0.523222804069519, Train Avg dice: 0.449440062046051\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 1038\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1038 Epoch 1- Batch[1038/11618], Train Avg Loss: 0.5231574177742004, Train Avg dice: 0.44933152198791504\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 1039\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1039 Epoch 1- Batch[1039/11618], Train Avg Loss: 0.5230932235717773, Train Avg dice: 0.4492214322090149\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 1040\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1040 Epoch 1- Batch[1040/11618], Train Avg Loss: 0.5230281352996826, Train Avg dice: 0.44910916686058044\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 1041\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1041 Epoch 1- Batch[1041/11618], Train Avg Loss: 0.5229619145393372, Train Avg dice: 0.4489947557449341\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 1042\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1042 Epoch 1- Batch[1042/11618], Train Avg Loss: 0.5228943824768066, Train Avg dice: 0.44887763261795044\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 1043\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1043 Epoch 1- Batch[1043/11618], Train Avg Loss: 0.5228253602981567, Train Avg dice: 0.44875791668891907\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 1044\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1044 Epoch 1- Batch[1044/11618], Train Avg Loss: 0.5227552652359009, Train Avg dice: 0.44863617420196533\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 1045\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1045 Epoch 1- Batch[1045/11618], Train Avg Loss: 0.5226839184761047, Train Avg dice: 0.4485127329826355\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 1046\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1046 Epoch 1- Batch[1046/11618], Train Avg Loss: 0.5226117372512817, Train Avg dice: 0.4483904242515564\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 1047\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1047 Epoch 1- Batch[1047/11618], Train Avg Loss: 0.5225403308868408, Train Avg dice: 0.4482702314853668\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 1048\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1048 Epoch 1- Batch[1048/11618], Train Avg Loss: 0.5224698185920715, Train Avg dice: 0.44815802574157715\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 1049\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1049 Epoch 1- Batch[1049/11618], Train Avg Loss: 0.5223957896232605, Train Avg dice: 0.44803503155708313\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 1050\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1050 Epoch 1- Batch[1050/11618], Train Avg Loss: 0.5223177075386047, Train Avg dice: 0.4479026198387146\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 1051\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1051 Epoch 1- Batch[1051/11618], Train Avg Loss: 0.5222376585006714, Train Avg dice: 0.4477602243423462\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 1052\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1052 Epoch 1- Batch[1052/11618], Train Avg Loss: 0.5221585035324097, Train Avg dice: 0.4476251006126404\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 1053\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1053 Epoch 1- Batch[1053/11618], Train Avg Loss: 0.5220805406570435, Train Avg dice: 0.4474891424179077\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 1054\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1054 Epoch 1- Batch[1054/11618], Train Avg Loss: 0.5219998955726624, Train Avg dice: 0.4473430812358856\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 1055\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1055 Epoch 1- Batch[1055/11618], Train Avg Loss: 0.5219167470932007, Train Avg dice: 0.44719070196151733\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 1056\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1056 Epoch 1- Batch[1056/11618], Train Avg Loss: 0.5218340754508972, Train Avg dice: 0.44704169034957886\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 1057\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1057 Epoch 1- Batch[1057/11618], Train Avg Loss: 0.5217500329017639, Train Avg dice: 0.44689488410949707\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 1058\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1058 Epoch 1- Batch[1058/11618], Train Avg Loss: 0.5216631293296814, Train Avg dice: 0.4467463195323944\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 1059\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1059 Epoch 1- Batch[1059/11618], Train Avg Loss: 0.5215733647346497, Train Avg dice: 0.44659027457237244\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 1060\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1060 Epoch 1- Batch[1060/11618], Train Avg Loss: 0.5214830040931702, Train Avg dice: 0.4464414119720459\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 1061\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1061 Epoch 1- Batch[1061/11618], Train Avg Loss: 0.5213938355445862, Train Avg dice: 0.44628915190696716\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 1062\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1062 Epoch 1- Batch[1062/11618], Train Avg Loss: 0.5213035345077515, Train Avg dice: 0.446127325296402\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 1063\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1063 Epoch 1- Batch[1063/11618], Train Avg Loss: 0.5212130546569824, Train Avg dice: 0.4459686279296875\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 1064\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1064 Epoch 1- Batch[1064/11618], Train Avg Loss: 0.5211241245269775, Train Avg dice: 0.4458133578300476\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 1065\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1065 Epoch 1- Batch[1065/11618], Train Avg Loss: 0.5210365056991577, Train Avg dice: 0.44565966725349426\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 1066\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1066 Epoch 1- Batch[1066/11618], Train Avg Loss: 0.5209553241729736, Train Avg dice: 0.4455031454563141\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 1067\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1067 Epoch 1- Batch[1067/11618], Train Avg Loss: 0.5208786129951477, Train Avg dice: 0.4453457295894623\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 1068\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1068 Epoch 1- Batch[1068/11618], Train Avg Loss: 0.5208052396774292, Train Avg dice: 0.4451935589313507\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 1069\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1069 Epoch 1- Batch[1069/11618], Train Avg Loss: 0.5207346081733704, Train Avg dice: 0.4450374245643616\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 1070\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1070 Epoch 1- Batch[1070/11618], Train Avg Loss: 0.5206655859947205, Train Avg dice: 0.444881409406662\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 1071\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1071 Epoch 1- Batch[1071/11618], Train Avg Loss: 0.5205978751182556, Train Avg dice: 0.44472721219062805\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 1072\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1072 Epoch 1- Batch[1072/11618], Train Avg Loss: 0.5205310583114624, Train Avg dice: 0.4445713758468628\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 1073\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1073 Epoch 1- Batch[1073/11618], Train Avg Loss: 0.5204652547836304, Train Avg dice: 0.44441482424736023\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 1074\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1074 Epoch 1- Batch[1074/11618], Train Avg Loss: 0.5204010605812073, Train Avg dice: 0.44425949454307556\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 1075\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1075 Epoch 1- Batch[1075/11618], Train Avg Loss: 0.5203374028205872, Train Avg dice: 0.44410693645477295\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 1076\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1076 Epoch 1- Batch[1076/11618], Train Avg Loss: 0.5202744603157043, Train Avg dice: 0.4439544975757599\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 1077\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1077 Epoch 1- Batch[1077/11618], Train Avg Loss: 0.5202136039733887, Train Avg dice: 0.44380104541778564\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 1078\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1078 Epoch 1- Batch[1078/11618], Train Avg Loss: 0.5201588869094849, Train Avg dice: 0.4436526894569397\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 1079\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1079 Epoch 1- Batch[1079/11618], Train Avg Loss: 0.5201104879379272, Train Avg dice: 0.44350484013557434\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 1080\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1080 Epoch 1- Batch[1080/11618], Train Avg Loss: 0.5200700759887695, Train Avg dice: 0.44335973262786865\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 1081\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1081 Epoch 1- Batch[1081/11618], Train Avg Loss: 0.520044207572937, Train Avg dice: 0.4432157874107361\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 1082\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1082 Epoch 1- Batch[1082/11618], Train Avg Loss: 0.5200475454330444, Train Avg dice: 0.4430665075778961\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 1083\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1083 Epoch 1- Batch[1083/11618], Train Avg Loss: 0.5201193690299988, Train Avg dice: 0.44290468096733093\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 1084\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1084 Epoch 1- Batch[1084/11618], Train Avg Loss: 0.5202673077583313, Train Avg dice: 0.44273003935813904\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 1085\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1085 Epoch 1- Batch[1085/11618], Train Avg Loss: 0.5204594135284424, Train Avg dice: 0.4425477981567383\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 1086\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1086 Epoch 1- Batch[1086/11618], Train Avg Loss: 0.5206673741340637, Train Avg dice: 0.4423671364784241\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 1087\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1087 Epoch 1- Batch[1087/11618], Train Avg Loss: 0.520868182182312, Train Avg dice: 0.4421905279159546\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 1088\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1088 Epoch 1- Batch[1088/11618], Train Avg Loss: 0.5210853219032288, Train Avg dice: 0.44201961159706116\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 1089\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1089 Epoch 1- Batch[1089/11618], Train Avg Loss: 0.52130126953125, Train Avg dice: 0.4418521225452423\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 1090\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1090 Epoch 1- Batch[1090/11618], Train Avg Loss: 0.5215054750442505, Train Avg dice: 0.44169870018959045\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 1091\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1091 Epoch 1- Batch[1091/11618], Train Avg Loss: 0.5217195153236389, Train Avg dice: 0.44154098629951477\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 1092\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1092 Epoch 1- Batch[1092/11618], Train Avg Loss: 0.5219392776489258, Train Avg dice: 0.4413750469684601\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 1093\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1093 Epoch 1- Batch[1093/11618], Train Avg Loss: 0.5221474766731262, Train Avg dice: 0.4412028193473816\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 1094\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1094 Epoch 1- Batch[1094/11618], Train Avg Loss: 0.5223410725593567, Train Avg dice: 0.44102758169174194\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 1095\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1095 Epoch 1- Batch[1095/11618], Train Avg Loss: 0.5224671363830566, Train Avg dice: 0.440871000289917\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 1096\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1096 Epoch 1- Batch[1096/11618], Train Avg Loss: 0.5224786996841431, Train Avg dice: 0.44073501229286194\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 1097\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1097 Epoch 1- Batch[1097/11618], Train Avg Loss: 0.5224411487579346, Train Avg dice: 0.44061246514320374\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 1098\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1098 Epoch 1- Batch[1098/11618], Train Avg Loss: 0.5223953723907471, Train Avg dice: 0.4405055642127991\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 1099\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1099 Epoch 1- Batch[1099/11618], Train Avg Loss: 0.5223440527915955, Train Avg dice: 0.4403800070285797\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 1100\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1100 Epoch 1- Batch[1100/11618], Train Avg Loss: 0.5222930312156677, Train Avg dice: 0.44027432799339294\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 1101\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1101 Epoch 1- Batch[1101/11618], Train Avg Loss: 0.5222388505935669, Train Avg dice: 0.44016286730766296\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 1102\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1102 Epoch 1- Batch[1102/11618], Train Avg Loss: 0.5221789479255676, Train Avg dice: 0.44003915786743164\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 1103\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1103 Epoch 1- Batch[1103/11618], Train Avg Loss: 0.5221181511878967, Train Avg dice: 0.439922571182251\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 1104\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1104 Epoch 1- Batch[1104/11618], Train Avg Loss: 0.5220584869384766, Train Avg dice: 0.43981051445007324\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 1105\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1105 Epoch 1- Batch[1105/11618], Train Avg Loss: 0.521998405456543, Train Avg dice: 0.439693808555603\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 1106\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1106 Epoch 1- Batch[1106/11618], Train Avg Loss: 0.5219453573226929, Train Avg dice: 0.43958666920661926\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 1107\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1107 Epoch 1- Batch[1107/11618], Train Avg Loss: 0.5218934416770935, Train Avg dice: 0.4394873082637787\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 1108\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1108 Epoch 1- Batch[1108/11618], Train Avg Loss: 0.521835446357727, Train Avg dice: 0.43937116861343384\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 1109\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1109 Epoch 1- Batch[1109/11618], Train Avg Loss: 0.5217767357826233, Train Avg dice: 0.439269483089447\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 1110\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1110 Epoch 1- Batch[1110/11618], Train Avg Loss: 0.5217162370681763, Train Avg dice: 0.4391598105430603\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 1111\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1111 Epoch 1- Batch[1111/11618], Train Avg Loss: 0.5216550230979919, Train Avg dice: 0.43904921412467957\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 1112\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1112 Epoch 1- Batch[1112/11618], Train Avg Loss: 0.5215984582901001, Train Avg dice: 0.4389444887638092\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 1113\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1113 Epoch 1- Batch[1113/11618], Train Avg Loss: 0.521549642086029, Train Avg dice: 0.43883705139160156\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 1114\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1114 Epoch 1- Batch[1114/11618], Train Avg Loss: 0.5215068459510803, Train Avg dice: 0.43873241543769836\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 1115\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1115 Epoch 1- Batch[1115/11618], Train Avg Loss: 0.5214670300483704, Train Avg dice: 0.438632994890213\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 1116\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1116 Epoch 1- Batch[1116/11618], Train Avg Loss: 0.5214276909828186, Train Avg dice: 0.43852972984313965\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 1117\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1117 Epoch 1- Batch[1117/11618], Train Avg Loss: 0.5213906168937683, Train Avg dice: 0.43843045830726624\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 1118\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1118 Epoch 1- Batch[1118/11618], Train Avg Loss: 0.5213547348976135, Train Avg dice: 0.43833127617836\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 1119\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1119 Epoch 1- Batch[1119/11618], Train Avg Loss: 0.521321713924408, Train Avg dice: 0.4382305145263672\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 1120\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1120 Epoch 1- Batch[1120/11618], Train Avg Loss: 0.5212932825088501, Train Avg dice: 0.4381362497806549\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 1121\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1121 Epoch 1- Batch[1121/11618], Train Avg Loss: 0.5212677717208862, Train Avg dice: 0.43803855776786804\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 1122\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1122 Epoch 1- Batch[1122/11618], Train Avg Loss: 0.5212433934211731, Train Avg dice: 0.43794411420822144\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 1123\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1123 Epoch 1- Batch[1123/11618], Train Avg Loss: 0.5212206244468689, Train Avg dice: 0.4378443658351898\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 1124\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1124 Epoch 1- Batch[1124/11618], Train Avg Loss: 0.5212007164955139, Train Avg dice: 0.43774527311325073\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 1125\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1125 Epoch 1- Batch[1125/11618], Train Avg Loss: 0.5211822986602783, Train Avg dice: 0.4376446306705475\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 1126\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1126 Epoch 1- Batch[1126/11618], Train Avg Loss: 0.5211596488952637, Train Avg dice: 0.4375416934490204\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 1127\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1127 Epoch 1- Batch[1127/11618], Train Avg Loss: 0.5211342573165894, Train Avg dice: 0.4374414384365082\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 1128\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1128 Epoch 1- Batch[1128/11618], Train Avg Loss: 0.5211068987846375, Train Avg dice: 0.43734171986579895\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 1129\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1129 Epoch 1- Batch[1129/11618], Train Avg Loss: 0.5210774540901184, Train Avg dice: 0.4372439384460449\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 1130\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1130 Epoch 1- Batch[1130/11618], Train Avg Loss: 0.5210449695587158, Train Avg dice: 0.4371490776538849\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 1131\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1131 Epoch 1- Batch[1131/11618], Train Avg Loss: 0.5210092067718506, Train Avg dice: 0.4370521605014801\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 1132\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1132 Epoch 1- Batch[1132/11618], Train Avg Loss: 0.520972490310669, Train Avg dice: 0.4369582533836365\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 1133\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1133 Epoch 1- Batch[1133/11618], Train Avg Loss: 0.5209353566169739, Train Avg dice: 0.43686071038246155\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 1134\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1134 Epoch 1- Batch[1134/11618], Train Avg Loss: 0.5208985209465027, Train Avg dice: 0.43676522374153137\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 1135\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1135 Epoch 1- Batch[1135/11618], Train Avg Loss: 0.5208622217178345, Train Avg dice: 0.4366692900657654\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 1136\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1136 Epoch 1- Batch[1136/11618], Train Avg Loss: 0.5208267569541931, Train Avg dice: 0.4365730881690979\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 1137\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1137 Epoch 1- Batch[1137/11618], Train Avg Loss: 0.5207903981208801, Train Avg dice: 0.4364762008190155\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 1138\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1138 Epoch 1- Batch[1138/11618], Train Avg Loss: 0.5207522511482239, Train Avg dice: 0.4363795220851898\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 1139\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1139 Epoch 1- Batch[1139/11618], Train Avg Loss: 0.5207129120826721, Train Avg dice: 0.43628159165382385\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 1140\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1140 Epoch 1- Batch[1140/11618], Train Avg Loss: 0.5206737518310547, Train Avg dice: 0.4361831843852997\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 1141\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1141 Epoch 1- Batch[1141/11618], Train Avg Loss: 0.520634651184082, Train Avg dice: 0.43608447909355164\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 1142\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1142 Epoch 1- Batch[1142/11618], Train Avg Loss: 0.5205947756767273, Train Avg dice: 0.4359840750694275\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 1143\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1143 Epoch 1- Batch[1143/11618], Train Avg Loss: 0.52055424451828, Train Avg dice: 0.4358830451965332\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 1144\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1144 Epoch 1- Batch[1144/11618], Train Avg Loss: 0.5205140113830566, Train Avg dice: 0.43578147888183594\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 1145\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1145 Epoch 1- Batch[1145/11618], Train Avg Loss: 0.5204740166664124, Train Avg dice: 0.43568000197410583\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 1146\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1146 Epoch 1- Batch[1146/11618], Train Avg Loss: 0.5204339623451233, Train Avg dice: 0.4355781376361847\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 1147\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1147 Epoch 1- Batch[1147/11618], Train Avg Loss: 0.5203939080238342, Train Avg dice: 0.4354763627052307\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 1148\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1148 Epoch 1- Batch[1148/11618], Train Avg Loss: 0.5203540921211243, Train Avg dice: 0.4353749752044678\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 1149\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1149 Epoch 1- Batch[1149/11618], Train Avg Loss: 0.5203145742416382, Train Avg dice: 0.4352724850177765\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 1150\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1150 Epoch 1- Batch[1150/11618], Train Avg Loss: 0.52027428150177, Train Avg dice: 0.43517056107521057\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 1151\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1151 Epoch 1- Batch[1151/11618], Train Avg Loss: 0.5202336311340332, Train Avg dice: 0.4350678324699402\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 1152\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1152 Epoch 1- Batch[1152/11618], Train Avg Loss: 0.5201928615570068, Train Avg dice: 0.4349648356437683\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 1153\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1153 Epoch 1- Batch[1153/11618], Train Avg Loss: 0.5201517939567566, Train Avg dice: 0.43486151099205017\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 1154\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1154 Epoch 1- Batch[1154/11618], Train Avg Loss: 0.5201104283332825, Train Avg dice: 0.4347579777240753\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 1155\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1155 Epoch 1- Batch[1155/11618], Train Avg Loss: 0.5200684666633606, Train Avg dice: 0.4346539080142975\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 1156\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1156 Epoch 1- Batch[1156/11618], Train Avg Loss: 0.5200263261795044, Train Avg dice: 0.4345494210720062\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 1157\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1157 Epoch 1- Batch[1157/11618], Train Avg Loss: 0.519984245300293, Train Avg dice: 0.4344449043273926\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 1158\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1158 Epoch 1- Batch[1158/11618], Train Avg Loss: 0.5199424028396606, Train Avg dice: 0.43433988094329834\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 1159\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1159 Epoch 1- Batch[1159/11618], Train Avg Loss: 0.5199007987976074, Train Avg dice: 0.4342338740825653\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 1160\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1160 Epoch 1- Batch[1160/11618], Train Avg Loss: 0.5198593139648438, Train Avg dice: 0.4341273009777069\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 1161\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1161 Epoch 1- Batch[1161/11618], Train Avg Loss: 0.5198183059692383, Train Avg dice: 0.4340199828147888\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 1162\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1162 Epoch 1- Batch[1162/11618], Train Avg Loss: 0.5197776556015015, Train Avg dice: 0.4339117407798767\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 1163\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1163 Epoch 1- Batch[1163/11618], Train Avg Loss: 0.5197373628616333, Train Avg dice: 0.43380266427993774\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 1164\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1164 Epoch 1- Batch[1164/11618], Train Avg Loss: 0.5196971893310547, Train Avg dice: 0.4336925745010376\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 1165\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1165 Epoch 1- Batch[1165/11618], Train Avg Loss: 0.5196572542190552, Train Avg dice: 0.43358156085014343\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 1166\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1166 Epoch 1- Batch[1166/11618], Train Avg Loss: 0.5196178555488586, Train Avg dice: 0.4334695339202881\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 1167\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1167 Epoch 1- Batch[1167/11618], Train Avg Loss: 0.5195790529251099, Train Avg dice: 0.43335655331611633\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 1168\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1168 Epoch 1- Batch[1168/11618], Train Avg Loss: 0.5195408463478088, Train Avg dice: 0.43324244022369385\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 1169\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1169 Epoch 1- Batch[1169/11618], Train Avg Loss: 0.5195035934448242, Train Avg dice: 0.43312710523605347\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 1170\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1170 Epoch 1- Batch[1170/11618], Train Avg Loss: 0.5194673538208008, Train Avg dice: 0.43301066756248474\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 1171\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1171 Epoch 1- Batch[1171/11618], Train Avg Loss: 0.5194320678710938, Train Avg dice: 0.43289297819137573\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 1172\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1172 Epoch 1- Batch[1172/11618], Train Avg Loss: 0.5193970799446106, Train Avg dice: 0.43277406692504883\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 1173\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1173 Epoch 1- Batch[1173/11618], Train Avg Loss: 0.5193620324134827, Train Avg dice: 0.43265363574028015\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 1174\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1174 Epoch 1- Batch[1174/11618], Train Avg Loss: 0.5193273425102234, Train Avg dice: 0.43253183364868164\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 1175\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1175 Epoch 1- Batch[1175/11618], Train Avg Loss: 0.5192927718162537, Train Avg dice: 0.43240851163864136\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 1176\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1176 Epoch 1- Batch[1176/11618], Train Avg Loss: 0.5192583799362183, Train Avg dice: 0.43228331208229065\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 1177\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1177 Epoch 1- Batch[1177/11618], Train Avg Loss: 0.5192241668701172, Train Avg dice: 0.4321560263633728\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 1178\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1178 Epoch 1- Batch[1178/11618], Train Avg Loss: 0.5191895365715027, Train Avg dice: 0.43202686309814453\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 1179\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1179 Epoch 1- Batch[1179/11618], Train Avg Loss: 0.5191553831100464, Train Avg dice: 0.4318959414958954\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 1180\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1180 Epoch 1- Batch[1180/11618], Train Avg Loss: 0.519120991230011, Train Avg dice: 0.4317627549171448\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 1181\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1181 Epoch 1- Batch[1181/11618], Train Avg Loss: 0.519085705280304, Train Avg dice: 0.4316275119781494\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 1182\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1182 Epoch 1- Batch[1182/11618], Train Avg Loss: 0.5190495252609253, Train Avg dice: 0.4314901530742645\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 1183\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1183 Epoch 1- Batch[1183/11618], Train Avg Loss: 0.5190120935440063, Train Avg dice: 0.4313507080078125\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 1184\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1184 Epoch 1- Batch[1184/11618], Train Avg Loss: 0.5189734697341919, Train Avg dice: 0.4312097132205963\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 1185\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1185 Epoch 1- Batch[1185/11618], Train Avg Loss: 0.5189332365989685, Train Avg dice: 0.43106693029403687\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 1186\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1186 Epoch 1- Batch[1186/11618], Train Avg Loss: 0.5188902616500854, Train Avg dice: 0.4309224784374237\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 1187\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1187 Epoch 1- Batch[1187/11618], Train Avg Loss: 0.5188457369804382, Train Avg dice: 0.43077659606933594\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 1188\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1188 Epoch 1- Batch[1188/11618], Train Avg Loss: 0.5187996625900269, Train Avg dice: 0.4306293725967407\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 1189\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1189 Epoch 1- Batch[1189/11618], Train Avg Loss: 0.51875239610672, Train Avg dice: 0.43048095703125\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 1190\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1190 Epoch 1- Batch[1190/11618], Train Avg Loss: 0.5187041759490967, Train Avg dice: 0.43033164739608765\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 1191\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1191 Epoch 1- Batch[1191/11618], Train Avg Loss: 0.5186546444892883, Train Avg dice: 0.4301813542842865\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 1192\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1192 Epoch 1- Batch[1192/11618], Train Avg Loss: 0.5186034440994263, Train Avg dice: 0.43003037571907043\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 1193\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1193 Epoch 1- Batch[1193/11618], Train Avg Loss: 0.5185508728027344, Train Avg dice: 0.4298790991306305\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 1194\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1194 Epoch 1- Batch[1194/11618], Train Avg Loss: 0.5184952616691589, Train Avg dice: 0.42972803115844727\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 1195\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1195 Epoch 1- Batch[1195/11618], Train Avg Loss: 0.5184341073036194, Train Avg dice: 0.4295784533023834\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 1196\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1196 Epoch 1- Batch[1196/11618], Train Avg Loss: 0.5183631181716919, Train Avg dice: 0.42943161725997925\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 1197\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1197 Epoch 1- Batch[1197/11618], Train Avg Loss: 0.5182774662971497, Train Avg dice: 0.42928946018218994\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 1198\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1198 Epoch 1- Batch[1198/11618], Train Avg Loss: 0.5181798934936523, Train Avg dice: 0.42915117740631104\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 1199\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1199 Epoch 1- Batch[1199/11618], Train Avg Loss: 0.5180648565292358, Train Avg dice: 0.4290183484554291\n",
      "lr: 9.215999e-05\n",
      "ckpt.step: 1200\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1200 Epoch 1- Batch[1200/11618], Train Avg Loss: 0.5178983807563782, Train Avg dice: 0.42890116572380066\n",
      "lr: 8.8473585e-05\n",
      "ckpt.step: 1201\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1201 Epoch 1- Batch[1201/11618], Train Avg Loss: 0.5176956653594971, Train Avg dice: 0.42879748344421387\n",
      "lr: 8.8473585e-05\n",
      "ckpt.step: 1202\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1202 Epoch 1- Batch[1202/11618], Train Avg Loss: 0.5174846649169922, Train Avg dice: 0.4287109375\n",
      "lr: 8.8473585e-05\n",
      "ckpt.step: 1203\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1203 Epoch 1- Batch[1203/11618], Train Avg Loss: 0.5172721147537231, Train Avg dice: 0.42865583300590515\n",
      "lr: 8.8473585e-05\n",
      "ckpt.step: 1204\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1204 Epoch 1- Batch[1204/11618], Train Avg Loss: 0.5170915722846985, Train Avg dice: 0.4286647140979767\n",
      "lr: 8.8473585e-05\n",
      "ckpt.step: 1205\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1205 Epoch 1- Batch[1205/11618], Train Avg Loss: 0.5168477892875671, Train Avg dice: 0.42858821153640747\n",
      "lr: 8.8473585e-05\n",
      "ckpt.step: 1206\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1206 Epoch 1- Batch[1206/11618], Train Avg Loss: 0.5166751742362976, Train Avg dice: 0.42860642075538635\n",
      "lr: 8.8473585e-05\n",
      "ckpt.step: 1207\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1207 Epoch 1- Batch[1207/11618], Train Avg Loss: 0.5165903568267822, Train Avg dice: 0.42868149280548096\n",
      "lr: 8.8473585e-05\n",
      "ckpt.step: 1208\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1208 Epoch 1- Batch[1208/11618], Train Avg Loss: 0.5165266394615173, Train Avg dice: 0.42876723408699036\n",
      "lr: 8.8473585e-05\n",
      "ckpt.step: 1209\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1209 Epoch 1- Batch[1209/11618], Train Avg Loss: 0.5162907838821411, Train Avg dice: 0.42870140075683594\n",
      "lr: 8.8473585e-05\n",
      "ckpt.step: 1210\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1210 Epoch 1- Batch[1210/11618], Train Avg Loss: 0.5163621306419373, Train Avg dice: 0.428835391998291\n",
      "lr: 8.8473585e-05\n",
      "ckpt.step: 1211\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1211 Epoch 1- Batch[1211/11618], Train Avg Loss: 0.5163930654525757, Train Avg dice: 0.42895975708961487\n",
      "lr: 8.8473585e-05\n",
      "ckpt.step: 1212\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1212 Epoch 1- Batch[1212/11618], Train Avg Loss: 0.5166086554527283, Train Avg dice: 0.4291500151157379\n",
      "lr: 8.8473585e-05\n",
      "ckpt.step: 1213\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1213 Epoch 1- Batch[1213/11618], Train Avg Loss: 0.516852080821991, Train Avg dice: 0.4293489158153534\n",
      "lr: 8.8473585e-05\n",
      "ckpt.step: 1214\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1214 Epoch 1- Batch[1214/11618], Train Avg Loss: 0.5170276165008545, Train Avg dice: 0.4295259118080139\n",
      "lr: 8.8473585e-05\n",
      "ckpt.step: 1215\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1215 Epoch 1- Batch[1215/11618], Train Avg Loss: 0.5170222520828247, Train Avg dice: 0.42963385581970215\n",
      "lr: 8.8473585e-05\n",
      "ckpt.step: 1216\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1216 Epoch 1- Batch[1216/11618], Train Avg Loss: 0.5170556902885437, Train Avg dice: 0.4297565519809723\n",
      "lr: 8.8473585e-05\n",
      "ckpt.step: 1217\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1217 Epoch 1- Batch[1217/11618], Train Avg Loss: 0.5169275403022766, Train Avg dice: 0.4297303259372711\n",
      "lr: 8.8473585e-05\n",
      "ckpt.step: 1218\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1218 Epoch 1- Batch[1218/11618], Train Avg Loss: 0.516853928565979, Train Avg dice: 0.4296649396419525\n",
      "lr: 8.8473585e-05\n",
      "ckpt.step: 1219\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1219 Epoch 1- Batch[1219/11618], Train Avg Loss: 0.5167813897132874, Train Avg dice: 0.4296032190322876\n",
      "lr: 8.8473585e-05\n",
      "ckpt.step: 1220\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1220 Epoch 1- Batch[1220/11618], Train Avg Loss: 0.5167051553726196, Train Avg dice: 0.4295412302017212\n",
      "lr: 8.8473585e-05\n",
      "ckpt.step: 1221\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1221 Epoch 1- Batch[1221/11618], Train Avg Loss: 0.5166335105895996, Train Avg dice: 0.4294869601726532\n",
      "lr: 8.8473585e-05\n",
      "ckpt.step: 1222\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1222 Epoch 1- Batch[1222/11618], Train Avg Loss: 0.5165566802024841, Train Avg dice: 0.4294179379940033\n",
      "lr: 8.8473585e-05\n",
      "ckpt.step: 1223\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1223 Epoch 1- Batch[1223/11618], Train Avg Loss: 0.516481339931488, Train Avg dice: 0.4293476343154907\n",
      "lr: 8.8473585e-05\n",
      "ckpt.step: 1224\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1224 Epoch 1- Batch[1224/11618], Train Avg Loss: 0.5164079070091248, Train Avg dice: 0.42928189039230347\n",
      "lr: 8.8473585e-05\n",
      "ckpt.step: 1225\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1225 Epoch 1- Batch[1225/11618], Train Avg Loss: 0.5163325667381287, Train Avg dice: 0.4292018711566925\n",
      "lr: 8.8473585e-05\n",
      "ckpt.step: 1226\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1226 Epoch 1- Batch[1226/11618], Train Avg Loss: 0.516257107257843, Train Avg dice: 0.4291211664676666\n",
      "lr: 8.8473585e-05\n",
      "ckpt.step: 1227\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1227 Epoch 1- Batch[1227/11618], Train Avg Loss: 0.5161844491958618, Train Avg dice: 0.429046094417572\n",
      "lr: 8.8473585e-05\n",
      "ckpt.step: 1228\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1228 Epoch 1- Batch[1228/11618], Train Avg Loss: 0.5161142945289612, Train Avg dice: 0.4289811849594116\n",
      "lr: 8.8473585e-05\n",
      "ckpt.step: 1229\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1229 Epoch 1- Batch[1229/11618], Train Avg Loss: 0.51604163646698, Train Avg dice: 0.42891067266464233\n",
      "lr: 8.8473585e-05\n",
      "ckpt.step: 1230\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1230 Epoch 1- Batch[1230/11618], Train Avg Loss: 0.5159696936607361, Train Avg dice: 0.42884373664855957\n",
      "lr: 8.8473585e-05\n",
      "ckpt.step: 1231\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1231 Epoch 1- Batch[1231/11618], Train Avg Loss: 0.515901505947113, Train Avg dice: 0.4287828803062439\n",
      "lr: 8.8473585e-05\n",
      "ckpt.step: 1232\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1232 Epoch 1- Batch[1232/11618], Train Avg Loss: 0.5158355236053467, Train Avg dice: 0.42872872948646545\n",
      "lr: 8.8473585e-05\n",
      "ckpt.step: 1233\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1233 Epoch 1- Batch[1233/11618], Train Avg Loss: 0.5157673954963684, Train Avg dice: 0.42867812514305115\n",
      "lr: 8.8473585e-05\n",
      "ckpt.step: 1234\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1234 Epoch 1- Batch[1234/11618], Train Avg Loss: 0.5156965851783752, Train Avg dice: 0.42862576246261597\n",
      "lr: 8.8473585e-05\n",
      "ckpt.step: 1235\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1235 Epoch 1- Batch[1235/11618], Train Avg Loss: 0.5156276226043701, Train Avg dice: 0.4285825490951538\n",
      "lr: 8.8473585e-05\n",
      "ckpt.step: 1236\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1236 Epoch 1- Batch[1236/11618], Train Avg Loss: 0.5155607461929321, Train Avg dice: 0.42853641510009766\n",
      "lr: 8.8473585e-05\n",
      "ckpt.step: 1237\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1237 Epoch 1- Batch[1237/11618], Train Avg Loss: 0.5154930353164673, Train Avg dice: 0.42849522829055786\n",
      "lr: 8.8473585e-05\n",
      "ckpt.step: 1238\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1238 Epoch 1- Batch[1238/11618], Train Avg Loss: 0.515427827835083, Train Avg dice: 0.42846331000328064\n",
      "lr: 8.8473585e-05\n",
      "ckpt.step: 1239\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1239 Epoch 1- Batch[1239/11618], Train Avg Loss: 0.5153630375862122, Train Avg dice: 0.4284416139125824\n",
      "lr: 8.8473585e-05\n",
      "ckpt.step: 1240\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1240 Epoch 1- Batch[1240/11618], Train Avg Loss: 0.5153007507324219, Train Avg dice: 0.42842575907707214\n",
      "lr: 8.8473585e-05\n",
      "ckpt.step: 1241\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1241 Epoch 1- Batch[1241/11618], Train Avg Loss: 0.5152387619018555, Train Avg dice: 0.4284181594848633\n",
      "lr: 8.8473585e-05\n",
      "ckpt.step: 1242\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1242 Epoch 1- Batch[1242/11618], Train Avg Loss: 0.5151753425598145, Train Avg dice: 0.42840850353240967\n",
      "lr: 8.8473585e-05\n",
      "ckpt.step: 1243\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1243 Epoch 1- Batch[1243/11618], Train Avg Loss: 0.5151129961013794, Train Avg dice: 0.42840248346328735\n",
      "lr: 8.8473585e-05\n",
      "ckpt.step: 1244\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1244 Epoch 1- Batch[1244/11618], Train Avg Loss: 0.5150492787361145, Train Avg dice: 0.4283955693244934\n",
      "lr: 8.8473585e-05\n",
      "ckpt.step: 1245\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1245 Epoch 1- Batch[1245/11618], Train Avg Loss: 0.5149853825569153, Train Avg dice: 0.4283897876739502\n",
      "lr: 8.8473585e-05\n",
      "ckpt.step: 1246\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1246 Epoch 1- Batch[1246/11618], Train Avg Loss: 0.5149222612380981, Train Avg dice: 0.42838597297668457\n",
      "lr: 8.8473585e-05\n",
      "ckpt.step: 1247\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1247 Epoch 1- Batch[1247/11618], Train Avg Loss: 0.5148592591285706, Train Avg dice: 0.4283851981163025\n",
      "lr: 8.8473585e-05\n",
      "ckpt.step: 1248\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1248 Epoch 1- Batch[1248/11618], Train Avg Loss: 0.5147964954376221, Train Avg dice: 0.428387314081192\n",
      "lr: 8.8473585e-05\n",
      "ckpt.step: 1249\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1249 Epoch 1- Batch[1249/11618], Train Avg Loss: 0.5147322416305542, Train Avg dice: 0.42838913202285767\n",
      "lr: 8.8473585e-05\n",
      "ckpt.step: 1250\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1250 Epoch 1- Batch[1250/11618], Train Avg Loss: 0.5146663784980774, Train Avg dice: 0.42839404940605164\n",
      "lr: 8.8473585e-05\n",
      "ckpt.step: 1251\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1251 Epoch 1- Batch[1251/11618], Train Avg Loss: 0.5145983099937439, Train Avg dice: 0.4283961057662964\n",
      "lr: 8.8473585e-05\n",
      "ckpt.step: 1252\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1252 Epoch 1- Batch[1252/11618], Train Avg Loss: 0.5145284533500671, Train Avg dice: 0.42840054631233215\n",
      "lr: 8.8473585e-05\n",
      "ckpt.step: 1253\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1253 Epoch 1- Batch[1253/11618], Train Avg Loss: 0.5144594311714172, Train Avg dice: 0.42840513586997986\n",
      "lr: 8.8473585e-05\n",
      "ckpt.step: 1254\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1254 Epoch 1- Batch[1254/11618], Train Avg Loss: 0.514390230178833, Train Avg dice: 0.4284093677997589\n",
      "lr: 8.8473585e-05\n",
      "ckpt.step: 1255\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1255 Epoch 1- Batch[1255/11618], Train Avg Loss: 0.514320433139801, Train Avg dice: 0.4284122884273529\n",
      "lr: 8.8473585e-05\n",
      "ckpt.step: 1256\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1256 Epoch 1- Batch[1256/11618], Train Avg Loss: 0.5142489671707153, Train Avg dice: 0.4284154176712036\n",
      "lr: 8.8473585e-05\n",
      "ckpt.step: 1257\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1257 Epoch 1- Batch[1257/11618], Train Avg Loss: 0.5141766667366028, Train Avg dice: 0.4284188151359558\n",
      "lr: 8.8473585e-05\n",
      "ckpt.step: 1258\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1258 Epoch 1- Batch[1258/11618], Train Avg Loss: 0.5141016840934753, Train Avg dice: 0.4284224510192871\n",
      "lr: 8.8473585e-05\n",
      "ckpt.step: 1259\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1259 Epoch 1- Batch[1259/11618], Train Avg Loss: 0.514024019241333, Train Avg dice: 0.42842477560043335\n",
      "lr: 8.8473585e-05\n",
      "ckpt.step: 1260\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1260 Epoch 1- Batch[1260/11618], Train Avg Loss: 0.5139448642730713, Train Avg dice: 0.4284279942512512\n",
      "lr: 8.8473585e-05\n",
      "ckpt.step: 1261\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1261 Epoch 1- Batch[1261/11618], Train Avg Loss: 0.5138642191886902, Train Avg dice: 0.4284297227859497\n",
      "lr: 8.8473585e-05\n",
      "ckpt.step: 1262\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1262 Epoch 1- Batch[1262/11618], Train Avg Loss: 0.5137819647789001, Train Avg dice: 0.42842990159988403\n",
      "lr: 8.8473585e-05\n",
      "ckpt.step: 1263\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1263 Epoch 1- Batch[1263/11618], Train Avg Loss: 0.5137000679969788, Train Avg dice: 0.4284294545650482\n",
      "lr: 8.8473585e-05\n",
      "ckpt.step: 1264\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1264 Epoch 1- Batch[1264/11618], Train Avg Loss: 0.5136173367500305, Train Avg dice: 0.4284283220767975\n",
      "lr: 8.8473585e-05\n",
      "ckpt.step: 1265\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1265 Epoch 1- Batch[1265/11618], Train Avg Loss: 0.5135335922241211, Train Avg dice: 0.4284263253211975\n",
      "lr: 8.8473585e-05\n",
      "ckpt.step: 1266\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1266 Epoch 1- Batch[1266/11618], Train Avg Loss: 0.513447642326355, Train Avg dice: 0.42842206358909607\n",
      "lr: 8.8473585e-05\n",
      "ckpt.step: 1267\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1267 Epoch 1- Batch[1267/11618], Train Avg Loss: 0.5133602023124695, Train Avg dice: 0.4284168481826782\n",
      "lr: 8.8473585e-05\n",
      "ckpt.step: 1268\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1268 Epoch 1- Batch[1268/11618], Train Avg Loss: 0.5132719874382019, Train Avg dice: 0.4284121096134186\n",
      "lr: 8.8473585e-05\n",
      "ckpt.step: 1269\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1269 Epoch 1- Batch[1269/11618], Train Avg Loss: 0.5131826400756836, Train Avg dice: 0.42840632796287537\n",
      "lr: 8.8473585e-05\n",
      "ckpt.step: 1270\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1270 Epoch 1- Batch[1270/11618], Train Avg Loss: 0.5130919218063354, Train Avg dice: 0.42839762568473816\n",
      "lr: 8.8473585e-05\n",
      "ckpt.step: 1271\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1271 Epoch 1- Batch[1271/11618], Train Avg Loss: 0.5130003690719604, Train Avg dice: 0.4283873736858368\n",
      "lr: 8.8473585e-05\n",
      "ckpt.step: 1272\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1272 Epoch 1- Batch[1272/11618], Train Avg Loss: 0.5129092335700989, Train Avg dice: 0.4283756613731384\n",
      "lr: 8.8473585e-05\n",
      "ckpt.step: 1273\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1273 Epoch 1- Batch[1273/11618], Train Avg Loss: 0.5128199458122253, Train Avg dice: 0.4283653497695923\n",
      "lr: 8.8473585e-05\n",
      "ckpt.step: 1274\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1274 Epoch 1- Batch[1274/11618], Train Avg Loss: 0.5127310156822205, Train Avg dice: 0.4283571243286133\n",
      "lr: 8.8473585e-05\n",
      "ckpt.step: 1275\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1275 Epoch 1- Batch[1275/11618], Train Avg Loss: 0.5126424431800842, Train Avg dice: 0.4283512234687805\n",
      "lr: 8.8473585e-05\n",
      "ckpt.step: 1276\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1276 Epoch 1- Batch[1276/11618], Train Avg Loss: 0.5125510096549988, Train Avg dice: 0.42834052443504333\n",
      "lr: 8.8473585e-05\n",
      "ckpt.step: 1277\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1277 Epoch 1- Batch[1277/11618], Train Avg Loss: 0.5124573111534119, Train Avg dice: 0.42832866311073303\n",
      "lr: 8.8473585e-05\n",
      "ckpt.step: 1278\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1278 Epoch 1- Batch[1278/11618], Train Avg Loss: 0.5123628377914429, Train Avg dice: 0.4283181130886078\n",
      "lr: 8.8473585e-05\n",
      "ckpt.step: 1279\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1279 Epoch 1- Batch[1279/11618], Train Avg Loss: 0.5122681856155396, Train Avg dice: 0.42830708622932434\n",
      "lr: 8.8473585e-05\n",
      "ckpt.step: 1280\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n",
      "Step: 1280 Epoch 1- Batch[1280/11618], Train Avg Loss: 0.5121742486953735, Train Avg dice: 0.4282987713813782\n",
      "lr: 8.8473585e-05\n",
      "ckpt.step: 1281\n",
      "x.shape: (2, 512, 512, 1)\n",
      "y.shape: (2, 512, 512, 1)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-a2482ce41e15>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0mopt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr_schedule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     train_and_checkpoint(train_dataset, model, EPOCHS, opt=opt, train_summary_writer=train_summary_writer,\n\u001b[0;32m---> 33\u001b[0;31m                          test_summary_writer=test_summary_writer, graph_writer=graph_writer, ckpt=ckpt, manager=manager)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-11-348822825d2c>\u001b[0m in \u001b[0;36mtrain_and_checkpoint\u001b[0;34m(train_dataset, model, EPOCHS, opt, train_summary_writer, test_summary_writer, graph_writer, ckpt, ckp_freq, manager)\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"y.shape:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0mwrite_tb_logs_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_summary_writer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"input_image\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m             \u001b[0mwrite_tb_logs_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_summary_writer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"input_target\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m             \u001b[0mbatch_count\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0;31m# print(x.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-38f0b5aab622>\u001b[0m in \u001b[0;36mwrite_tb_logs_image\u001b[0;34m(writer, name_list, value_list, step, max_outs)\u001b[0m\n\u001b[1;32m      6\u001b[0m             \u001b[0;31m# batch_images = np.expand_dims(value_list[i], -1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m             \u001b[0;31m# print(batch_images.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m             \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_outs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m             \u001b[0;31m# value_list[i].reset_states()  # Clear accumulated values with .reset_states()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/TF21Wio/lib/python3.7/site-packages/tensorboard/plugins/image/summary_v2.py\u001b[0m in \u001b[0;36mimage\u001b[0;34m(name, data, step, max_outputs, description)\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;31m# are written, we pass callable to `tensor` parameter.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m     return tf.summary.write(\n\u001b[0;32m---> 97\u001b[0;31m         tag=tag, tensor=lazy_tensor, step=step, metadata=summary_metadata)\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/TF21Wio/lib/python3.7/site-packages/tensorflow_core/python/ops/summary_ops_v2.py\u001b[0m in \u001b[0;36mwrite\u001b[0;34m(tag, tensor, step, metadata, name)\u001b[0m\n\u001b[1;32m    666\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    667\u001b[0m     op = smart_cond.smart_cond(\n\u001b[0;32m--> 668\u001b[0;31m         _should_record_summaries_v2(), record, _nothing, name=\"summary_cond\")\n\u001b[0m\u001b[1;32m    669\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    670\u001b[0m       \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_to_collection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGraphKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_SUMMARY_COLLECTION\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/TF21Wio/lib/python3.7/site-packages/tensorflow_core/python/framework/smart_cond.py\u001b[0m in \u001b[0;36msmart_cond\u001b[0;34m(pred, true_fn, false_fn, name)\u001b[0m\n\u001b[1;32m     52\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mpred_value\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpred_value\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtrue_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mfalse_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/TF21Wio/lib/python3.7/site-packages/tensorflow_core/python/ops/summary_ops_v2.py\u001b[0m in \u001b[0;36mrecord\u001b[0;34m()\u001b[0m\n\u001b[1;32m    653\u001b[0m       \u001b[0;31m# Note the identity to move the tensor to the CPU.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    654\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cpu:0\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 655\u001b[0;31m         summary_tensor = tensor() if callable(tensor) else array_ops.identity(\n\u001b[0m\u001b[1;32m    656\u001b[0m             tensor)\n\u001b[1;32m    657\u001b[0m         write_summary_op = gen_summary_ops.write_summary(\n",
      "\u001b[0;32m~/anaconda3/envs/TF21Wio/lib/python3.7/site-packages/tensorboard/util/lazy_tensor_creator.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tensor\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_CALL_IN_PROGRESS_SENTINEL\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tensor_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/TF21Wio/lib/python3.7/site-packages/tensorboard/plugins/image/summary_v2.py\u001b[0m in \u001b[0;36mlazy_tensor\u001b[0;34m()\u001b[0m\n\u001b[1;32m     81\u001b[0m       encoded_images = tf.map_fn(tf.image.encode_png, limited_images,\n\u001b[1;32m     82\u001b[0m                                  \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m                                  name='encode_each_image')\n\u001b[0m\u001b[1;32m     84\u001b[0m       \u001b[0;31m# Workaround for map_fn returning float dtype for an empty elems input.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m       encoded_images = tf.cond(\n",
      "\u001b[0;32m~/anaconda3/envs/TF21Wio/lib/python3.7/site-packages/tensorflow_core/python/ops/map_fn.py\u001b[0m in \u001b[0;36mmap_fn\u001b[0;34m(fn, elems, dtype, parallel_iterations, back_prop, swap_memory, infer_shape, name)\u001b[0m\n\u001b[1;32m    266\u001b[0m         \u001b[0mback_prop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mback_prop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m         \u001b[0mswap_memory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mswap_memory\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 268\u001b[0;31m         maximum_iterations=n)\n\u001b[0m\u001b[1;32m    269\u001b[0m     \u001b[0mresults_flat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mr_a\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/TF21Wio/lib/python3.7/site-packages/tensorflow_core/python/ops/control_flow_ops.py\u001b[0m in \u001b[0;36mwhile_loop\u001b[0;34m(cond, body, loop_vars, shape_invariants, parallel_iterations, back_prop, swap_memory, name, maximum_iterations, return_same_structure)\u001b[0m\n\u001b[1;32m   2712\u001b[0m                                               list(loop_vars))\n\u001b[1;32m   2713\u001b[0m       \u001b[0;32mwhile\u001b[0m \u001b[0mcond\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mloop_vars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2714\u001b[0;31m         \u001b[0mloop_vars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mloop_vars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2715\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtry_to_pack\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloop_vars\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_basetuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2716\u001b[0m           \u001b[0mpacked\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/TF21Wio/lib/python3.7/site-packages/tensorflow_core/python/ops/control_flow_ops.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(i, lv)\u001b[0m\n\u001b[1;32m   2703\u001b[0m         cond = lambda i, lv: (  # pylint: disable=g-long-lambda\n\u001b[1;32m   2704\u001b[0m             math_ops.logical_and(i < maximum_iterations, orig_cond(*lv)))\n\u001b[0;32m-> 2705\u001b[0;31m         \u001b[0mbody\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlv\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morig_body\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mlv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2706\u001b[0m       \u001b[0mtry_to_pack\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2707\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/TF21Wio/lib/python3.7/site-packages/tensorflow_core/python/ops/map_fn.py\u001b[0m in \u001b[0;36mcompute\u001b[0;34m(i, tas)\u001b[0m\n\u001b[1;32m    255\u001b[0m       \"\"\"\n\u001b[1;32m    256\u001b[0m       \u001b[0mpacked_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_pack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0melem_ta\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0melem_ta\u001b[0m \u001b[0;32min\u001b[0m \u001b[0melems_ta\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m       \u001b[0mpacked_fn_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpacked_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m       \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massert_same_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0melems\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpacked_fn_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m       \u001b[0mflat_fn_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput_flatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpacked_fn_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/TF21Wio/lib/python3.7/site-packages/tensorflow_core/python/ops/gen_image_ops.py\u001b[0m in \u001b[0;36mencode_png\u001b[0;34m(image, compression, name)\u001b[0m\n\u001b[1;32m   1637\u001b[0m       _result = _pywrap_tensorflow.TFE_Py_FastPathExecute(\n\u001b[1;32m   1638\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_context_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"EncodePng\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1639\u001b[0;31m         tld.op_callbacks, image, \"compression\", compression)\n\u001b[0m\u001b[1;32m   1640\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1641\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    \n",
    "    strategy = tf.distribute.MirroredStrategy()\n",
    "    # with strategy.scope():\n",
    "\n",
    "    # use designed model\n",
    "    model = U_NetV2()\n",
    "    # plot model graph\n",
    "    # tf.keras.utils.plot_model(model, show_shapes=True, dpi=200, expand_nested=True)\n",
    "    tb_log_root = \"logs\"\n",
    "    ckpt = tf.train.Checkpoint(step=tf.Variable(1), net=model)\n",
    "    manager = tf.train.CheckpointManager(ckpt, ckp_log_root, max_to_keep=3)\n",
    "\n",
    "    if not os.path.exists(tb_log_root):\n",
    "        print(\"build tensorboard log folder\")\n",
    "        os.makedirs(tb_log_root)\n",
    "    train_summary_writer = tf.summary.create_file_writer(os.path.join(tb_log_root, 'train')) # tensorboard --logdir /tmp/summaries\n",
    "    test_summary_writer = tf.summary.create_file_writer(os.path.join(tb_log_root, 'test'))\n",
    "    graph_writer = tf.summary.create_file_writer(os.path.join(tb_log_root, 'graph'))\n",
    "\n",
    "    initial_learning_rate = 1e-4\n",
    "    lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "        initial_learning_rate,\n",
    "        decay_steps=400,  # learning rate decay after every 100 steps\n",
    "        decay_rate=0.96, #\n",
    "        staircase=True)\n",
    "    opt = tf.keras.optimizers.Adam(lr_schedule)\n",
    "    train_and_checkpoint(train_dataset, model, EPOCHS, opt=opt, train_summary_writer=train_summary_writer,\n",
    "                         test_summary_writer=test_summary_writer, graph_writer=graph_writer, ckpt=ckpt, manager=manager)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
