{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pydicom in e:\\projects\\intepreters\\anaconda\\envs\\tf21\\lib\\site-packages (1.4.2)\n",
      "Requirement already satisfied: xlrd in e:\\projects\\intepreters\\anaconda\\envs\\tf21\\lib\\site-packages (1.2.0)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "!pip install pydicom\n",
    "import pydicom\n",
    "from IPython import display\n",
    "!pip install xlrd\n",
    "import xlrd\n",
    "# check generated positive samples\n",
    "import tensorflow_io as tfio\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from copy import copy\n",
    "import matplotlib.colors as colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def _bytes_feature(value):\n",
    "  \"\"\"Returns a bytes_list from a string / byte.\"\"\"\n",
    "  if isinstance(value, type(tf.constant(0))):\n",
    "    value = value.numpy() # BytesList won't unpack a string from an EagerTensor.\n",
    "  return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "def _float_feature(value):\n",
    "  \"\"\"Returns a float_list from a float / double.\"\"\"\n",
    "  return tf.train.Feature(float_list=tf.train.FloatList(value=[value]))\n",
    "\n",
    "def _int64_feature(value):\n",
    "  \"\"\"Returns an int64_list from a bool / enum / int / uint.\"\"\"\n",
    "  return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n",
    "\n",
    "\n",
    "# Create a dictionary with features that may be relevant. # create tf.example message\n",
    "def image_example(each_dicom_path, seg_label, cls_label):\n",
    "    feature = {\n",
    "      \n",
    "        'dicom_path': _bytes_feature(each_dicom_path.encode()),\n",
    "        'seg_label': _bytes_feature(seg_label.tostring()), \n",
    "        'cls_label': _int64_feature(cls_label),\n",
    "        \n",
    "    }\n",
    "\n",
    "    return tf.train.Example(features=tf.train.Features(feature=feature))\n",
    "\n",
    "\n",
    "def image_pair_example(each_dicom_path1, each_dicom_path2, each_dicom_path3, seg_label1, seg_label2, seg_label3, cls_label1, cls_label2,  cls_label3):\n",
    "    feature = {\n",
    "      \n",
    "        'dicom_path1': _bytes_feature(each_dicom_path1.encode()),\n",
    "        'seg_label1': _bytes_feature(seg_label1.tostring()), \n",
    "        'cls_label1': _int64_feature(cls_label1),\n",
    "        \n",
    "        'dicom_path2': _bytes_feature(each_dicom_path2.encode()),\n",
    "        'seg_label2': _bytes_feature(seg_label2.tostring()), \n",
    "        'cls_label2': _int64_feature(cls_label2),\n",
    "        \n",
    "        'dicom_path3': _bytes_feature(each_dicom_path3.encode()),\n",
    "        'seg_label3': _bytes_feature(seg_label3.tostring()), \n",
    "        'cls_label3': _int64_feature(cls_label3),\n",
    "        \n",
    "    }\n",
    "\n",
    "    return tf.train.Example(features=tf.train.Features(feature=feature))\n",
    "\n",
    "def make_sample_pairs(root_folder_path_for_cases, excel_loc, train_or_val, marked_datatype= np.ubyte):\n",
    "    # get case_list\n",
    "    case_list = glob(root_folder_path_for_cases + \"*\")\n",
    "    # print(\"case_list:\", case_list)\n",
    "    print(\"len of case_list:\", len(case_list))\n",
    "    \n",
    "    \n",
    "    # get work_book of excel\n",
    "    train_wb = xlrd.open_workbook(excel_loc)\n",
    "    # get train filp colomn\n",
    "    sheet =  train_wb.sheet_by_index(0)\n",
    "    print(\"sheet:\", sheet)\n",
    "\n",
    "    # extract_number_of rows\n",
    "    print(sheet.nrows)\n",
    "    \n",
    "    case_names =  []\n",
    "    case_flips = []\n",
    "    for i in range(1, sheet.nrows):\n",
    "        case_names.append(sheet.cell_value(i, 0))\n",
    "        case_flips.append(sheet.cell_value(i, 1))\n",
    "\n",
    "    \n",
    "    # convert flip notes into integer \n",
    "    print(\"case_flips:\", case_flips)\n",
    "    case_flips =[int(s) for s in case_flips] \n",
    "    print(\"case_names:\", case_names)\n",
    "    print(\"case_flips:\", case_flips)\n",
    "    print(\"len of case data:\", len(case_names))\n",
    "    print(\"len of flip data:\", len(case_flips))\n",
    "\n",
    "    # check case list\n",
    "    print(\"case list len:\", len(case_list))\n",
    "    assert len(case_names) ==  len(case_flips), \" len of cases names does not macthed with flip len in train excel \"\n",
    "    assert len(case_list) ==  len(case_names), \" len of cases in the train folder not macthed with cases in train excel \"\n",
    "    \n",
    "    # one feature contains : input 1 , seg1, cls1, input 2, seg2, cls2, input3, seg3, cls3 with shape [1, 9]\n",
    "    \n",
    "    positive_sample_features =  []\n",
    "    negative_sample_features =  []\n",
    "    \n",
    "    \n",
    "    # loop each case folder to select samples\n",
    "    for case_index, each_case in enumerate(case_list):\n",
    "        display.clear_output(wait=True)\n",
    "        print(\"case index:{}, case:{}\".format(case_index, each_case))\n",
    "        each_case_str = os.path.split(each_case) \n",
    "        print(\"each_case_str:\", each_case_str)\n",
    "        case_name =  each_case_str[-1]\n",
    "        print(\"case name:\", case_name)\n",
    "        print(\"case index:\", case_index, \"case name:\", case_names[case_index])\n",
    "        # check case name match or not\n",
    "        assert case_name ==  case_names[case_index]\n",
    "        \n",
    "        # get dicom files and label file\n",
    "        # read_labels for specified case name\n",
    "        # read rawfile to get seg labels for the entire case: ----------------------->  for seg label \n",
    "        raw_files = glob(each_case + \"/*.raw\")\n",
    "        # get label path\n",
    "        for each_path in raw_files:\n",
    "            if \"marked\" in each_path:\n",
    "                label_path =  each_path\n",
    "        print(\"label path:\", label_path)\n",
    "        # read labels\n",
    "        labels = np.fromfile(label_path, dtype = marked_datatype)\n",
    "        reshaped_labels =  labels.reshape([-1, 512, 512])\n",
    "        print(\"reshape labels shape:\", reshaped_labels.shape)\n",
    "#\n",
    "\n",
    "       # check whether needs to flip the label\n",
    "        if case_flips[case_index] ==1: # flip the enitre label\n",
    "            final_case_labels = np.flip(reshaped_labels, 0)\n",
    "        else: \n",
    "            final_case_labels =  reshaped_labels\n",
    "        \n",
    "        \n",
    "        step=3 # number of elements for each sample\n",
    "        \n",
    "        # rearrange the label array ever 3 elemtns step 1\n",
    "        rearrange_3_seg_labels_list = []\n",
    "        \n",
    "       \n",
    "        for i in range(0, final_case_labels.shape[0]-step+1):\n",
    "            rearrange_3_seg_labels_list.append(final_case_labels[i:i+step])\n",
    "        print(\"rearrange_3labels_list len---->:\", len(rearrange_3_seg_labels_list))\n",
    "        final_seg_label_list = rearrange_3_seg_labels_list\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        # get dicom list...-------------------------------->   for dicom input path\n",
    "        dicom_files = glob(each_case + \"/*.DCM\")\n",
    "        \n",
    "        # in order for ubuntu system, needs to rewrite the path of dicom\n",
    "        dicom_path_list =[]\n",
    "        cls_list=[]\n",
    "        for dicom_index, each_dicom in enumerate(dicom_files):\n",
    "            path_split = os.path.split(each_dicom)\n",
    "#             print(path_split)\n",
    "            file_name =  path_split[-1]\n",
    "#             print(file_name)\n",
    "\n",
    "\n",
    "            each_slice_label =  final_case_labels[dicom_index]\n",
    "            # count non_zeros value from the each slice label\n",
    "            num_nonzero = np.count_nonzero(each_slice_label)\n",
    "\n",
    "            if train_or_val == \"train\": # change replace paths considering for ubuntu path root changed\n",
    "                each_dicom =  \"E:\\\\dataset\\\\Leisang\\\\myTry\\\\BleedingDataDCM\\\\train/\" + case_name +\"/\" +file_name\n",
    "            else:\n",
    "                each_dicom =  \"E:\\\\dataset\\\\Leisang\\\\myTry\\\\BleedingDataDCM\\\\val/\" + case_name+ \"/\" +file_name\n",
    "\n",
    "            if num_nonzero > 10:  # only consider the # of nonzeros values >10 to be the positive labels\n",
    "                each_slice_cls_label = 1\n",
    "                print(\"num_nonzero:\", num_nonzero)\n",
    "#                 None_zero_label_index.append(dicom_index+1)\n",
    "                print(\"each_slice_label shape:\", each_slice_label.shape)\n",
    "                print(\"each_slice label range before binarization: [{}, {}]\".format(each_slice_label.min(), each_slice_label.max()))\n",
    "\n",
    "                # binarize the mask label\n",
    "                each_slice_label[each_slice_label> 0] = 255\n",
    "                print(\"each_slice label range before after binarization: [{}, {}]\".format(each_slice_label.min(), each_slice_label.max()))\n",
    "            else: # negative examples\n",
    "                each_slice_cls_label = 0\n",
    "            \n",
    "            if dicom_index % 20 == 0:\n",
    "                    display.clear_output(wait=True)\n",
    "            \n",
    "            dicom_path_list.append(each_dicom)\n",
    "            cls_list.append(each_slice_cls_label)\n",
    "        \n",
    "        # rearrange dicom path list and cls_list into 3 elements per group\n",
    "        final_dicom_path_list = [dicom_path_list[i:i+step] for i in range(0, len(dicom_path_list)-step+1)]\n",
    "        final_cls_list = [cls_list[i:i+step] for i in range(0, len(cls_list)-step+1)]\n",
    "        \n",
    "        print(\"final_dicom_path_list:\", len(final_dicom_path_list))\n",
    "#         print(final_dicom_path_list)\n",
    "        print(\"final_cls_list:\", len(final_cls_list))\n",
    "#         print(final_cls_list)\n",
    "        \n",
    "#         # reform the dicom list into 3 elements per step\n",
    "      \n",
    "#         dicom_3_files_list = [dicom_files[i:i+step] for i in range(0, len(dicom_files)-step+1)]\n",
    "#         print(\"dicom_3_files_list len---->:\", len(dicom_3_files_list))\n",
    "        print(\"dicom_files[0]:\", dicom_files[0])\n",
    "        print(\"final_dicom_path_list[0][0]:\", final_dicom_path_list[0][0])\n",
    "        print(\"dicom_files[-1]:\", dicom_files[-1])\n",
    "        print(\"final_dicom_path_list[-1][-1]:\", final_dicom_path_list[-1][-1])\n",
    "        print(\"final_dicom_path_list[0]:\", final_dicom_path_list[0])\n",
    "        print(\"final_dicom_path_list[1]:\", final_dicom_path_list[1])\n",
    "        \n",
    "        print()\n",
    "        print(\"final case label shape:\", final_case_labels.shape)\n",
    "        assert os.path.normpath(dicom_files[0]) == os.path.normpath(final_dicom_path_list[0][0])  # Use os.path.normpath to convert c:/fold1/fold2 to c:\\fold1\\fold2\n",
    "        assert os.path.normpath (dicom_files[-1]) == os.path.normpath(final_dicom_path_list[-1][-1])\n",
    "        print(\"len of dicoms in this case:\", len(dicom_files))\n",
    "        assert  reshaped_labels.shape[0] ==  len(dicom_files), 'len of dicoms is not equal to the raw label values'\n",
    "        print()    \n",
    "        \n",
    "        # pack the data with as [(inputs1, seg_tar1, cls_tar1) , （inputs2, seg_tar2, cls_tar2）， (inputs3, seg_tar3, cls_tar3)] \n",
    "        final_samples =  zip(final_dicom_path_list, final_seg_label_list, final_cls_list)\n",
    "#         print(\"len of case samples:\", list(final_samples))\n",
    "        final_case_samples_list =  list(final_samples)\n",
    "#         print(\"final_case_samples_list[0]:\",  final_case_samples_list[0]) \n",
    "        # Consider add_positive samples and negative samples\n",
    "        for i in range(len(final_cls_list)):\n",
    "            if i  % 20 == 0:\n",
    "                display.clear_output(wait=True)\n",
    "            print(\"i:\",i)\n",
    "            print(\"final_cls_list[i]:\", final_cls_list[i])\n",
    "#             print(\"final_clas_list[i]:\", final_cls_list[i])\n",
    "            num_non_zero_clses = np.count_nonzero(final_cls_list[i])\n",
    "            print(\"num_non_zero_clses\", num_non_zero_clses)\n",
    "            if num_non_zero_clses >1:\n",
    "#                 print(\"final_case_samples_list[i]:\", final_case_samples_list[i])\n",
    "                positive_sample_features.append(final_case_samples_list[i])\n",
    "            else: \n",
    "#                 print(\"final_case_samples_list[i]:\", final_case_samples_list[i])\n",
    "                negative_sample_features.append(final_case_samples_list[i])\n",
    "            \n",
    "           \n",
    "                \n",
    "    print(\"total positive samples:\",len(positive_sample_features))\n",
    "    print(\"total negative samples:\",len(negative_sample_features))            \n",
    "    return positive_sample_features, negative_sample_features\n",
    "        \n",
    "# def make_samples_win(root_folder_path_for_cases, excel_loc, train_or_val, marked_datatype= np.ubyte):\n",
    "    \n",
    "    \n",
    "#     # get case_list\n",
    "#     case_list = glob(root_folder_path_for_cases + \"*\")\n",
    "#     # print(\"case_list:\", case_list)\n",
    "#     print(\"len of case_list:\", len(case_list))\n",
    "    \n",
    "    \n",
    "#     # get work_book of excel\n",
    "#     train_wb = xlrd.open_workbook(excel_loc)\n",
    "#     # get train filp colomn\n",
    "#     sheet =  train_wb.sheet_by_index(0)\n",
    "#     print(\"sheet:\", sheet)\n",
    "\n",
    "#     # extract_number_of rows\n",
    "#     print(sheet.nrows)\n",
    "    \n",
    "#     case_names =  []\n",
    "#     case_flips = []\n",
    "#     for i in range(1, sheet.nrows):\n",
    "#         case_names.append(sheet.cell_value(i, 0))\n",
    "#         case_flips.append(sheet.cell_value(i, 1))\n",
    "\n",
    "#     # convert flip notes into integer \n",
    "#     print(\"case_flips:\", case_flips)\n",
    "#     case_flips =[int(s) for s in case_flips] \n",
    "#     print(\"case_names:\", case_names)\n",
    "#     print(\"case_flips:\", case_flips)\n",
    "#     print(\"len of case data:\", len(case_names))\n",
    "#     print(\"len of flip data:\", len(case_flips))\n",
    "\n",
    "#     # check case list\n",
    "#     print(\"case list len:\", len(case_list))\n",
    "#     assert len(case_names) ==  len(case_flips), \" len of cases names does not macthed with flip len in train excel \"\n",
    "#     assert len(case_list) ==  len(case_names), \" len of cases in the train folder not macthed with cases in train excel \"\n",
    "\n",
    "#     my_positive_exmaple ={}\n",
    "#     my_negative_exmaple={}\n",
    "    \n",
    "#     for case_index, each_case in enumerate(case_list):\n",
    "#         print(each_case)\n",
    "#         # get case name from case list:\n",
    "#         each_case_str = os.path.split(each_case) \n",
    "#         print(\"each_case_str:\", each_case_str)\n",
    "#         case_name =  each_case_str[-1]\n",
    "#         print(\"case name:\", case_name)\n",
    "#         print(\"case index:\", case_index, \"case name:\", case_names[case_index])\n",
    "#         # check case name match or not\n",
    "#         assert case_name ==  case_names[case_index]\n",
    "\n",
    "#         # read_labels for specified case name\n",
    "#         raw_files = glob(each_case + \"/*.raw\")\n",
    "#         dicom_files = glob(each_case + \"/*.DCM\")\n",
    "        \n",
    "        \n",
    "#         # get label path\n",
    "#         for each_path in raw_files:\n",
    "#             if \"marked\" in each_path:\n",
    "#                 label_path =  each_path\n",
    "#         print(\"label path:\", label_path)\n",
    "#         # read labels\n",
    "#         labels = np.fromfile(label_path, dtype = marked_datatype)\n",
    "#         reshaped_labels =  labels.reshape([-1, 512, 512])\n",
    "#         print(\"reshape labels shape:\", reshaped_labels.shape)\n",
    "#         print(\"len of dicoms in this case:\", len(dicom_files))\n",
    "#         assert  reshaped_labels.shape[0] ==  len(dicom_files)  , 'len of dicoms is not equal to the raw label values'\n",
    "\n",
    "#        # check whether needs to flip the label\n",
    "#         if case_flips[case_index] ==1: # flip the enitre label\n",
    "#             train_final_case_labels = np.flip(reshaped_labels, 0)\n",
    "#         else: \n",
    "#             train_final_case_labels =  reshaped_labels\n",
    "\n",
    "#         None_zero_label_index = []\n",
    "#         # read inputs\n",
    "#         for dicom_index, each_dicom in enumerate(dicom_files):\n",
    "#             path_split = os.path.split(each_dicom)\n",
    "#             print(path_split)\n",
    "#             file_name =  path_split[-1]\n",
    "#             print(file_name)\n",
    "            \n",
    "           \n",
    "#             each_slice_label =  train_final_case_labels[dicom_index]\n",
    "#             # count non_zeros value from the each slice label\n",
    "#             num_nonzero = np.count_nonzero(each_slice_label)\n",
    "            \n",
    "#             if train_or_val == \"train\": # change replace paths considering for ubuntu path root changed\n",
    "#                 each_dicom =  \"E:\\\\dataset\\\\Leisang\\\\myTry\\\\BleedingDataDCM\\\\train/\" + case_name +\"/\" +file_name\n",
    "#             else:\n",
    "#                 each_dicom =  \"E:\\\\dataset\\\\Leisang\\\\myTry\\\\BleedingDataDCM\\\\val/\" + case_name+ \"/\" +file_name\n",
    "            \n",
    "#             if num_nonzero > 10:  # only consider the # of nonzeros values >10 to be the positive labels\n",
    "#                 each_slice_cls_label = 1\n",
    "#                 print(\"num_nonzero:\", num_nonzero)\n",
    "#                 None_zero_label_index.append(dicom_index+1)\n",
    "#                 print(\"each_slice_label shape:\", each_slice_label.shape)\n",
    "#                 print(\"each_slice label range before binarization: [{}, {}]\".format(each_slice_label.min(), each_slice_label.max()))\n",
    "\n",
    "#                 # binarize the mask label\n",
    "#                 each_slice_label[each_slice_label> 0] = 255\n",
    "#                 print(\"each_slice label range before after binarization: [{}, {}]\".format(each_slice_label.min(), each_slice_label.max()))\n",
    "\n",
    "#                 # genrate positive examples dict = {\"path\"： label}\n",
    "#                 my_positive_exmaple[each_dicom]=(each_slice_label, each_slice_cls_label)\n",
    "\n",
    "#             else: # negative examples\n",
    "#                 each_slice_cls_label = 0\n",
    "#                 my_negative_exmaple[each_dicom]=(each_slice_label, each_slice_cls_label)\n",
    "\n",
    "#             if dicom_index % 20 == 0:\n",
    "#                     display.clear_output(wait=True)  \n",
    "\n",
    "#             if dicom_index == len(dicom_files)-1:\n",
    "#                 print(\"total \" + str(len(dicom_files)) + \" checked ---------------------------------------------------------------------------->\")\n",
    "\n",
    "   \n",
    "#         # check the length for each case\n",
    "#         print(\"generate positive {} positive samples at currently case step:\".format(len(my_positive_exmaple)))\n",
    "#         print(\"generate positive {} negative samples at currently case step:\".format(len(my_negative_exmaple)))   \n",
    "#     return my_positive_exmaple, my_negative_exmaple\n",
    "\n",
    "\n",
    "def Make_InputPairsTFrecords(sample_list, file_nameToSave):\n",
    "    from pathlib import Path\n",
    "    # start to write \n",
    "    print(\"writing tfrecords....\")\n",
    "   \n",
    "    # strat to write postive train dicoms\n",
    "    with tf.io.TFRecordWriter(file_nameToSave) as writer:\n",
    "        print(\"paths sample_list[0][0]:\", sample_list[0][0])\n",
    "        print(\"seg_tars sample_list[0][1]:\", sample_list[0][1].shape)\n",
    "        print(\"cls_tars sample_list[0][2]:\", sample_list[0][2])\n",
    "        for i in range(len(sample_list)):\n",
    "            if i % 10 ==0:\n",
    "                 display.clear_output(wait=True)\n",
    "                    \n",
    "            print(\"sample index:\", i)\n",
    "            one_sample =  sample_list[i]\n",
    "#             print(\"one temp sample:\", one_sample)\n",
    "            # extract feture elements:\n",
    "            # for paths-->:\n",
    "            each_dicom_paths = sample_list[i][0]\n",
    "            \n",
    "            each_dicom_path0 = each_dicom_paths[0]\n",
    "            each_dicom_path1 = each_dicom_paths[1]\n",
    "            each_dicom_path2 = each_dicom_paths[2]\n",
    "            # for seg_tars-->:\n",
    "            each_seg_tars = sample_list[i][1]\n",
    "            \n",
    "            each_seg_tar0 = each_seg_tars[0]\n",
    "            each_seg_tar1 = each_seg_tars[1]\n",
    "            each_seg_tar2 = each_seg_tars[2]\n",
    "            \n",
    "            # for cls_tars-->:\n",
    "            each_cls_tars = sample_list[i][2]\n",
    "            each_cls_tar0 = each_cls_tars[0]\n",
    "            each_cls_tar1 = each_cls_tars[1]\n",
    "            each_cls_tar2 = each_cls_tars[2]\n",
    "            \n",
    "            \n",
    "            # write sample elements into pre-defined proto\n",
    "            tf_example = image_pair_example(each_dicom_path0, each_dicom_path1, each_dicom_path2, \n",
    "                                            each_seg_tar0, each_seg_tar1, each_seg_tar2, \n",
    "                                            each_cls_tar0, each_cls_tar1, each_cls_tar2)  # need image_example function to genearte mesage for writing tf records\n",
    "            writer.write(tf_example.SerializeToString())\n",
    "#             count_display+=1\n",
    "#             if count_display >20:\n",
    "#                 display.clear_output(wait=True)  \n",
    "    print(\"Generation of TFRecords is finished.\")\n",
    "        \n",
    "    \n",
    "#     for dicom_path, labels in feature_samples_dict.items():\n",
    "#         if previous_sample is None:\n",
    "#             previous_sample_sample\n",
    "#         p =  Path(dicom_path)\n",
    "# #         path_split = os.path.split(dicom_path)\n",
    "# #         sub_path_split = os.path.split(path_split[0])\n",
    "# #         case_name =  sub_path_split[-1]\n",
    "# #         file_name = path_split[-1] \n",
    "# #         file_index =  p.stem()\n",
    "# #         print(\"dicom_path:\", dicom_path)\n",
    "# #         print(\"dicom_path_split:\", path_split)\n",
    "# #         print(\"sub_path_split:\", sub_path_split)\n",
    "# #         print(\"case name:\", case_name)\n",
    "# #         print(\"filename:\", file_name)\n",
    "#         print(p.with_suffix(''))\n",
    "#         print(p.stem)\n",
    "#         file_index = int(p.stem)\n",
    "        \n",
    "#         print(\"file index:\", file_index)\n",
    "#         print(\"file index:\", file_index)\n",
    "        \n",
    "#     with tf.io.TFRecordWriter(file_nameToSave) as writer:\n",
    "#         for dicom_path, labels in feature_samples_dict.items():\n",
    "#             print(\"dicom_path:\", dicom_path)\n",
    "#     #         dicom_path_string = open(dicom_path, 'rb').read()\n",
    "#             seg_label = labels[0]\n",
    "#             print(seg_label.shape)\n",
    "#             cls_label = labels[1]\n",
    "# #             tf_example = image_example(dicom_path, seg_label, cls_label)  # need image_example function to genearte mesage for writing tf records\n",
    "# #             writer.write(tf_example.SerializeToString())\n",
    "# #             count_display+=1\n",
    "# #             if count_display >20:\n",
    "# #                 display.clear_output(wait=True)  \n",
    "#     print(\"Generation of TFRecords is finished.\")\n",
    "\n",
    "\n",
    "# def Make_TFRecords(feature_samples_dict, file_nameToSave):\n",
    "#     # start to write \n",
    "#     print(\"writing tfrecords....\")\n",
    "#     count_display = 0\n",
    "#     # strat to write postive train dicoms\n",
    "    \n",
    "#     with tf.io.TFRecordWriter(file_name) as writer:\n",
    "#         for dicom_path, labels in feature_samples_dict.items():\n",
    "#             print(\"dicom_path:\", dicom_path)\n",
    "#     #         dicom_path_string = open(dicom_path, 'rb').read()\n",
    "#             seg_label = labels[0]\n",
    "#             print(seg_label.shape)\n",
    "#             cls_label = labels[1]\n",
    "#             tf_example = image_example(dicom_path, seg_label, cls_label)  # need image_example function to genearte mesage for writing tf records\n",
    "#             writer.write(tf_example.SerializeToString())\n",
    "#             count_display+=1\n",
    "#             if count_display >20:\n",
    "#                 display.clear_output(wait=True)  \n",
    "#     print(\"Generation of TFRecords is finished.\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Train TFRECORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# for train\n",
    "train_root_folder_path_for_cases= \"E:/dataset/Leisang/myTry/BleedingDataDCM/train/\"\n",
    "\n",
    "# give the location of the file\n",
    "train_excel_loc = \"E:\\\\dataset\\\\Leisang\\\\myTry\\\\BleedingDataDCM/TrainfilpOrNot.xlsx\"\n",
    "\n",
    "\n",
    "# make positive and negative samples:\n",
    "\n",
    "# train_positive_samples, train_negative_samples = make_samples_win(root_folder_path_for_cases=train_root_folder_path_for_cases,\n",
    "#                                                               excel_loc = train_excel_loc,\n",
    "#                                                                     train_or_val=\"train\")\n",
    "\n",
    "# make sample pairs: [(inputs1, seg_tar1, cls_tar1) , （inputs2, seg_tar2, cls_tar2）， (inputs3, seg_tar3, cls_tar3)] \n",
    "\n",
    "positive_sample_features_list, negative_sample_features_list =make_sample_pairs(root_folder_path_for_cases=train_root_folder_path_for_cases,\n",
    "                                                              excel_loc = train_excel_loc,\n",
    "                                                                    train_or_val=\"train\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Make into functions for genrerate tf records\n",
    "train_positive_file_name =  \"E:\\\\dataset\\\\Leisang\\\\myTry\\\\BleedingDataDCM/train_positive_sample_pairs_win.tfrecords\"\n",
    "train_negative_file_name =  'E:\\\\dataset\\\\Leisang\\\\myTry\\\\BleedingDataDCM/train_negative_sample_pairs_win.tfrecords'\n",
    "# # make paris dataset for positive\n",
    "# Make_InputPairsTFrecords(positive_sample_features_list,file_nameToSave=train_positive_file_name )\n",
    "\n",
    "# # make paris dataset for negative\n",
    "# Make_InputPairsTFrecords(negative_sample_features_list,file_nameToSave=train_negative_file_name )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# make train_positive\n",
    "# Make_TFRecords(feature_samples_dict=train_positive_samples, file_nameToSave=train_positive_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# make train_negative\n",
    "# Make_TFRecords(feature_samples_dict=train_negative_samples, file_nameToSave=train_negative_file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## checek the generated TRAIN TFRECORDS dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dicom_path: tf.Tensor(b'E:\\\\dataset\\\\Leisang\\\\myTry\\\\BleedingDataDCM\\\\train/ZA-017_001/00001047.DCM', shape=(), dtype=string)\n",
      "input_image.shape (1, 512, 512, 1)\n",
      "seg_label (1, 512, 512, 1)\n",
      "cls_label tf.Tensor(1, shape=(), dtype=int64)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31m_FallbackException\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32mE:\\Projects\\Intepreters\\Anaconda\\envs\\TF21\\lib\\site-packages\\tensorflow_core\\python\\ops\\gen_array_ops.py\u001b[0m in \u001b[0;36mreshape\u001b[1;34m(tensor, shape, name)\u001b[0m\n\u001b[0;32m   7430\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_context_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtld\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Reshape\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 7431\u001b[1;33m         tld.op_callbacks, tensor, shape)\n\u001b[0m\u001b[0;32m   7432\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31m_FallbackException\u001b[0m: This function does not handle the case of the path where all inputs are not already EagerTensors.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-0c8afefbcf02>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    149\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    150\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 151\u001b[1;33m \u001b[0mcheck_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparsed_train_positive_dataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    152\u001b[0m \u001b[0mcheck_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparsed_train_negative_dataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-0c8afefbcf02>\u001b[0m in \u001b[0;36mcheck_dataset\u001b[1;34m(dataset)\u001b[0m\n\u001b[0;32m    143\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mdicom_paths\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseg_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcls_labels\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mparsed_dataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    144\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdicom_paths\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 145\u001b[1;33m             \u001b[0mplot_one_input\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdicom_paths\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseg_labels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcls_labels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    146\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    147\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-0c8afefbcf02>\u001b[0m in \u001b[0;36mplot_one_input\u001b[1;34m(dicom_path, input, seg_label, cls_label)\u001b[0m\n\u001b[0;32m     87\u001b[0m     \u001b[0mnum_nonzeros\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcount_nonzero\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mseg_label\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m     \u001b[1;31m# reshape label\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 89\u001b[1;33m     \u001b[0mtarget\u001b[0m \u001b[1;33m=\u001b[0m  \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mseg_label\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     90\u001b[0m     \u001b[0mfig\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfigsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Projects\\Intepreters\\Anaconda\\envs\\TF21\\lib\\site-packages\\tensorflow_core\\python\\ops\\array_ops.py\u001b[0m in \u001b[0;36mreshape\u001b[1;34m(tensor, shape, name)\u001b[0m\n\u001b[0;32m    191\u001b[0m     \u001b[0mA\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mHas\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msame\u001b[0m \u001b[0mtype\u001b[0m \u001b[1;32mas\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    192\u001b[0m   \"\"\"\n\u001b[1;32m--> 193\u001b[1;33m   \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgen_array_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    194\u001b[0m   \u001b[0mtensor_util\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmaybe_set_static_shape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    195\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Projects\\Intepreters\\Anaconda\\envs\\TF21\\lib\\site-packages\\tensorflow_core\\python\\ops\\gen_array_ops.py\u001b[0m in \u001b[0;36mreshape\u001b[1;34m(tensor, shape, name)\u001b[0m\n\u001b[0;32m   7434\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   7435\u001b[0m         return reshape_eager_fallback(\n\u001b[1;32m-> 7436\u001b[1;33m             tensor, shape, name=name, ctx=_ctx)\n\u001b[0m\u001b[0;32m   7437\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_SymbolicException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   7438\u001b[0m         \u001b[1;32mpass\u001b[0m  \u001b[1;31m# Add nodes to the TensorFlow graph.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Projects\\Intepreters\\Anaconda\\envs\\TF21\\lib\\site-packages\\tensorflow_core\\python\\ops\\gen_array_ops.py\u001b[0m in \u001b[0;36mreshape_eager_fallback\u001b[1;34m(tensor, shape, name, ctx)\u001b[0m\n\u001b[0;32m   7461\u001b[0m   \u001b[0m_attrs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"T\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_attr_T\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Tshape\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_attr_Tshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   7462\u001b[0m   _result = _execute.execute(b\"Reshape\", 1, inputs=_inputs_flat, attrs=_attrs,\n\u001b[1;32m-> 7463\u001b[1;33m                              ctx=ctx, name=name)\n\u001b[0m\u001b[0;32m   7464\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0m_execute\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmust_record_gradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   7465\u001b[0m     _execute.record_gradient(\n",
      "\u001b[1;32mE:\\Projects\\Intepreters\\Anaconda\\envs\\TF21\\lib\\site-packages\\tensorflow_core\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[0;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m                                                num_outputs)\n\u001b[0m\u001b[0;32m     62\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "train_positive_dataset = tf.data.TFRecordDataset(train_positive_file_name)\n",
    "train_negative_dataset = tf.data.TFRecordDataset(train_negative_file_name)\n",
    "# Create a dictionary describing the features.\n",
    "image_feature_description = {\n",
    "    \n",
    "        'dicom_path1': tf.io.FixedLenFeature([], tf.string),\n",
    "        'seg_label1': tf.io.FixedLenFeature([], tf.string), \n",
    "        'cls_label1': tf.io.FixedLenFeature([], tf.int64),\n",
    "        \n",
    "        'dicom_path2': tf.io.FixedLenFeature([], tf.string),\n",
    "        'seg_label2': tf.io.FixedLenFeature([], tf.string), \n",
    "        'cls_label2': tf.io.FixedLenFeature([], tf.int64),\n",
    "        \n",
    "        'dicom_path3': tf.io.FixedLenFeature([], tf.string),\n",
    "        'seg_label3': tf.io.FixedLenFeature([], tf.string), \n",
    "        'cls_label3': tf.io.FixedLenFeature([], tf.int64),\n",
    "}\n",
    "\n",
    "\n",
    "def _parse_image_function(example_proto):\n",
    "    parsed_features = tf.io.parse_single_example(example_proto, image_feature_description)\n",
    "    # Parse the input tf.Example proto using the dictionary above.\n",
    "    \n",
    "    # for input 1 --->\n",
    "    # decode dicom\n",
    "    dicom_path1 = parsed_features[\"dicom_path1\"]\n",
    "    image_bytes1 = tf.io.read_file(dicom_path1)\n",
    "    input_image1 = tf.cast(tfio.image.decode_dicom_image(image_bytes1, dtype=tf.uint16), tf.float32)\n",
    "    # decode mask\n",
    "    seg_label1 = tf.cast(tf.io.decode_raw(parsed_features['seg_label1'], tf.uint8), tf.float32)\n",
    "    seg_label1 = tf.reshape(seg_label1, [-1, 512,512,1])\n",
    "    \n",
    "    # for input 2 --->\n",
    "    # decode dicom\n",
    "    dicom_path2 = parsed_features[\"dicom_path2\"]\n",
    "    image_bytes2 = tf.io.read_file(dicom_path2)\n",
    "    input_image2 = tf.cast(tfio.image.decode_dicom_image(image_bytes2, dtype=tf.uint16), tf.float32)\n",
    "    # decode mask\n",
    "    seg_label2 = tf.cast(tf.io.decode_raw(parsed_features['seg_label2'], tf.uint8), tf.float32)\n",
    "    seg_label2 = tf.reshape(seg_label2, [-1, 512,512,1])\n",
    "    \n",
    "    # for input 3 --->\n",
    "    # decode dicom\n",
    "    dicom_path3 = parsed_features[\"dicom_path3\"]\n",
    "    image_bytes3 = tf.io.read_file(dicom_path3)\n",
    "    input_image3 = tf.cast(tfio.image.decode_dicom_image(image_bytes3, dtype=tf.uint16), tf.float32)\n",
    "    # decode mask\n",
    "    seg_label3 = tf.cast(tf.io.decode_raw(parsed_features['seg_label3'], tf.uint8), tf.float32)\n",
    "    seg_label3 = tf.reshape(seg_label3, [-1, 512,512,1])\n",
    "    \n",
    "    return (dicom_path1, dicom_path2, dicom_path3), (input_image1, input_image2, input_image3), (seg_label1, seg_label2, seg_label3),  (parsed_features[\"cls_label1\"], parsed_features[\"cls_label2\"], parsed_features[\"cls_label3\"])\n",
    "\n",
    "\n",
    "parsed_train_positive_dataset = train_positive_dataset.map(_parse_image_function)\n",
    "parsed_train_negative_dataset = train_negative_dataset.map(_parse_image_function)\n",
    "\n",
    "def truncate(x, min, max):\n",
    "#     print(x.shape)\n",
    "    cliped =  tf.clip_by_value(x, min, max)\n",
    "    return cliped\n",
    " \n",
    "def norm(x, min, max):\n",
    "    # normalize_value = (value − min_value) / (max_value − min_value)\n",
    "    tensor = tf.math.divide(tf.subtract(x, min),\n",
    "                    tf.subtract(max, min))\n",
    "    return tensor\n",
    "\n",
    "def linear_normalization(input, min=30720.0, max=34816.0):\n",
    "    truncated_input = truncate(input, min, max)\n",
    "    norm_input = norm(truncated_input, min, max )\n",
    "    return  norm_input\n",
    "\n",
    "def winwise(input,LB,HB):\n",
    "    # 20 ,380 for range (-32768, 32767)\n",
    "    # for tf input , (0, 65535)-? LB =  32788, 33148\n",
    "    input[input<LB] = LB # low boundary , if < LW , set to LW\n",
    "    input[input>HB] = HB # high boundary, if > Hw, Set to 255\n",
    "    return input\n",
    "\n",
    "def plot_one_input(dicom_path, input, seg_label, cls_label):\n",
    "    palette = copy(plt.cm.gray)\n",
    "    palette.set_over('r', 1.0)\n",
    "    print(\"dicom_path:\", dicom_path)\n",
    "    print(\"input_image.shape\", input.shape)\n",
    "    print(\"seg_label\", seg_label.shape)\n",
    "    print(\"cls_label\", cls_label)\n",
    "    num_nonzeros=np.count_nonzero(seg_label)\n",
    "    # reshape label \n",
    "    target =  tf.reshape(seg_label, input.shape)\n",
    "    fig, axes = plt.subplots(1,2, figsize=(20,20))\n",
    "    \n",
    "    mask =  np.squeeze(target.numpy())\n",
    "    norm_mask =mask/255.0\n",
    "# #     input_arr = np.squeeze(winwise(input.numpy(), 32788,33148 ))\n",
    "    norm_input = linear_normalization(input, 32788.0,33148.0)\n",
    "    norm_input = np.squeeze(norm_input.numpy())\n",
    "#     masked_in = norm_mask + norm_input\n",
    "#     print('masked_in range:[{}, {}]'.format((masked_in.numpy().min()), masked_in.numpy().max()))\n",
    "\n",
    "#     masked = np.ma.masked_where(norm_mask==0, masked_in)  # this is only generated mask\n",
    "#     print('masked range:[{}, {}]'.format(masked.min(), masked.max())) \n",
    "#     print()\n",
    "    axes[0].imshow(norm_input, cmap='gray')\n",
    "    axes[0].set_title('input range:[{}, {}]'.format((norm_input.min()), np.max(norm_input)))\n",
    "    axes[1].imshow(norm_mask, cmap='gray')\n",
    "    axes[1].set_title('seg target range:[{}, {}] \\n cls_tar: {} # of nonzeros: {}'.format(np.min(norm_mask), np.max(norm_mask), cls_label, num_nonzeros))\n",
    "    \n",
    "    print(\"norm_mask\", norm_mask.shape)\n",
    "    print(\"norm_input\", norm_input.shape)\n",
    "    masked_in = norm_mask + norm_input\n",
    "    print('masked_in range:[{}, {}]'.format((masked_in.min()), masked_in.max()))\n",
    "\n",
    "    masked = np.ma.masked_where(norm_mask==0, masked_in)  # this is only generated mask\n",
    "    print('masked range:[{}, {}]'.format(masked.min(), masked.max())) \n",
    "#     axes[2].imshow(np.squeeze(norm_input), cmap='gray')\n",
    "#     axes[2].imshow(np.squeeze(masked), palette, colors.Normalize(vmin=0, vmax=1), interpolation='none', alpha=0.4)\n",
    "#     axes[2].set_title('target range:[{}, {}]'.format(np.min(masked), np.max(masked)) + dicom_path)\n",
    "    \n",
    "    fig2, axes2 = plt.subplots(1,1, figsize=(20,20))\n",
    "    axes2.imshow(norm_input, cmap='gray')\n",
    "    axes2.imshow(masked, palette, colors.Normalize(vmin=0, vmax=1), interpolation='none', alpha=0.4)\n",
    "    axes2.set_title('target range:[{}, {}]'.format(np.min(masked), np.max(masked)) + dicom_path)\n",
    "    \n",
    "    \n",
    "    \n",
    "def check_dataset(dataset):\n",
    "    # window the input\n",
    "  \n",
    "\n",
    "    # print(len(parsed_train_positive_dataset))\n",
    "    BUFFER_SIZE =512\n",
    "    # random shuffle the train positive dagtaset\n",
    "    parsed_dataset = dataset.shuffle(BUFFER_SIZE)\n",
    "    \n",
    " \n",
    "#     palette.set_under('g', 1.0)\n",
    "#     palette.set_bad('b', 1.0)\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    for dicom_paths, inputs, seg_labels, cls_labels in parsed_dataset.take(1):\n",
    "        for i in range(len(dicom_paths)):\n",
    "            plot_one_input(dicom_paths[i], inputs[i], seg_labels[i], cls_labels[i])\n",
    "            \n",
    "\n",
    "       \n",
    "\n",
    "        \n",
    "check_dataset(parsed_train_positive_dataset)\n",
    "check_dataset(parsed_train_negative_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Val TFRECORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# for val\n",
    "val_root_folder_path_for_cases= \"E:/dataset/Leisang/myTry/BleedingDataDCM/val/\"\n",
    "\n",
    "# give the location of the file\n",
    "val_excel_loc = \"E:\\\\dataset\\\\Leisang\\\\myTry\\\\BleedingDataDCM/valfilpOrNot.xlsx\"\n",
    "\n",
    "\n",
    "# make positive and negative samples:\n",
    "\n",
    "val_positive_sample_features_list, val_negative_sample_features_list =make_sample_pairs(root_folder_path_for_cases=val_root_folder_path_for_cases,\n",
    "                                                              excel_loc = val_excel_loc,\n",
    "                                                                    train_or_val=\"val\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# ## Make into functions for genrerate tf records\n",
    "# train_positive_file_name =  \"E:\\\\dataset\\\\Leisang\\\\myTry\\\\BleedingDataDCM/train_positive_sample_pairs_win.tfrecords\"\n",
    "# train_negative_file_name =  'E:\\\\dataset\\\\Leisang\\\\myTry\\\\BleedingDataDCM/train_negative_sample_pairs_win.tfrecords'\n",
    "val_positive_file_name =  \"E:\\\\dataset\\\\Leisang\\\\myTry\\\\BleedingDataDCM/val_positive_sample_pairs_win.tfrecords\"\n",
    "val_negative_file_name =  'E:\\\\dataset\\\\Leisang\\\\myTry\\\\BleedingDataDCM/val_negative_sample_pairs_win.tfrecords'\n",
    "# make paris dataset for positive\n",
    "Make_InputPairsTFrecords(val_positive_sample_features_list,file_nameToSave=val_positive_file_name )\n",
    "\n",
    "# make paris dataset for negative\n",
    "Make_InputPairsTFrecords(val_negative_sample_features_list,file_nameToSave=val_negative_file_name )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# make val_positive\n",
    "# Make_TFRecords(feature_samples_dict=val_negative_samples, file_name=val_negative_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "val_positive_dataset = tf.data.TFRecordDataset(val_positive_file_name)\n",
    "val_negative_dataset = tf.data.TFRecordDataset(val_negative_file_name)\n",
    "parsed_val_positive_dataset = val_positive_dataset.map(_parse_image_function)\n",
    "parsed_val_negative_dataset = val_negative_dataset.map(_parse_image_function)\n",
    "\n",
    "check_dataset(parsed_val_positive_dataset)\n",
    "check_dataset(parsed_val_negative_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
